# ç”µå•†æ¬ºè¯ˆé£é™©é¢„æµ‹ç³»ç»Ÿ - æŠ€æœ¯æ–‡æ¡£

## ğŸ“‹ ç›®å½•
- [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
- [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
- [æ ¸å¿ƒåŠŸèƒ½æ¨¡å—](#æ ¸å¿ƒåŠŸèƒ½æ¨¡å—)
- [å‰ç«¯ç•Œé¢åŠŸèƒ½](#å‰ç«¯ç•Œé¢åŠŸèƒ½)
- [åç«¯æ ¸å¿ƒç®—æ³•](#åç«¯æ ¸å¿ƒç®—æ³•)
- [æ•°æ®æµç¨‹](#æ•°æ®æµç¨‹)
- [æŠ€æœ¯å®ç°ç»†èŠ‚](#æŠ€æœ¯å®ç°ç»†èŠ‚)

## ğŸ¯ ç³»ç»Ÿæ¦‚è¿°

ç”µå•†æ¬ºè¯ˆé£é™©é¢„æµ‹ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºæœºå™¨å­¦ä¹ çš„æ™ºèƒ½é£æ§å¹³å°ï¼Œé‡‡ç”¨å¤šç­–ç•¥èåˆçš„æ–¹æ³•è¿›è¡Œæ¬ºè¯ˆæ£€æµ‹ã€‚ç³»ç»Ÿé€šè¿‡æ— ç›‘ç£å­¦ä¹ ã€åŠç›‘ç£å­¦ä¹ å’Œç›‘ç£å­¦ä¹ ç›¸ç»“åˆçš„æ–¹å¼ï¼Œå®ç°å¯¹ç”µå•†äº¤æ˜“çš„å…¨æ–¹ä½é£é™©è¯„ä¼°ã€‚

### æ ¸å¿ƒç‰¹æ€§
- **å¤šç»´åº¦é£é™©è¯„ä¼°**: ç»“åˆèšç±»åˆ†æã€ä¸“å®¶è§„åˆ™å’Œæœºå™¨å­¦ä¹ æ¨¡å‹
- **ä¼ªæ ‡ç­¾ç”Ÿæˆ**: åŸºäºå¤šç­–ç•¥çš„é«˜è´¨é‡ä¼ªæ ‡ç­¾ç”Ÿæˆ
- **å®æ—¶é£é™©è¯„åˆ†**: åŠ¨æ€é˜ˆå€¼ç®¡ç†å’Œé£é™©ç­‰çº§åˆ’åˆ†
- **æ”»å‡»ç±»å‹è¯†åˆ«**: æ™ºèƒ½è¯†åˆ«4ç§ä¸»è¦æ”»å‡»æ¨¡å¼
- **å¯è§£é‡Šæ€§åˆ†æ**: SHAP/LIMEæ·±åº¦è§£é‡Šæ¨¡å‹å†³ç­–

### æŠ€æœ¯æ ˆ
- **å‰ç«¯**: Streamlit (Python Webæ¡†æ¶)
- **åç«¯**: Python + Scikit-learn + CatBoost + XGBoost
- **æ•°æ®å¤„ç†**: Pandas + NumPy
- **å¯è§†åŒ–**: Plotly + Seaborn
- **æœºå™¨å­¦ä¹ **: é›†æˆå­¦ä¹  + æ— ç›‘ç£å­¦ä¹  + åŠç›‘ç£å­¦ä¹ 

## ğŸ—ï¸ æ¶æ„è®¾è®¡

```
ç”µå•†æ¬ºè¯ˆé£é™©é¢„æµ‹ç³»ç»Ÿ
â”œâ”€â”€ frontend/                    # å‰ç«¯ç•Œé¢å±‚
â”‚   â”œâ”€â”€ pages/                   # é¡µé¢æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ upload_page.py       # æ•°æ®ä¸Šä¼ 
â”‚   â”‚   â”œâ”€â”€ feature_analysis_page.py  # ç‰¹å¾å·¥ç¨‹
â”‚   â”‚   â”œâ”€â”€ clustering_page.py   # èšç±»åˆ†æ
â”‚   â”‚   â”œâ”€â”€ risk_scoring_page.py # é£é™©è¯„åˆ†
â”‚   â”‚   â”œâ”€â”€ pseudo_labeling_page.py   # ä¼ªæ ‡ç­¾ç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ model_prediction_page.py  # æ¨¡å‹é¢„æµ‹
â”‚   â”‚   â”œâ”€â”€ attack_analysis_page.py   # æ”»å‡»åˆ†ç±»
â”‚   â”‚   â””â”€â”€ report_page.py       # åˆ†ææŠ¥å‘Š
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ backend/                     # åç«¯ä¸šåŠ¡å±‚
â”‚   â”œâ”€â”€ data_processor/          # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ feature_engineering/     # ç‰¹å¾å·¥ç¨‹æ¨¡å—
â”‚   â”œâ”€â”€ clustering/              # èšç±»åˆ†ææ¨¡å—
â”‚   â”œâ”€â”€ risk_scoring/            # é£é™©è¯„åˆ†æ¨¡å—
â”‚   â”œâ”€â”€ pseudo_labeling/         # ä¼ªæ ‡ç­¾ç”Ÿæˆæ¨¡å—
â”‚   â”œâ”€â”€ ml_models/               # æœºå™¨å­¦ä¹ æ¨¡å‹æ¨¡å—
â”‚   â”œâ”€â”€ attack_classification/   # æ”»å‡»åˆ†ç±»æ¨¡å—
â”‚   â”œâ”€â”€ explainer/               # å¯è§£é‡Šæ€§æ¨¡å—
â”‚   â””â”€â”€ analysis_reporting/      # æŠ¥å‘Šç”Ÿæˆæ¨¡å—
â”œâ”€â”€ models/                      # æ¨¡å‹å­˜å‚¨
â”‚   â”œâ”€â”€ pretrained/              # é¢„è®­ç»ƒæ¨¡å‹
â”‚   â””â”€â”€ user_trained/            # ç”¨æˆ·è®­ç»ƒæ¨¡å‹
â”œâ”€â”€ data/                        # æ•°æ®å­˜å‚¨
â””â”€â”€ scripts/                     # å·¥å…·è„šæœ¬
```

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

### 1. æ•°æ®ä¸Šä¼ ä¸é¢„å¤„ç†æ¨¡å—

**å‰ç«¯ç•Œé¢**: `frontend/pages/upload_page.py`
**åç«¯å®ç°**: `backend/data_processor/`

#### ä¸»è¦åŠŸèƒ½
- æ”¯æŒCSVæ–‡ä»¶ä¸Šä¼ å’Œåœ¨çº¿æ•°æ®åŠ è½½
- è‡ªåŠ¨æ•°æ®è´¨é‡æ£€æµ‹å’Œæ¸…ç†
- æ•°æ®æ ‡å‡†åŒ–å’Œæ ¼å¼è½¬æ¢

#### æ ¸å¿ƒç±»å’Œæ–¹æ³•
```python
# backend/data_processor/data_loader.py
class DataLoader:
    def load_csv_file(self, file_path: str) -> pd.DataFrame
    def validate_data_format(self, data: pd.DataFrame) -> bool
    def get_data_summary(self, data: pd.DataFrame) -> Dict

# backend/data_processor/data_cleaner.py
class DataCleaner:
    def clean_data(self, data: pd.DataFrame) -> pd.DataFrame
    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame
    def _handle_duplicates(self, df: pd.DataFrame) -> pd.DataFrame
    def _handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame
```

#### æŠ€æœ¯å®ç°
- **æ•°æ®éªŒè¯**: æ£€æŸ¥å¿…éœ€å­—æ®µã€æ•°æ®ç±»å‹ã€å–å€¼èŒƒå›´
- **ç¼ºå¤±å€¼å¤„ç†**: åŸºäºå­—æ®µç±»å‹çš„æ™ºèƒ½å¡«å……ç­–ç•¥
- **å¼‚å¸¸å€¼æ£€æµ‹**: IQRæ–¹æ³•æ£€æµ‹å’Œå¤„ç†å¼‚å¸¸å€¼
- **é‡å¤æ•°æ®**: åŸºäºå…³é”®å­—æ®µçš„å»é‡é€»è¾‘

### 2. ç‰¹å¾å·¥ç¨‹æ¨¡å—

**å‰ç«¯ç•Œé¢**: `frontend/pages/feature_analysis_page.py`
**åç«¯å®ç°**: `backend/feature_engineering/` + `backend/feature_engineer/`

#### ä¸»è¦åŠŸèƒ½
- è‡ªåŠ¨ç”Ÿæˆé£é™©ç‰¹å¾
- ç‰¹å¾é€‰æ‹©å’Œé‡è¦æ€§åˆ†æ
- ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–

#### æ ¸å¿ƒç±»å’Œæ–¹æ³•
```python
# backend/feature_engineer/risk_features.py
class RiskFeatureEngineer:
    def engineer_all_features(self, data: pd.DataFrame) -> pd.DataFrame
    def create_time_features(self, data: pd.DataFrame) -> pd.DataFrame
    def create_amount_features(self, data: pd.DataFrame) -> pd.DataFrame
    def create_customer_features(self, data: pd.DataFrame) -> pd.DataFrame
    def create_behavioral_features(self, data: pd.DataFrame) -> pd.DataFrame

# backend/feature_engineering/feature_selector.py
class FeatureSelector:
    def select_features(self, X: pd.DataFrame, y: pd.Series) -> List[str]
    def calculate_feature_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict
```

#### ç®—æ³•æ€è·¯
1. **æ—¶é—´ç‰¹å¾**: äº¤æ˜“æ—¶é—´ã€å·¥ä½œæ—¥/å‘¨æœ«ã€èŠ‚å‡æ—¥æ ‡è¯†
2. **é‡‘é¢ç‰¹å¾**: é‡‘é¢åˆ†ä½æ•°ã€å¼‚å¸¸é‡‘é¢æ ‡è¯†ã€é‡‘é¢å˜åŒ–ç‡
3. **å®¢æˆ·ç‰¹å¾**: è´¦æˆ·å¹´é¾„ã€å†å²äº¤æ˜“é¢‘ç‡ã€é£é™©è¯„åˆ†
4. **è¡Œä¸ºç‰¹å¾**: è®¾å¤‡æŒ‡çº¹ã€åœ°ç†ä½ç½®ã€äº¤æ˜“æ¨¡å¼

### 3. èšç±»åˆ†ææ¨¡å— â­

**å‰ç«¯ç•Œé¢**: `frontend/pages/clustering_page.py`
**åç«¯å®ç°**: `backend/clustering/`

#### ä¸»è¦åŠŸèƒ½
- æ— ç›‘ç£ç”¨æˆ·è¡Œä¸ºæ¨¡å¼å‘ç°
- èšç±»é£é™©ç­‰çº§æ˜ å°„
- èšç±»ç»“æœå¯è§†åŒ–å’Œè§£é‡Š

#### æ ¸å¿ƒç±»å’Œæ–¹æ³•
```python
# backend/clustering/cluster_analyzer.py
class ClusterAnalyzer:
    def analyze_clusters(self, data: pd.DataFrame) -> Dict[str, Any]
    def _prepare_clustering_data(self, data: pd.DataFrame) -> pd.DataFrame
    def _perform_clustering(self, cluster_data: pd.DataFrame) -> np.ndarray
    def _analyze_cluster_characteristics(self, data: pd.DataFrame, labels: np.ndarray) -> Dict

# backend/clustering/cluster_risk_mapper.py
class ClusterRiskMapper:
    def map_clusters_to_risk_levels(self, cluster_results: Dict) -> Dict[int, str]
    def _calculate_cluster_risk_score(self, cluster_data: pd.DataFrame) -> float
    def _assign_risk_level(self, risk_score: float) -> str

# backend/clustering/cluster_interpreter.py
class ClusterInterpreter:
    def interpret_clusters(self, cluster_results: Dict) -> Dict[str, Any]
    def _generate_cluster_profiles(self, cluster_results: Dict) -> List[Dict]
```

#### ç®—æ³•å®ç°
1. **èšç±»ç®—æ³•**: K-Meansèšç±»ï¼Œè‡ªåŠ¨ç¡®å®šæœ€ä¼˜èšç±»æ•°
2. **ç‰¹å¾é€‰æ‹©**: é€‰æ‹©å…³é”®é£é™©ç‰¹å¾è¿›è¡Œèšç±»
3. **é£é™©æ˜ å°„**: åŸºäºèšç±»å†…æ¬ºè¯ˆç‡å’Œé£é™©ç‰¹å¾åˆ†å¸ƒç¡®å®šé£é™©ç­‰çº§
4. **ç»“æœè§£é‡Š**: ç”Ÿæˆæ¯ä¸ªèšç±»çš„ç‰¹å¾ç”»åƒå’Œé£é™©æè¿°

#### æŠ€æœ¯ç»†èŠ‚
```python
# èšç±»ç‰¹å¾é€‰æ‹©
clustering_features = [
    'transaction_amount', 'quantity', 'customer_age', 
    'account_age_days', 'transaction_hour'
]

# é£é™©ç­‰çº§æ˜ å°„é€»è¾‘
def _assign_risk_level(self, risk_score: float) -> str:
    if risk_score >= 80:
        return 'critical'  # æé«˜é£é™©
    elif risk_score >= 60:
        return 'high'      # é«˜é£é™©
    elif risk_score >= 40:
        return 'medium'    # ä¸­é£é™©
    else:
        return 'low'       # ä½é£é™©
```

### 4. é£é™©è¯„åˆ†æ¨¡å— â­

**å‰ç«¯ç•Œé¢**: `frontend/pages/risk_scoring_page.py`
**åç«¯å®ç°**: `backend/risk_scoring/`

#### ä¸»è¦åŠŸèƒ½
- å¤šç»´åº¦æ— ç›‘ç£é£é™©è¯„åˆ†
- åŠ¨æ€é˜ˆå€¼ç®¡ç†
- é£é™©ç­‰çº§åˆ†å¸ƒåˆ†æ

#### æ ¸å¿ƒç±»å’Œæ–¹æ³•
```python
# backend/risk_scoring/risk_calculator.py
class RiskCalculator:
    def calculate_unsupervised_risk_score(self, data: pd.DataFrame, cluster_results: Dict) -> Dict
    def _calculate_cluster_risk(self, data: pd.DataFrame, cluster_results: Dict) -> np.ndarray
    def _calculate_rule_risk(self, data: pd.DataFrame) -> np.ndarray
    def _calculate_model_risk(self, data: pd.DataFrame) -> np.ndarray
    def _combine_risk_scores(self, cluster_risk: np.ndarray, rule_risk: np.ndarray, model_risk: np.ndarray) -> np.ndarray

# backend/risk_scoring/standard_risk_calculator.py
class StandardRiskCalculator(RiskCalculator):
    def calculate_unsupervised_risk_score(self, data: pd.DataFrame, cluster_results: Dict) -> Dict

# backend/risk_scoring/fast_risk_calculator.py
class FastRiskCalculator(RiskCalculator):
    def calculate_unsupervised_risk_score(self, data: pd.DataFrame, cluster_results: Dict) -> Dict

# backend/risk_scoring/dynamic_threshold_manager.py
class DynamicThresholdManager:
    def calculate_dynamic_thresholds(self, risk_scores: np.ndarray) -> Dict[str, float]
    def _calculate_percentile_thresholds(self, risk_scores: np.ndarray) -> Dict[str, float]
```

#### ç®—æ³•æ€è·¯
1. **å¤šç­–ç•¥èåˆ**: èšç±»é£é™© + ä¸“å®¶è§„åˆ™ + æ¨¡å‹é¢„æµ‹
2. **æƒé‡åˆ†é…**: èšç±»é£é™©(40%) + è§„åˆ™é£é™©(35%) + æ¨¡å‹é£é™©(25%)
3. **åŠ¨æ€é˜ˆå€¼**: åŸºäºæ•°æ®åˆ†å¸ƒè‡ªåŠ¨è°ƒæ•´é£é™©ç­‰çº§é˜ˆå€¼
4. **æ ‡å‡†/å¿«é€Ÿæ¨¡å¼**: æä¾›ä¸åŒç²¾åº¦å’Œé€Ÿåº¦çš„è®¡ç®—é€‰é¡¹

#### æŠ€æœ¯å®ç°
```python
# é£é™©è¯„åˆ†èåˆç®—æ³•
def _combine_risk_scores(self, cluster_risk, rule_risk, model_risk):
    weights = {
        'cluster': 0.40,  # èšç±»é£é™©æƒé‡
        'rule': 0.35,     # è§„åˆ™é£é™©æƒé‡
        'model': 0.25     # æ¨¡å‹é£é™©æƒé‡
    }

    combined_risk = (
        weights['cluster'] * cluster_risk +
        weights['rule'] * rule_risk +
        weights['model'] * model_risk
    )

    return np.clip(combined_risk, 0, 100)

# åŠ¨æ€é˜ˆå€¼è®¡ç®—
def calculate_dynamic_thresholds(self, risk_scores):
    return {
        'low': np.percentile(risk_scores, 25),      # 25åˆ†ä½æ•°
        'medium': np.percentile(risk_scores, 50),   # 50åˆ†ä½æ•°
        'high': np.percentile(risk_scores, 75),     # 75åˆ†ä½æ•°
        'critical': np.percentile(risk_scores, 90)  # 90åˆ†ä½æ•°
    }
```

### 5. ä¼ªæ ‡ç­¾ç”Ÿæˆæ¨¡å— â­â­

**å‰ç«¯ç•Œé¢**: `frontend/pages/pseudo_labeling_page.py`
**åç«¯å®ç°**: `backend/pseudo_labeling/`

#### ä¸»è¦åŠŸèƒ½
- å¤šç­–ç•¥ä¼ªæ ‡ç­¾ç”Ÿæˆ
- æ ‡ç­¾è´¨é‡è¯„ä¼°å’Œæ ¡å‡†
- é«˜è´¨é‡æ ‡ç­¾ç­›é€‰

#### æ ¸å¿ƒç±»å’Œæ–¹æ³•
```python
# backend/pseudo_labeling/pseudo_label_generator.py
class PseudoLabelGenerator:
    def generate_pseudo_labels(self, data: pd.DataFrame, strategy: str = 'ensemble') -> Dict
    def generate_high_quality_pseudo_labels(self, data: pd.DataFrame, min_confidence: float = 0.8) -> Dict
    def _ensemble_strategy(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]
    def _risk_based_strategy(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]
    def _cluster_based_strategy(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]
    def _rule_based_strategy(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]

# backend/pseudo_labeling/fast_pseudo_label_generator.py
class FastPseudoLabelGenerator:
    def generate_fast_pseudo_labels(self, data: pd.DataFrame, risk_results: Dict = None) -> Dict
    def _simplified_ensemble(self, data: pd.DataFrame, risk_results: Dict) -> Tuple[np.ndarray, np.ndarray]
```

#### ç®—æ³•ç­–ç•¥
1. **é›†æˆç­–ç•¥ (Ensemble)**: èåˆå¤šç§ç­–ç•¥çš„æŠ•ç¥¨ç»“æœ
2. **é£é™©ç­–ç•¥ (Risk-based)**: åŸºäºé£é™©è¯„åˆ†é˜ˆå€¼ç”Ÿæˆæ ‡ç­¾
3. **èšç±»ç­–ç•¥ (Cluster-based)**: åŸºäºèšç±»é£é™©ç­‰çº§ç”Ÿæˆæ ‡ç­¾
4. **è§„åˆ™ç­–ç•¥ (Rule-based)**: åŸºäºä¸“å®¶ä¸šåŠ¡è§„åˆ™ç”Ÿæˆæ ‡ç­¾

#### æŠ€æœ¯å®ç°
```python
# é›†æˆç­–ç•¥å®ç°
def _ensemble_strategy(self, data: pd.DataFrame):
    # è·å–å„ç­–ç•¥ç»“æœ
    risk_labels, risk_conf = self._risk_based_strategy(data)
    cluster_labels, cluster_conf = self._cluster_based_strategy(data)
    rule_labels, rule_conf = self._rule_based_strategy(data)

    # åŠ æƒæŠ•ç¥¨
    weights = {'risk': 0.45, 'cluster': 0.35, 'rule': 0.20}

    ensemble_confidence = (
        weights['risk'] * risk_conf +
        weights['cluster'] * cluster_conf +
        weights['rule'] * rule_conf
    )

    # åŸºäºç½®ä¿¡åº¦é˜ˆå€¼ç”Ÿæˆæœ€ç»ˆæ ‡ç­¾
    ensemble_labels = (ensemble_confidence >= self.confidence_threshold).astype(int)

    return ensemble_labels, ensemble_confidence

# é«˜è´¨é‡æ ‡ç­¾ç­›é€‰
def generate_high_quality_pseudo_labels(self, data: pd.DataFrame, min_confidence: float = 0.8):
    labels, confidences = self._ensemble_strategy(data)

    # ç­›é€‰é«˜ç½®ä¿¡åº¦æ ‡ç­¾
    high_quality_mask = confidences >= min_confidence
    high_quality_indices = np.where(high_quality_mask)[0]
    high_quality_labels = labels[high_quality_mask]
    high_quality_confidences = confidences[high_quality_mask]

    return {
        'all_labels': labels,
        'all_confidences': confidences,
        'high_quality_indices': high_quality_indices,
        'high_quality_labels': high_quality_labels,
        'high_quality_confidences': high_quality_confidences,
        'quality_rate': len(high_quality_indices) / len(labels)
    }
```

### 6. æ¨¡å‹é¢„æµ‹æ¨¡å—

**å‰ç«¯ç•Œé¢**: `frontend/pages/model_prediction_page.py`
**åç«¯å®ç°**: `backend/ml_models/`

#### ä¸»è¦åŠŸèƒ½
- å¤šæ¨¡å‹é¢„æµ‹å’Œæ€§èƒ½å¯¹æ¯”
- é›†æˆå­¦ä¹ é¢„æµ‹
- é¢„æµ‹ç»“æœåˆ†æå’Œå¯è§†åŒ–

#### æ ¸å¿ƒç±»å’Œæ–¹æ³•
```python
# backend/ml_models/model_manager.py
class ModelManager:
    def load_model(self, model_name: str) -> Any
    def predict_with_model(self, model, X: pd.DataFrame, y: pd.Series = None) -> Tuple[np.ndarray, np.ndarray]
    def evaluate_model(self, predictions: np.ndarray, probabilities: np.ndarray, y_true: pd.Series) -> Dict
    def get_available_models(self) -> List[str]

# backend/ml_models/ensemble_predictor.py
class EnsemblePredictor:
    def predict(self, model_probabilities: Dict[str, np.ndarray], threshold: float = 0.5) -> Tuple[np.ndarray, np.ndarray]
    def _weighted_voting(self, model_probabilities: Dict[str, np.ndarray]) -> np.ndarray
    def _simple_voting(self, model_probabilities: Dict[str, np.ndarray]) -> np.ndarray
```

#### æ”¯æŒçš„æ¨¡å‹
1. **CatBoost**: å¤„ç†ç±»åˆ«ç‰¹å¾å¼ºï¼ŒæŠ—è¿‡æ‹Ÿåˆ
2. **XGBoost**: é€Ÿåº¦å¿«ï¼Œå†…å­˜æ•ˆç‡é«˜
3. **Random Forest**: ç¨³å®šæ€§å¥½ï¼Œä¸æ˜“è¿‡æ‹Ÿåˆ
4. **Ensemble**: èåˆä¸Šè¿°æ¨¡å‹çš„é›†æˆé¢„æµ‹

#### æŠ€æœ¯å®ç°
```python
# é›†æˆé¢„æµ‹ç®—æ³•
def _weighted_voting(self, model_probabilities):
    ensemble_probs = np.zeros(len(list(model_probabilities.values())[0]))
    total_weight = 0

    # æ¨¡å‹æƒé‡é…ç½®
    weights = {
        'catboost': 0.4,
        'xgboost': 0.3,
        'randomforest': 0.3
    }

    for model_name, probs in model_probabilities.items():
        weight = weights.get(model_name.lower(), 1.0)
        ensemble_probs += weight * probs
        total_weight += weight

    return ensemble_probs / total_weight if total_weight > 0 else ensemble_probs
```

### 7. æ”»å‡»åˆ†ç±»æ¨¡å—

**å‰ç«¯ç•Œé¢**: `frontend/pages/attack_analysis_page.py`
**åç«¯å®ç°**: `backend/attack_classification/`

#### ä¸»è¦åŠŸèƒ½
- è¯†åˆ«4ç§ä¸»è¦æ”»å‡»ç±»å‹
- æ”»å‡»æ¨¡å¼åˆ†æ
- é˜²æŠ¤å»ºè®®ç”Ÿæˆ

#### æ ¸å¿ƒç±»å’Œæ–¹æ³•
```python
# backend/attack_classification/attack_classifier.py
class AttackClassifier:
    def classify_attacks(self, data: pd.DataFrame) -> Dict[str, Any]
    def _classify_single_transaction(self, transaction: pd.Series) -> str
    def _calculate_pattern_score(self, transaction: pd.Series, pattern: Dict) -> float
    def _analyze_attack_patterns(self, data: pd.DataFrame, results: List[Dict]) -> Dict

# backend/attack_classification/attack_pattern_analyzer.py
class AttackPatternAnalyzer:
    def analyze_patterns(self, attack_results: Dict) -> Dict[str, Any]
    def _analyze_temporal_patterns(self, attack_data: pd.DataFrame) -> Dict
    def _analyze_amount_patterns(self, attack_data: pd.DataFrame) -> Dict
```

#### æ”»å‡»ç±»å‹å®šä¹‰
1. **è´¦æˆ·æ¥ç®¡æ”»å‡» (Account Takeover)**
   - ç‰¹å¾: æ–°è´¦æˆ·å¤§é¢äº¤æ˜“ã€å¼‚å¸¸ç™»å½•æ—¶é—´
   - é£é™©ç­‰çº§: æé«˜

2. **èº«ä»½ç›—ç”¨æ”»å‡» (Identity Theft)**
   - ç‰¹å¾: ä¸ªäººä¿¡æ¯ä¸åŒ¹é…ã€å¤šè´¦æˆ·å…³è”
   - é£é™©ç­‰çº§: é«˜

3. **æ‰¹é‡æ¬ºè¯ˆæ”»å‡» (Bulk Fraud)**
   - ç‰¹å¾: çŸ­æ—¶é—´å¤§é‡äº¤æ˜“ã€ç›¸ä¼¼äº¤æ˜“æ¨¡å¼
   - é£é™©ç­‰çº§: é«˜

4. **æµ‹è¯•æ€§æ”»å‡» (Testing Attack)**
   - ç‰¹å¾: å°é¢å¤šæ¬¡äº¤æ˜“ã€æ¢æµ‹æ€§è¡Œä¸º
   - é£é™©ç­‰çº§: ä¸­

#### ç®—æ³•å®ç°
```python
# æ”»å‡»æ¨¡å¼å®šä¹‰
attack_patterns = {
    'account_takeover': {
        'characteristics': [
            'æ–°è´¦æˆ·å¤§é¢äº¤æ˜“',
            'å¼‚å¸¸æ—¶é—´ç™»å½•',
            'è®¾å¤‡æŒ‡çº¹å˜åŒ–'
        ],
        'risk_level': 'critical',
        'weight': 0.4
    },
    'identity_theft': {
        'characteristics': [
            'ä¸ªäººä¿¡æ¯ä¸åŒ¹é…',
            'å¤šè´¦æˆ·å…³è”',
            'åœ°ç†ä½ç½®å¼‚å¸¸'
        ],
        'risk_level': 'high',
        'weight': 0.3
    }
    # ... å…¶ä»–æ”»å‡»ç±»å‹
}

# æ”»å‡»åˆ†ç±»ç®—æ³•
def _classify_single_transaction(self, transaction):
    scores = {}

    for attack_type, pattern in self.attack_patterns.items():
        score = self._calculate_pattern_score(transaction, pattern)
        scores[attack_type] = score

    # è¿”å›å¾—åˆ†æœ€é«˜çš„æ”»å‡»ç±»å‹
    return max(scores, key=scores.get)
```

### 8. å¯è§£é‡Šæ€§åˆ†ææ¨¡å—

**å‰ç«¯ç•Œé¢**: `frontend/pages/report_page.py` (SHAPåˆ†æéƒ¨åˆ†)
**åç«¯å®ç°**: `backend/explainer/`

#### ä¸»è¦åŠŸèƒ½
- SHAPå€¼è®¡ç®—å’Œå¯è§†åŒ–
- ç‰¹å¾é‡è¦æ€§åˆ†æ
- æ¨¡å‹å†³ç­–è§£é‡Š

#### æ ¸å¿ƒç±»å’Œæ–¹æ³•
```python
# backend/explainer/shap_explainer.py
class SHAPExplainer:
    def explain_predictions(self, model, X: pd.DataFrame, sample_size: int = 100) -> Dict
    def _calculate_shap_values(self, model, X: pd.DataFrame) -> np.ndarray
    def _create_shap_plots(self, shap_values: np.ndarray, X: pd.DataFrame) -> Dict
```

## ğŸ“Š æ•°æ®æµç¨‹

### å®Œæ•´ä¸šåŠ¡æµç¨‹
```
1. æ•°æ®ä¸Šä¼  â†’ 2. ç‰¹å¾å·¥ç¨‹ â†’ 3. èšç±»åˆ†æ â†’ 4. é£é™©è¯„åˆ† â†’ 5. ä¼ªæ ‡ç­¾ç”Ÿæˆ â†’ 6. æ¨¡å‹é¢„æµ‹ â†’ 7. æ”»å‡»åˆ†ç±» â†’ 8. åˆ†ææŠ¥å‘Š
```

#### è¯¦ç»†æµç¨‹è¯´æ˜

1. **æ•°æ®ä¸Šä¼ é˜¶æ®µ**
   - ç”¨æˆ·ä¸Šä¼ CSVæ–‡ä»¶æˆ–ä½¿ç”¨ç¤ºä¾‹æ•°æ®
   - ç³»ç»Ÿè‡ªåŠ¨éªŒè¯æ•°æ®æ ¼å¼å’Œå®Œæ•´æ€§
   - æ‰§è¡Œæ•°æ®æ¸…ç†å’Œé¢„å¤„ç†

2. **ç‰¹å¾å·¥ç¨‹é˜¶æ®µ**
   - åŸºäºåŸå§‹æ•°æ®ç”Ÿæˆé£é™©ç‰¹å¾
   - åŒ…æ‹¬æ—¶é—´ã€é‡‘é¢ã€å®¢æˆ·ã€è¡Œä¸ºå››å¤§ç±»ç‰¹å¾
   - ç‰¹å¾é€‰æ‹©å’Œé‡è¦æ€§åˆ†æ

3. **èšç±»åˆ†æé˜¶æ®µ**
   - ä½¿ç”¨K-Meanså¯¹ç”¨æˆ·è¡Œä¸ºè¿›è¡Œèšç±»
   - è‡ªåŠ¨ç¡®å®šæœ€ä¼˜èšç±»æ•°é‡
   - ä¸ºæ¯ä¸ªèšç±»åˆ†é…é£é™©ç­‰çº§

4. **é£é™©è¯„åˆ†é˜¶æ®µ**
   - èåˆèšç±»ã€è§„åˆ™ã€æ¨¡å‹ä¸‰ç§é£é™©è¯„åˆ†
   - è®¡ç®—ç»¼åˆé£é™©åˆ†æ•°(0-100)
   - åŠ¨æ€è°ƒæ•´é£é™©ç­‰çº§é˜ˆå€¼

5. **ä¼ªæ ‡ç­¾ç”Ÿæˆé˜¶æ®µ**
   - åŸºäºå¤šç­–ç•¥ç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾
   - æ”¯æŒé›†æˆã€é£é™©ã€èšç±»ã€è§„åˆ™å››ç§ç­–ç•¥
   - ç­›é€‰é«˜ç½®ä¿¡åº¦æ ‡ç­¾ç”¨äºæ¨¡å‹è®­ç»ƒ

6. **æ¨¡å‹é¢„æµ‹é˜¶æ®µ**
   - ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¬ºè¯ˆé¢„æµ‹
   - æ”¯æŒå¤šæ¨¡å‹å¯¹æ¯”å’Œé›†æˆé¢„æµ‹
   - æä¾›æ¦‚ç‡åˆ†å¸ƒå’Œé˜ˆå€¼åˆ†æ

7. **æ”»å‡»åˆ†ç±»é˜¶æ®µ**
   - è¯†åˆ«å…·ä½“çš„æ”»å‡»ç±»å‹å’Œæ¨¡å¼
   - åˆ†ææ”»å‡»ç‰¹å¾å’Œæ—¶é—´åˆ†å¸ƒ
   - ç”Ÿæˆé’ˆå¯¹æ€§é˜²æŠ¤å»ºè®®

8. **åˆ†ææŠ¥å‘Šé˜¶æ®µ**
   - ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
   - SHAPå¯è§£é‡Šæ€§åˆ†æ
   - å¯¼å‡ºç»“æœå’Œå»ºè®®

## ğŸ”¬ æŠ€æœ¯å®ç°ç»†èŠ‚

### æ ¸å¿ƒç®—æ³•è¯¦è§£

#### 1. èšç±»é£é™©æ˜ å°„ç®—æ³•
```python
def _calculate_cluster_risk_score(self, cluster_data: pd.DataFrame) -> float:
    # åŸºäºå¤šä¸ªé£é™©å› å­è®¡ç®—èšç±»é£é™©åˆ†æ•°
    risk_factors = {
        'high_amount_rate': self._calculate_high_amount_rate(cluster_data),
        'new_account_rate': self._calculate_new_account_rate(cluster_data),
        'unusual_time_rate': self._calculate_unusual_time_rate(cluster_data),
        'high_quantity_rate': self._calculate_high_quantity_rate(cluster_data)
    }

    # åŠ æƒè®¡ç®—é£é™©åˆ†æ•°
    weights = {'high_amount_rate': 0.3, 'new_account_rate': 0.3,
               'unusual_time_rate': 0.2, 'high_quantity_rate': 0.2}

    risk_score = sum(weights[factor] * score for factor, score in risk_factors.items())
    return min(risk_score * 100, 100)  # å½’ä¸€åŒ–åˆ°0-100
```

#### 2. ä¼ªæ ‡ç­¾è´¨é‡è¯„ä¼°ç®—æ³•
```python
def _evaluate_label_quality(self, labels: np.ndarray, confidences: np.ndarray) -> Dict:
    # è®¡ç®—æ ‡ç­¾è´¨é‡æŒ‡æ ‡
    high_conf_mask = confidences >= 0.8
    medium_conf_mask = (confidences >= 0.6) & (confidences < 0.8)
    low_conf_mask = confidences < 0.6

    quality_metrics = {
        'high_quality_rate': np.sum(high_conf_mask) / len(labels),
        'medium_quality_rate': np.sum(medium_conf_mask) / len(labels),
        'low_quality_rate': np.sum(low_conf_mask) / len(labels),
        'avg_confidence': np.mean(confidences),
        'fraud_rate': np.mean(labels)
    }

    return quality_metrics
```

#### 3. åŠ¨æ€é˜ˆå€¼ä¼˜åŒ–ç®—æ³•
```python
def optimize_thresholds(self, risk_scores: np.ndarray, target_fraud_rate: float = 0.1) -> Dict:
    # åŸºäºç›®æ ‡æ¬ºè¯ˆç‡ä¼˜åŒ–é˜ˆå€¼
    sorted_scores = np.sort(risk_scores)[::-1]  # é™åºæ’åˆ—
    target_count = int(len(sorted_scores) * target_fraud_rate)

    if target_count > 0:
        optimal_threshold = sorted_scores[target_count - 1]
    else:
        optimal_threshold = np.percentile(risk_scores, 90)

    return {
        'optimal_threshold': optimal_threshold,
        'expected_fraud_rate': target_fraud_rate,
        'threshold_percentile': (1 - target_fraud_rate) * 100
    }
```

### æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### 1. æ•°æ®å¤„ç†ä¼˜åŒ–
- **åˆ†å—å¤„ç†**: å¤§æ•°æ®é›†åˆ†å—å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º
- **å¹¶è¡Œè®¡ç®—**: ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†ç‹¬ç«‹çš„è®¡ç®—ä»»åŠ¡
- **ç¼“å­˜æœºåˆ¶**: ç¼“å­˜ä¸­é—´ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—

#### 2. æ¨¡å‹åŠ è½½ä¼˜åŒ–
- **å»¶è¿ŸåŠ è½½**: æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œå‡å°‘å¯åŠ¨æ—¶é—´
- **æ¨¡å‹å‹ç¼©**: ä½¿ç”¨æ¨¡å‹å‹ç¼©æŠ€æœ¯å‡å°‘å­˜å‚¨ç©ºé—´
- **é¢„çƒ­æœºåˆ¶**: é¢„å…ˆåŠ è½½å¸¸ç”¨æ¨¡å‹åˆ°å†…å­˜

#### 3. å‰ç«¯æ€§èƒ½ä¼˜åŒ–
- **å¼‚æ­¥å¤„ç†**: é•¿æ—¶é—´è®¡ç®—ä½¿ç”¨å¼‚æ­¥å¤„ç†
- **è¿›åº¦æ˜¾ç¤º**: å®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦
- **ç»“æœç¼“å­˜**: ç¼“å­˜è®¡ç®—ç»“æœï¼Œæ”¯æŒå¿«é€Ÿåˆ‡æ¢

## ğŸ¯ ç³»ç»Ÿç‰¹è‰²ä¸åˆ›æ–°

### 1. å¤šç­–ç•¥èåˆæ¶æ„
- æ— ç›‘ç£å­¦ä¹ å‘ç°æœªçŸ¥æ¨¡å¼
- åŠç›‘ç£å­¦ä¹ ç”Ÿæˆé«˜è´¨é‡æ ‡ç­¾
- ç›‘ç£å­¦ä¹ æä¾›ç²¾ç¡®é¢„æµ‹
- ä¸“å®¶è§„åˆ™è¡¥å……ä¸šåŠ¡é€»è¾‘

### 2. è‡ªé€‚åº”é£é™©è¯„ä¼°
- åŠ¨æ€é˜ˆå€¼ç®¡ç†
- å®æ—¶é£é™©ç­‰çº§è°ƒæ•´
- ä¸ªæ€§åŒ–é£é™©ç”»åƒ
- æ™ºèƒ½å¼‚å¸¸æ£€æµ‹

### 3. å¯è§£é‡Šæ€§è®¾è®¡
- SHAPæ·±åº¦è§£é‡Š
- ç‰¹å¾é‡è¦æ€§åˆ†æ
- å†³ç­–è·¯å¾„å¯è§†åŒ–
- ä¸šåŠ¡å‹å¥½çš„è§£é‡Š

### 4. å·¥ç¨‹åŒ–å®ç°
- æ¨¡å—åŒ–æ¶æ„è®¾è®¡
- å¯æ‰©å±•çš„æ’ä»¶æœºåˆ¶
- å®Œå–„çš„é”™è¯¯å¤„ç†
- ç”¨æˆ·å‹å¥½çš„ç•Œé¢

## ğŸ“ˆ ç³»ç»Ÿæ•ˆæœè¯„ä¼°

### é¢„æœŸæ€§èƒ½æŒ‡æ ‡
- **å‡†ç¡®ç‡**: > 95%
- **å¬å›ç‡**: > 90%
- **ç²¾ç¡®ç‡**: > 85%
- **F1åˆ†æ•°**: > 87%
- **å¤„ç†é€Ÿåº¦**: < 2ç§’/åƒæ¡è®°å½•

### ä¸šåŠ¡ä»·å€¼
- **é£é™©è¯†åˆ«**: æå‰å‘ç°æ½œåœ¨æ¬ºè¯ˆé£é™©
- **æŸå¤±å‡å°‘**: é™ä½æ¬ºè¯ˆé€ æˆçš„ç»æµæŸå¤±
- **æ•ˆç‡æå‡**: è‡ªåŠ¨åŒ–é£é™©è¯„ä¼°æµç¨‹
- **å†³ç­–æ”¯æŒ**: æä¾›æ•°æ®é©±åŠ¨çš„å†³ç­–ä¾æ®

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-01-17
**ç»´æŠ¤å›¢é˜Ÿ**: ç”µå•†é£æ§æŠ€æœ¯å›¢é˜Ÿ
