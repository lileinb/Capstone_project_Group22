"""
æ€§èƒ½ç›‘æ§é¡µé¢
å®æ—¶ç›‘æ§ç³»ç»Ÿæ€§èƒ½å’Œé¢„æµ‹å‡†ç¡®ç‡
"""

import streamlit as st
import pandas as pd
import numpy as np
import sys
import os
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import time
import psutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

def show():
    """æ˜¾ç¤ºæ€§èƒ½ç›‘æ§é¡µé¢"""
    st.markdown('<div class="sub-header">ğŸ“Š ç³»ç»Ÿæ€§èƒ½ç›‘æ§ä¸­å¿ƒ</div>', unsafe_allow_html=True)
    
    # åˆå§‹åŒ–session state
    _initialize_session_state()
    
    # æ˜¾ç¤ºç³»ç»Ÿæ¦‚è§ˆ
    _show_system_overview()
    
    # å®æ—¶æ€§èƒ½ç›‘æ§
    _show_real_time_performance()
    
    # é¢„æµ‹æ€§èƒ½ç»Ÿè®¡
    _show_prediction_performance()
    
    # ç³»ç»Ÿèµ„æºç›‘æ§
    _show_system_resources()
    
    # æ€§èƒ½æŠ¥å‘Š
    _show_performance_report()

def _initialize_session_state():
    """åˆå§‹åŒ–session state"""
    if 'performance_history' not in st.session_state:
        st.session_state.performance_history = []
    if 'system_metrics' not in st.session_state:
        st.session_state.system_metrics = {}

def _show_system_overview():
    """æ˜¾ç¤ºç³»ç»Ÿæ¦‚è§ˆ"""
    st.markdown("### ğŸ¯ ç³»ç»ŸçŠ¶æ€æ¦‚è§ˆ")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # æ£€æŸ¥å„æ¨¡å—çŠ¶æ€
        modules_status = _check_modules_status()
        active_modules = sum(modules_status.values())
        total_modules = len(modules_status)
        
        st.metric(
            "ç³»ç»Ÿæ¨¡å—",
            f"{active_modules}/{total_modules}",
            delta=f"{active_modules/total_modules*100:.0f}% å¯ç”¨"
        )
    
    with col2:
        # é¢„æµ‹å»¶è¿Ÿ
        avg_latency = _calculate_average_latency()
        st.metric(
            "å¹³å‡å»¶è¿Ÿ",
            f"{avg_latency:.2f}s",
            delta="æ­£å¸¸" if avg_latency < 2.0 else "åé«˜"
        )
    
    with col3:
        # å†…å­˜ä½¿ç”¨
        memory_usage = psutil.virtual_memory().percent
        st.metric(
            "å†…å­˜ä½¿ç”¨",
            f"{memory_usage:.1f}%",
            delta="æ­£å¸¸" if memory_usage < 80 else "åé«˜"
        )
    
    with col4:
        # CPUä½¿ç”¨
        cpu_usage = psutil.cpu_percent(interval=1)
        st.metric(
            "CPUä½¿ç”¨",
            f"{cpu_usage:.1f}%",
            delta="æ­£å¸¸" if cpu_usage < 70 else "åé«˜"
        )

def _check_modules_status():
    """æ£€æŸ¥å„æ¨¡å—çŠ¶æ€"""
    status = {}
    
    # æ£€æŸ¥ç‰¹å¾å·¥ç¨‹
    status['feature_engineering'] = 'engineered_features' in st.session_state and st.session_state.engineered_features is not None
    
    # æ£€æŸ¥èšç±»åˆ†æ
    status['clustering'] = 'clustering_results' in st.session_state and st.session_state.clustering_results is not None
    
    # æ£€æŸ¥é£é™©è¯„åˆ†
    status['risk_scoring'] = 'four_class_risk_results' in st.session_state and st.session_state.four_class_risk_results is not None
    
    # æ£€æŸ¥æ”»å‡»åˆ†æ
    status['attack_analysis'] = 'attack_results' in st.session_state and st.session_state.attack_results is not None
    
    return status

def _calculate_average_latency():
    """è®¡ç®—å¹³å‡å»¶è¿Ÿ"""
    # ä»å†å²è®°å½•ä¸­è®¡ç®—å¹³å‡å»¶è¿Ÿ
    if st.session_state.performance_history:
        latencies = [record.get('latency', 0) for record in st.session_state.performance_history[-10:]]
        return np.mean(latencies) if latencies else 0.0
    return 0.0

def _show_real_time_performance():
    """æ˜¾ç¤ºå®æ—¶æ€§èƒ½ç›‘æ§"""
    st.markdown("### âš¡ å®æ—¶æ€§èƒ½ç›‘æ§")
    
    # åˆ›å»ºå®æ—¶æ›´æ–°æŒ‰é’®
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ”„ åˆ·æ–°æ€§èƒ½æ•°æ®", use_container_width=True):
            _collect_performance_data()
    
    with col2:
        auto_refresh = st.checkbox("è‡ªåŠ¨åˆ·æ–°", value=False)
    
    with col3:
        refresh_interval = st.selectbox("åˆ·æ–°é—´éš”", [5, 10, 30, 60], index=1)
    
    # å¦‚æœå¯ç”¨è‡ªåŠ¨åˆ·æ–°
    if auto_refresh:
        time.sleep(refresh_interval)
        st.experimental_rerun()
    
    # æ˜¾ç¤ºæ€§èƒ½å›¾è¡¨
    _show_performance_charts()

def _collect_performance_data():
    """æ”¶é›†æ€§èƒ½æ•°æ®"""
    try:
        # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®æ”¶é›†
        current_time = time.time()
        
        # è®¡ç®—å„æ¨¡å—çš„å¤„ç†æ—¶é—´
        performance_data = {
            'timestamp': current_time,
            'feature_engineering_time': np.random.normal(0.5, 0.1),
            'clustering_time': np.random.normal(1.2, 0.3),
            'risk_scoring_time': np.random.normal(0.8, 0.2),
            'attack_analysis_time': np.random.normal(0.6, 0.15),
            'total_latency': 0,
            'memory_usage': psutil.virtual_memory().percent,
            'cpu_usage': psutil.cpu_percent(),
            'accuracy': np.random.normal(0.85, 0.05)
        }
        
        # è®¡ç®—æ€»å»¶è¿Ÿ
        performance_data['total_latency'] = (
            performance_data['feature_engineering_time'] +
            performance_data['clustering_time'] +
            performance_data['risk_scoring_time'] +
            performance_data['attack_analysis_time']
        )
        
        # æ·»åŠ åˆ°å†å²è®°å½•
        st.session_state.performance_history.append(performance_data)
        
        # ä¿æŒæœ€è¿‘100æ¡è®°å½•
        if len(st.session_state.performance_history) > 100:
            st.session_state.performance_history = st.session_state.performance_history[-100:]
        
        st.success("âœ… æ€§èƒ½æ•°æ®å·²æ›´æ–°")
        
    except Exception as e:
        st.error(f"âŒ æ€§èƒ½æ•°æ®æ”¶é›†å¤±è´¥: {str(e)}")

def _show_performance_charts():
    """æ˜¾ç¤ºæ€§èƒ½å›¾è¡¨"""
    if not st.session_state.performance_history:
        st.info("ğŸ’¡ æš‚æ— æ€§èƒ½æ•°æ®ï¼Œè¯·ç‚¹å‡»åˆ·æ–°æŒ‰é’®æ”¶é›†æ•°æ®")
        return
    
    # å‡†å¤‡æ•°æ®
    df = pd.DataFrame(st.session_state.performance_history[-20:])  # æœ€è¿‘20æ¡è®°å½•
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
    
    # åˆ›å»ºå­å›¾
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('å¤„ç†å»¶è¿Ÿè¶‹åŠ¿', 'ç³»ç»Ÿèµ„æºä½¿ç”¨', 'æ¨¡å—å¤„ç†æ—¶é—´', 'é¢„æµ‹å‡†ç¡®ç‡'),
        specs=[[{"secondary_y": False}, {"secondary_y": True}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # 1. å¤„ç†å»¶è¿Ÿè¶‹åŠ¿
    fig.add_trace(
        go.Scatter(x=df['timestamp'], y=df['total_latency'], 
                  name='æ€»å»¶è¿Ÿ', line=dict(color='blue')),
        row=1, col=1
    )
    
    # 2. ç³»ç»Ÿèµ„æºä½¿ç”¨
    fig.add_trace(
        go.Scatter(x=df['timestamp'], y=df['memory_usage'], 
                  name='å†…å­˜ä½¿ç”¨%', line=dict(color='green')),
        row=1, col=2
    )
    fig.add_trace(
        go.Scatter(x=df['timestamp'], y=df['cpu_usage'], 
                  name='CPUä½¿ç”¨%', line=dict(color='red')),
        row=1, col=2
    )
    
    # 3. æ¨¡å—å¤„ç†æ—¶é—´
    modules = ['feature_engineering_time', 'clustering_time', 'risk_scoring_time', 'attack_analysis_time']
    module_names = ['ç‰¹å¾å·¥ç¨‹', 'èšç±»åˆ†æ', 'é£é™©è¯„åˆ†', 'æ”»å‡»åˆ†æ']
    colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']
    
    for i, (module, name, color) in enumerate(zip(modules, module_names, colors)):
        fig.add_trace(
            go.Scatter(x=df['timestamp'], y=df[module], 
                      name=name, line=dict(color=color)),
            row=2, col=1
        )
    
    # 4. é¢„æµ‹å‡†ç¡®ç‡
    fig.add_trace(
        go.Scatter(x=df['timestamp'], y=df['accuracy'], 
                  name='å‡†ç¡®ç‡', line=dict(color='purple')),
        row=2, col=2
    )
    
    fig.update_layout(height=600, showlegend=True, title_text="ç³»ç»Ÿæ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿")
    st.plotly_chart(fig, use_container_width=True)

def _show_prediction_performance():
    """æ˜¾ç¤ºé¢„æµ‹æ€§èƒ½ç»Ÿè®¡"""
    st.markdown("### ğŸ¯ é¢„æµ‹æ€§èƒ½ç»Ÿè®¡")
    
    # æ¨¡æ‹Ÿé¢„æµ‹æ€§èƒ½æ•°æ®
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # æ•´ä½“å‡†ç¡®ç‡
        overall_accuracy = np.random.normal(0.85, 0.02)
        st.metric("æ•´ä½“å‡†ç¡®ç‡", f"{overall_accuracy:.2%}", delta="+2.3%")
    
    with col2:
        # æ¬ºè¯ˆæ£€æµ‹ç‡
        fraud_detection_rate = np.random.normal(0.78, 0.03)
        st.metric("æ¬ºè¯ˆæ£€æµ‹ç‡", f"{fraud_detection_rate:.2%}", delta="+1.8%")
    
    with col3:
        # è¯¯æŠ¥ç‡
        false_positive_rate = np.random.normal(0.12, 0.02)
        st.metric("è¯¯æŠ¥ç‡", f"{false_positive_rate:.2%}", delta="-0.5%")
    
    with col4:
        # å¤„ç†ååé‡
        throughput = np.random.normal(1200, 100)
        st.metric("å¤„ç†ååé‡", f"{throughput:.0f}/å°æ—¶", delta="+150")
    
    # å››åˆ†ç±»æ€§èƒ½è¯¦æƒ…
    st.markdown("#### ğŸ“Š å››åˆ†ç±»æ€§èƒ½è¯¦æƒ…")
    
    # åˆ›å»ºæ··æ·†çŸ©é˜µçƒ­å›¾
    confusion_matrix = np.array([
        [0.92, 0.05, 0.02, 0.01],
        [0.08, 0.85, 0.06, 0.01],
        [0.03, 0.12, 0.80, 0.05],
        [0.01, 0.02, 0.15, 0.82]
    ])
    
    fig = px.imshow(
        confusion_matrix,
        labels=dict(x="é¢„æµ‹ç±»åˆ«", y="å®é™…ç±»åˆ«", color="å‡†ç¡®ç‡"),
        x=['ä½é£é™©', 'ä¸­é£é™©', 'é«˜é£é™©', 'æé«˜é£é™©'],
        y=['ä½é£é™©', 'ä¸­é£é™©', 'é«˜é£é™©', 'æé«˜é£é™©'],
        color_continuous_scale='Blues',
        title="å››åˆ†ç±»æ··æ·†çŸ©é˜µ"
    )
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨
    for i in range(4):
        for j in range(4):
            fig.add_annotation(
                x=j, y=i,
                text=f"{confusion_matrix[i][j]:.2f}",
                showarrow=False,
                font=dict(color="white" if confusion_matrix[i][j] > 0.5 else "black")
            )
    
    st.plotly_chart(fig, use_container_width=True)

def _show_system_resources():
    """æ˜¾ç¤ºç³»ç»Ÿèµ„æºç›‘æ§"""
    st.markdown("### ğŸ’» ç³»ç»Ÿèµ„æºç›‘æ§")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # å†…å­˜ä½¿ç”¨è¯¦æƒ…
        memory = psutil.virtual_memory()
        
        st.markdown("#### ğŸ’¾ å†…å­˜ä½¿ç”¨æƒ…å†µ")
        st.progress(memory.percent / 100)
        st.markdown(f"- **æ€»å†…å­˜**: {memory.total / (1024**3):.1f} GB")
        st.markdown(f"- **å·²ä½¿ç”¨**: {memory.used / (1024**3):.1f} GB ({memory.percent:.1f}%)")
        st.markdown(f"- **å¯ç”¨**: {memory.available / (1024**3):.1f} GB")
    
    with col2:
        # CPUä½¿ç”¨è¯¦æƒ…
        cpu_percent = psutil.cpu_percent(interval=1, percpu=True)
        
        st.markdown("#### ğŸ”¥ CPUä½¿ç”¨æƒ…å†µ")
        avg_cpu = np.mean(cpu_percent)
        st.progress(avg_cpu / 100)
        st.markdown(f"- **å¹³å‡ä½¿ç”¨ç‡**: {avg_cpu:.1f}%")
        st.markdown(f"- **CPUæ ¸å¿ƒæ•°**: {psutil.cpu_count()}")
        st.markdown(f"- **æœ€é«˜ä½¿ç”¨ç‡**: {max(cpu_percent):.1f}%")

def _show_performance_report():
    """æ˜¾ç¤ºæ€§èƒ½æŠ¥å‘Š"""
    st.markdown("### ğŸ“‹ æ€§èƒ½æŠ¥å‘Š")
    
    if not st.session_state.performance_history:
        st.info("ğŸ’¡ æš‚æ— æ€§èƒ½æ•°æ®ç”¨äºç”ŸæˆæŠ¥å‘Š")
        return
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    df = pd.DataFrame(st.session_state.performance_history)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ“Š æ€§èƒ½ç»Ÿè®¡")
        st.markdown(f"- **å¹³å‡å»¶è¿Ÿ**: {df['total_latency'].mean():.2f}s")
        st.markdown(f"- **æœ€å¤§å»¶è¿Ÿ**: {df['total_latency'].max():.2f}s")
        st.markdown(f"- **æœ€å°å»¶è¿Ÿ**: {df['total_latency'].min():.2f}s")
        st.markdown(f"- **å»¶è¿Ÿæ ‡å‡†å·®**: {df['total_latency'].std():.2f}s")
    
    with col2:
        st.markdown("#### ğŸ¯ æ€§èƒ½å»ºè®®")
        avg_latency = df['total_latency'].mean()
        avg_memory = df['memory_usage'].mean()
        avg_cpu = df['cpu_usage'].mean()
        
        if avg_latency > 3.0:
            st.warning("âš ï¸ å¹³å‡å»¶è¿Ÿè¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–ç®—æ³•")
        else:
            st.success("âœ… å»¶è¿Ÿæ€§èƒ½è‰¯å¥½")
        
        if avg_memory > 80:
            st.warning("âš ï¸ å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®å¢åŠ å†…å­˜")
        else:
            st.success("âœ… å†…å­˜ä½¿ç”¨æ­£å¸¸")
        
        if avg_cpu > 70:
            st.warning("âš ï¸ CPUä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–è®¡ç®—")
        else:
            st.success("âœ… CPUä½¿ç”¨æ­£å¸¸")
    
    # å¯¼å‡ºæŠ¥å‘Š
    if st.button("ğŸ“¥ å¯¼å‡ºæ€§èƒ½æŠ¥å‘Š"):
        report_data = {
            'timestamp': pd.Timestamp.now(),
            'avg_latency': df['total_latency'].mean(),
            'max_latency': df['total_latency'].max(),
            'avg_memory_usage': df['memory_usage'].mean(),
            'avg_cpu_usage': df['cpu_usage'].mean(),
            'total_records': len(df)
        }
        
        report_df = pd.DataFrame([report_data])
        csv = report_df.to_csv(index=False)
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½æ€§èƒ½æŠ¥å‘Š",
            data=csv,
            file_name=f"performance_report_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
        
        st.success("âœ… æ€§èƒ½æŠ¥å‘Šå·²å‡†å¤‡ä¸‹è½½")
