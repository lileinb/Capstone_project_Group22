"""
Clustering Analysis Page
Responsible for user behavior pattern clustering and anomalous group identification
"""

import streamlit as st
import pandas as pd
import numpy as np
import sys
import os
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å¯¼å…¥åç«¯æ¨¡å—
from backend.clustering.cluster_analyzer import ClusterAnalyzer
from backend.clustering.cluster_interpreter import ClusterInterpreter

def show():
    """Display clustering analysis page"""
    st.markdown('<div class="sub-header">ğŸ“Š Clustering Analysis & Anomalous Group Identification</div>', unsafe_allow_html=True)

    # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å¾å·¥ç¨‹æ•°æ®
    if 'engineered_features' not in st.session_state or st.session_state.engineered_features is None:
        st.warning("âš ï¸ Please complete feature engineering first!")
        st.info("ğŸ’¡ Please complete feature generation on the 'ğŸ”§ Feature Engineering' page")
        return
    
    # åˆå§‹åŒ–session state
    if 'clustering_results' not in st.session_state:
        st.session_state.clustering_results = None
    if 'cluster_labels' not in st.session_state:
        st.session_state.cluster_labels = None
    if 'cluster_analysis' not in st.session_state:
        st.session_state.cluster_analysis = None
    
    # è·å–ç‰¹å¾å·¥ç¨‹æ•°æ®
    engineered_data = st.session_state.engineered_features
    
    st.markdown("### ğŸ“Š Data Overview")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Records", f"{len(engineered_data):,}")

    with col2:
        st.metric("Features", f"{len(engineered_data.columns)}")

    with col3:
        numeric_features = len(engineered_data.select_dtypes(include=['number']).columns)
        st.metric("Numerical Features", f"{numeric_features}")

    with col4:
        if 'is_fraudulent' in engineered_data.columns:
            fraud_rate = (engineered_data['is_fraudulent'].sum() / len(engineered_data) * 100).round(2)
            st.metric("Fraud Rate", f"{fraud_rate}%")
        else:
            st.metric("Fraud Rate", "N/A")

    # èšç±»åˆ†æåŒºåŸŸ
    st.markdown("### ğŸ” Clustering Analysis Configuration")

    st.markdown("""
    **Clustering Algorithm Description:**
    - **K-means Clustering**: Distance-based hard clustering, suitable for discovering spherical clusters
    - **DBSCAN Clustering**: Density-based clustering, suitable for discovering irregularly shaped clusters
    - **Gaussian Mixture Model**: Probabilistic clustering method, provides soft clustering results
    """)
    
    # æ™ºèƒ½èšç±»é€‰é¡¹
    st.markdown("### ğŸ¤– Intelligent Clustering Mode")

    # æ·»åŠ æ™ºèƒ½èšç±»é€‰æ‹©
    clustering_mode = st.radio(
        "Select Clustering Mode",
        ["ğŸ¤– Intelligent Auto Clustering (Recommended)", "âš™ï¸ Manual Parameter Adjustment"],
        help="Intelligent mode automatically selects optimal features, algorithms and parameters, manual mode allows custom configuration"
    )

    if clustering_mode == "ğŸ¤– Intelligent Auto Clustering (Recommended)":
        st.info("""
        ğŸ¯ **Intelligent Clustering Mode Features**:
        - ğŸ” Automatic feature selection and engineering
        - âš™ï¸ Automatic algorithm and parameter optimization
        - ğŸ“Š Automatic risk threshold adjustment
        - ğŸ¯ Ensure four-tier risk distribution
        - ğŸ“ˆ Maximize clustering quality metrics
        """)

        # ç›®æ ‡é£é™©åˆ†å¸ƒè®¾ç½®
        st.markdown("#### ğŸ¯ Target Risk Distribution")
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            target_low = st.slider("Low Risk Ratio", 0.2, 0.8, 0.5, 0.05, help="Target low risk user ratio")
        with col2:
            target_medium = st.slider("Medium Risk Ratio", 0.1, 0.5, 0.3, 0.05, help="Target medium risk user ratio")
        with col3:
            target_high = st.slider("High Risk Ratio", 0.05, 0.4, 0.15, 0.05, help="Target high risk user ratio")
        with col4:
            target_critical = st.slider("Critical Risk Ratio", 0.01, 0.2, 0.05, 0.01, help="Target critical risk user ratio")

        # ç¡®ä¿æ¯”ä¾‹æ€»å’Œä¸º1
        total = target_low + target_medium + target_high + target_critical
        if abs(total - 1.0) > 0.01:
            st.warning(f"âš ï¸ Risk ratio sum should be 100%, currently {total*100:.1f}%")

        target_risk_distribution = {
            'low': target_low,
            'medium': target_medium,
            'high': target_high,
            'critical': target_critical
        }

        # æ™ºèƒ½èšç±»æŒ‰é’®
        if st.button("ğŸš€ Start Intelligent Clustering", type="primary", help="One-click execution of optimal clustering analysis"):
            with st.spinner("ğŸ”„ Executing intelligent clustering optimization..."):
                try:
                    analyzer = ClusterAnalyzer()
                    result = analyzer.intelligent_auto_clustering(
                        engineered_data, target_risk_distribution
                    )

                    # å­˜å‚¨æ™ºèƒ½èšç±»ç»“æœ
                    st.session_state.clustering_result = result

                    # åŒæ—¶ä»¥æ ‡å‡†æ ¼å¼å­˜å‚¨ï¼Œä¾›å…¶ä»–æ¨¡å—ä½¿ç”¨
                    standard_clustering_results = {
                        'algorithm': result.get('algorithm', 'kmeans'),
                        'n_clusters': result.get('n_clusters', 3),
                        'cluster_count': result.get('n_clusters', 3),
                        'cluster_details': result.get('cluster_details', []),
                        'cluster_labels': result.get('cluster_labels', []),
                        'silhouette_score': result.get('silhouette_score', 0),
                        'calinski_harabasz_score': result.get('calinski_harabasz_score', 0),
                        'selected_features': result.get('selected_features', []),
                        'optimal_thresholds': result.get('optimal_thresholds', {}),
                        'risk_mapping': result.get('risk_mapping', {}),
                        'optimization_summary': result.get('optimization_summary', {}),
                        'source': 'intelligent_clustering'
                    }

                    st.session_state.clustering_results = standard_clustering_results
                    st.session_state.cluster_labels = result.get('cluster_labels', [])
                    st.session_state.cluster_analysis = {
                        'algorithm': result.get('algorithm', 'kmeans'),
                        'features': result.get('selected_features', []),
                        'n_clusters': result.get('n_clusters', 3),
                        'cluster_sizes': {},
                        'source': 'intelligent_clustering'
                    }

                    st.success("âœ… Intelligent clustering completed!")
                    st.rerun()

                except Exception as e:
                    st.error(f"âŒ Intelligent clustering failed: {e}")

        # å¦‚æœæœ‰æ™ºèƒ½èšç±»ç»“æœï¼Œæ˜¾ç¤ºä¼˜åŒ–æ‘˜è¦
        if ('clustering_result' in st.session_state and
            st.session_state.clustering_result is not None and
            'optimization_summary' in st.session_state.clustering_result):
            result = st.session_state.clustering_result
            summary = result['optimization_summary']

            st.markdown("### ğŸ“Š Intelligent Optimization Summary")

            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Silhouette Score", f"{summary['silhouette_score']:.3f}")
            with col2:
                st.metric("Cluster Count", summary['n_clusters'])
            with col3:
                st.metric("Selected Features", summary['feature_count'])
            with col4:
                st.metric("Algorithm", summary['algorithm'].upper())

            # æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
            if 'recommendations' in result:
                st.markdown("#### ğŸ’¡ Optimization Recommendations")
                for rec in result['recommendations']:
                    st.write(f"- {rec}")

            # æ˜¾ç¤ºé€‰æ‹©çš„ç‰¹å¾
            if 'selected_features' in result:
                st.markdown("#### ğŸ¯ Auto-Selected Features")
                st.write(", ".join(result['selected_features']))

            # æ˜¾ç¤ºä¼˜åŒ–åçš„é˜ˆå€¼
            if 'optimal_thresholds' in result:
                st.markdown("#### âš™ï¸ Optimized Risk Thresholds")
                thresholds = result['optimal_thresholds']
                st.write(f"Low Risk: <{thresholds['low']}, Medium Risk: {thresholds['low']}-{thresholds['medium']}, "
                        f"High Risk: {thresholds['medium']}-{thresholds['high']}, Critical Risk: >{thresholds['high']}")

        return  # æ™ºèƒ½æ¨¡å¼ä¸‹ä¸éœ€è¦æ‰‹åŠ¨å‚æ•°è®¾ç½®

    # æ‰‹åŠ¨å‚æ•°è°ƒæ•´æ¨¡å¼
    st.markdown("### âš™ï¸ Manual Parameter Adjustment")

    # èšç±»å‚æ•°è®¾ç½®
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("#### âš™ï¸ Clustering Algorithm Selection")

        # ç®—æ³•é€‰æ‹©
        algorithm = st.selectbox(
            "Select Clustering Algorithm",
            ["K-means", "DBSCAN", "Gaussian Mixture"],
            help="Choose clustering algorithm suitable for data characteristics"
        )

        # ç‰¹å¾é€‰æ‹©
        numeric_cols = engineered_data.select_dtypes(include=['number']).columns.tolist()
        if 'is_fraudulent' in numeric_cols:
            numeric_cols.remove('is_fraudulent')  # æ’é™¤æ ‡ç­¾åˆ—

        selected_features = st.multiselect(
            "Select Clustering Features",
            numeric_cols,
            default=numeric_cols[:min(10, len(numeric_cols))],
            help="Select numerical features for clustering"
        )
    
    with col2:
        st.markdown("#### ğŸ“Š Algorithm Parameters")

        # é»˜è®¤å‚æ•°
        n_clusters = 5
        random_state = 42

        if algorithm == "K-means":
            n_clusters = st.slider("Number of Clusters", 2, 10, 5, help="K-means cluster count")
            max_iter = st.slider("Max Iterations", 100, 1000, 300, help="Maximum number of iterations")
            random_state = st.slider("Random Seed", 0, 100, 42, help="Random seed")

        elif algorithm == "DBSCAN":
            eps = st.slider("Neighborhood Radius", 0.1, 2.0, 0.5, 0.1, help="DBSCAN neighborhood radius")
            min_samples = st.slider("Min Samples", 2, 20, 5, help="Minimum samples required to form a core point")
            n_clusters = 5  # DBSCANä¸éœ€è¦é¢„è®¾èšç±»æ•°ï¼Œä½†ClusterAnalyzeréœ€è¦è¿™ä¸ªå‚æ•°

        elif algorithm == "Gaussian Mixture":
            n_clusters = st.slider("Number of Components", 2, 10, 5, help="Number of Gaussian mixture components")
            max_iter = st.slider("Max Iterations", 100, 1000, 300, help="Maximum number of iterations")
            random_state = st.slider("Random Seed", 0, 100, 42, help="Random seed")
    
    # æ‰§è¡Œèšç±»åˆ†æ
    col1, col2 = st.columns([3, 1])

    with col1:
        run_clustering = st.button("ğŸš€ Execute Clustering Analysis", type="primary", help="Perform clustering analysis based on current parameters")

    with col2:
        clear_cache = st.button("ğŸ—‘ï¸ Clear Cache", help="Clear previous clustering results")

    if clear_cache:
        # æ¸…é™¤session stateä¸­çš„èšç±»ç›¸å…³æ•°æ®
        keys_to_clear = ['clustering_results', 'cluster_labels', 'clustering_analysis', 'cluster_analysis']
        for key in keys_to_clear:
            if key in st.session_state:
                del st.session_state[key]
        st.success("âœ… Cache cleared!")
        st.rerun()

    if run_clustering:
        try:
            with st.spinner("Performing clustering analysis..."):
                # å‡†å¤‡æ•°æ®
                clustering_data = engineered_data[selected_features].copy()

                # æ•°æ®æ ‡å‡†åŒ–
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                clustering_data_scaled = scaler.fit_transform(clustering_data)

                # åˆ›å»ºèšç±»åˆ†æå™¨
                cluster_analyzer = ClusterAnalyzer(n_clusters=n_clusters, random_state=random_state)

                # æ‰§è¡Œèšç±»ï¼ˆä½¿ç”¨æ–°çš„APIï¼‰
                algorithm_map = {
                    "K-means": "kmeans",
                    "DBSCAN": "dbscan",
                    "Gaussian Mixture": "gaussian_mixture"
                }

                clustering_results = cluster_analyzer.analyze_clusters(
                    engineered_data,
                    algorithm=algorithm_map.get(algorithm, "kmeans")
                )

                # æå–èšç±»æ ‡ç­¾
                cluster_labels = clustering_results.get('cluster_labels', [])
                
                # ä¿å­˜èšç±»ç»“æœ
                st.session_state.cluster_labels = cluster_labels
                st.session_state.clustering_results = clustering_results
                st.session_state.clustering_analysis = {
                    'algorithm': algorithm,
                    'features': selected_features,
                    'n_clusters': len(np.unique(cluster_labels)),
                    'cluster_sizes': pd.Series(cluster_labels).value_counts().to_dict()
                }
                
                # èšç±»è§£é‡Š
                cluster_interpreter = ClusterInterpreter()
                cluster_analysis = cluster_interpreter.analyze_clusters(
                    engineered_data, cluster_labels, selected_features
                )
                st.session_state.cluster_analysis = cluster_analysis
                
                st.success("âœ… Clustering analysis completed!")

                # è°ƒè¯•ä¿¡æ¯
                with st.expander("ğŸ” Debug Information", expanded=False):
                    st.write("Clustering result keys:", list(clustering_results.keys()))
                    st.write("Cluster count:", clustering_results.get('cluster_count', 0))

                    cluster_details = clustering_results.get('cluster_details', [])
                    st.write("Cluster details count:", len(cluster_details))

                    for i, detail in enumerate(cluster_details):
                        st.write(f"Cluster {i}:")
                        st.write(f"  - Size: {detail.get('size', 0)}")
                        st.write(f"  - Fraud rate: {detail.get('fraud_rate', 0):.4f}")
                        st.write(f"  - Risk level: {detail.get('risk_level', 'unknown')}")

        except Exception as e:
            st.error(f"âŒ Clustering analysis failed: {e}")
            st.exception(e)
    
    # æ˜¾ç¤ºèšç±»ç»“æœ
    if st.session_state.clustering_results is not None:
        st.markdown("### ğŸ“ˆ Clustering Analysis Results")

        clustering_results = st.session_state.clustering_results
        cluster_labels = st.session_state.cluster_labels
        cluster_analysis = st.session_state.cluster_analysis

        # èšç±»ç»Ÿè®¡
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("Algorithm", clustering_results['algorithm'])

        with col2:
            st.metric("Cluster Count", clustering_results.get('cluster_count', 0))

        with col3:
            st.metric("Feature Count", len(selected_features))

        with col4:
            total_records = len(cluster_labels)
            st.metric("Total Records", f"{total_records:,}")

        # èšç±»åˆ†å¸ƒ
        st.markdown("#### ğŸ“Š Cluster Distribution")
        
        cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()
        
        # èšç±»å¤§å°åˆ†å¸ƒå›¾
        fig = px.bar(
            x=cluster_sizes.index,
            y=cluster_sizes.values,
            title="Cluster Size Distribution",
            labels={'x': 'Cluster Label', 'y': 'Record Count'}
        )
        st.plotly_chart(fig, use_container_width=True)

        # èšç±»å¤§å°è¡¨æ ¼
        cluster_df = pd.DataFrame({
            'Cluster Label': cluster_sizes.index,
            'Record Count': cluster_sizes.values,
            'Percentage (%)': (cluster_sizes.values / total_records * 100).round(2)
        })
        st.dataframe(cluster_df, use_container_width=True)
        
        # èšç±»ç‰¹å¾åˆ†æ
        if clustering_results and 'cluster_details' in clustering_results:
            st.markdown("#### ğŸ” Cluster Feature Analysis")

            cluster_details = clustering_results['cluster_details']

            # èšç±»è¯¦æƒ…è¡¨æ ¼
            if cluster_details:
                st.markdown("**Cluster Details**")

                details_data = []
                for detail in cluster_details:
                    details_data.append({
                        'Cluster ID': detail.get('cluster_id', 'N/A'),
                        'Size': detail.get('size', 0),
                        'Percentage (%)': detail.get('percentage', 0),
                        'Avg Transaction Amount': detail.get('avg_transaction_amount', 0),
                        'Avg Customer Age': detail.get('avg_customer_age', 0),
                        'Common Payment Method': detail.get('common_payment_method', 'unknown'),
                        'Common Device': detail.get('common_device', 'unknown'),
                        'Fraud Rate': detail.get('fraud_rate', 0),
                        'Risk Level': detail.get('risk_level', 'low')
                    })

                details_df = pd.DataFrame(details_data)
                st.dataframe(details_df, use_container_width=True)

                # èšç±»é£é™©ç­‰çº§åˆ†å¸ƒ - ä¿®å¤ï¼šä½¿ç”¨é£é™©æ˜ å°„å™¨çš„ç»“æœ
                # ä¼˜å…ˆä½¿ç”¨é£é™©æ˜ å°„å™¨çš„ç»“æœï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨cluster_detailsä¸­çš„é£é™©ç­‰çº§
                cluster_risk_mapping = clustering_results.get('cluster_risk_mapping', {})

                if cluster_risk_mapping:
                    # ä½¿ç”¨é£é™©æ˜ å°„å™¨çš„ç»“æœ
                    risk_levels = []
                    for detail in cluster_details:
                        cluster_id = detail.get('cluster_id', -1)
                        if cluster_id in cluster_risk_mapping:
                            risk_level = cluster_risk_mapping[cluster_id].get('risk_level', 'low')
                        else:
                            risk_level = detail.get('risk_level', 'low')
                        risk_levels.append(risk_level)
                else:
                    # å›é€€åˆ°cluster_detailsä¸­çš„é£é™©ç­‰çº§
                    risk_levels = [detail.get('risk_level', 'low') for detail in cluster_details]

                risk_counts = pd.Series(risk_levels).value_counts()

                if len(risk_counts) > 0:
                    # å®šä¹‰é£é™©ç­‰çº§é¢œè‰²
                    risk_colors = {
                        'low': '#22c55e',      # ç»¿è‰²
                        'medium': '#f59e0b',   # é»„è‰²
                        'high': '#f97316',     # æ©™è‰²
                        'critical': '#ef4444'  # çº¢è‰²
                    }

                    fig = px.pie(
                        values=risk_counts.values,
                        names=risk_counts.index,
                        title="Cluster Risk Level Distribution",
                        color=risk_counts.index,
                        color_discrete_map=risk_colors
                    )
                    st.plotly_chart(fig, use_container_width=True)

                    # æ˜¾ç¤ºé£é™©åˆ†å¸ƒç»Ÿè®¡
                    st.markdown("**Risk Distribution Statistics:**")
                    for level, count in risk_counts.items():
                        percentage = count / len(risk_levels) * 100
                        st.markdown(f"- {level.title()}: {count} clusters ({percentage:.1f}%)")
            
        # èšç±»è´¨é‡è¯„ä¼°
        if clustering_results and 'quality_metrics' in clustering_results:
            st.markdown("#### ğŸ“Š Clustering Quality Assessment")

            quality_metrics = clustering_results['quality_metrics']

            col1, col2 = st.columns(2)

            with col1:
                silhouette_score = quality_metrics.get('silhouette_score', 0)
                st.metric(
                    "Silhouette Score",
                    f"{silhouette_score:.3f}",
                    help="Range [-1,1], closer to 1 indicates better clustering"
                )

            with col2:
                calinski_score = quality_metrics.get('calinski_harabasz_score', 0)
                st.metric(
                    "Calinski-Harabasz Index",
                    f"{calinski_score:.1f}",
                    help="Higher values indicate better clustering"
                )
        
        # èšç±»å¯è§†åŒ–
        st.markdown("#### ğŸ¨ Clustering Visualization")

        # é€‰æ‹©å¯è§†åŒ–ç‰¹å¾
        if len(selected_features) >= 2:
            col1, col2 = st.columns(2)

            with col1:
                x_feature = st.selectbox("X-axis Feature", selected_features, index=0)

            with col2:
                y_feature = st.selectbox("Y-axis Feature", selected_features, index=1)

            # åˆ›å»ºæ•£ç‚¹å›¾
            plot_data = pd.DataFrame({
                'x': engineered_data[x_feature],
                'y': engineered_data[y_feature],
                'cluster': cluster_labels
            })

            fig = px.scatter(
                plot_data,
                x='x',
                y='y',
                color='cluster',
                title=f"Clustering Results Scatter Plot ({x_feature} vs {y_feature})",
                labels={'x': x_feature, 'y': y_feature}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # å¼‚å¸¸èšç±»è¯†åˆ«
        st.markdown("#### âš ï¸ å¼‚å¸¸èšç±»è¯†åˆ«")

        if clustering_results and 'anomaly_analysis' in clustering_results:
            anomaly_analysis = clustering_results['anomaly_analysis']

            # é«˜é£é™©èšç±»
            high_risk_clusters = anomaly_analysis.get('high_risk_clusters', [])
            if high_risk_clusters:
                st.warning(f"å‘ç° {len(high_risk_clusters)} ä¸ªé«˜é£é™©èšç±»")

                for cluster_info in high_risk_clusters:
                    with st.expander(f"é«˜é£é™©èšç±» {cluster_info['cluster_id']}"):
                        st.markdown(f"**æ¬ºè¯ˆç‡**: {cluster_info['fraud_rate']:.3f}")
                        st.markdown(f"**èšç±»å¤§å°**: {cluster_info['size']} æ¡è®°å½•")

            # å°èšç±»
            small_clusters = anomaly_analysis.get('small_clusters', [])
            if small_clusters:
                st.info(f"å‘ç° {len(small_clusters)} ä¸ªå°èšç±»ï¼ˆå¯èƒ½çš„å¼‚å¸¸æ¨¡å¼ï¼‰")

                for cluster_info in small_clusters:
                    with st.expander(f"å°èšç±» {cluster_info['cluster_id']}"):
                        st.markdown(f"**èšç±»å¤§å°**: {cluster_info['size']} æ¡è®°å½•")
                        st.markdown(f"**å æ¯”**: {cluster_info['percentage']:.2f}%")

            if not high_risk_clusters and not small_clusters:
                st.success("âœ… æœªå‘ç°æ˜æ˜¾å¼‚å¸¸èšç±»")
        
        # èšç±»ä¸æ¬ºè¯ˆå…³ç³»åˆ†æ
        if 'is_fraudulent' in engineered_data.columns:
            st.markdown("#### ğŸ¯ èšç±»ä¸æ¬ºè¯ˆå…³ç³»åˆ†æ")
            
            # è®¡ç®—æ¯ä¸ªèšç±»çš„æ¬ºè¯ˆç‡
            fraud_by_cluster = engineered_data.groupby(cluster_labels)['is_fraudulent'].agg(['count', 'sum', 'mean'])
            fraud_by_cluster.columns = ['æ€»è®°å½•æ•°', 'æ¬ºè¯ˆè®°å½•æ•°', 'æ¬ºè¯ˆç‡']
            fraud_by_cluster['æ¬ºè¯ˆç‡(%)'] = (fraud_by_cluster['æ¬ºè¯ˆç‡'] * 100).round(2)
            
            # æ¬ºè¯ˆç‡åˆ†å¸ƒå›¾
            fig = px.bar(
                x=fraud_by_cluster.index,
                y=fraud_by_cluster['æ¬ºè¯ˆç‡(%)'],
                title="å„èšç±»æ¬ºè¯ˆç‡åˆ†å¸ƒ",
                labels={'x': 'èšç±»æ ‡ç­¾', 'y': 'æ¬ºè¯ˆç‡(%)'}
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # æ¬ºè¯ˆç‡è¡¨æ ¼
            st.dataframe(fraud_by_cluster, use_container_width=True)
        
        # æ•°æ®é¢„è§ˆ
        st.markdown("#### ğŸ“‹ èšç±»ç»“æœé¢„è§ˆ")
        
        # æ·»åŠ èšç±»æ ‡ç­¾åˆ°æ•°æ®
        result_data = engineered_data.copy()
        result_data['cluster_label'] = cluster_labels
        
        st.dataframe(result_data.head(10), use_container_width=True)
        
        # ä¸‹ä¸€æ­¥æŒ‰é’®
        st.markdown("---")
        col1, col2, col3 = st.columns([1, 1, 1])
        
        with col2:
            if st.button("ğŸš€ è¿›å…¥é£é™©è¯„åˆ†", type="primary", use_container_width=True):
                st.success("âœ… èšç±»åˆ†æå®Œæˆï¼Œå¯ä»¥è¿›å…¥é£é™©è¯„åˆ†é¡µé¢ï¼")
                st.info("ğŸ’¡ è¯·åœ¨ä¾§è¾¹æ é€‰æ‹©'ğŸ¯ é£é™©è¯„åˆ†'é¡µé¢ç»§ç»­")
    
    else:
        # æ˜¾ç¤ºèšç±»åˆ†æè¯´æ˜
        st.markdown("### ğŸ“ èšç±»åˆ†æè¯´æ˜")
        
        st.markdown("""
        **èšç±»ç®—æ³•ç‰¹ç‚¹ï¼š**
        
        1. **K-meansèšç±»**
           - åŸºäºæ¬§å‡ é‡Œå¾—è·ç¦»çš„ç¡¬èšç±»
           - é€‚åˆå‘ç°çƒå½¢æˆ–å‡¸å½¢èšç±»
           - è®¡ç®—é€Ÿåº¦å¿«ï¼Œç»“æœç¨³å®š
           - éœ€è¦é¢„å…ˆæŒ‡å®šèšç±»æ•°é‡
        
        2. **DBSCANèšç±»**
           - åŸºäºå¯†åº¦çš„èšç±»ç®—æ³•
           - èƒ½å‘ç°ä¸è§„åˆ™å½¢çŠ¶çš„èšç±»
           - è‡ªåŠ¨è¯†åˆ«å¼‚å¸¸ç‚¹
           - ä¸éœ€è¦é¢„å…ˆæŒ‡å®šèšç±»æ•°é‡
        
        3. **é«˜æ–¯æ··åˆæ¨¡å‹**
           - æ¦‚ç‡èšç±»æ–¹æ³•
           - æä¾›è½¯èšç±»ç»“æœ
           - èƒ½å¤„ç†é‡å èšç±»
           - é€‚åˆå¤æ‚çš„æ•°æ®åˆ†å¸ƒ
        
        **èšç±»åˆ†æç›®æ ‡ï¼š**
        - å‘ç°ç”¨æˆ·è¡Œä¸ºæ¨¡å¼
        - è¯†åˆ«å¼‚å¸¸ç¾¤ä½“
        - ä¸ºé£é™©è¯„åˆ†æä¾›ä¾æ®
        - æ”¯æŒä¸ªæ€§åŒ–é£é™©ç­–ç•¥
        """) 