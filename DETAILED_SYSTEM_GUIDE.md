# ç”µå•†æ¬ºè¯ˆé£Žé™©é¢„æµ‹ç³»ç»Ÿ - è¯¦ç»†ä½¿ç”¨æŒ‡å—

## ðŸ“‹ ç›®å½•
- [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
- [åŠŸèƒ½æ¨¡å—è¯¦è§£](#åŠŸèƒ½æ¨¡å—è¯¦è§£)
- [æ ¸å¿ƒç®—æ³•å®žçŽ°](#æ ¸å¿ƒç®—æ³•å®žçŽ°)
- [å‰ç«¯ç•Œé¢åŠŸèƒ½](#å‰ç«¯ç•Œé¢åŠŸèƒ½)
- [åŽç«¯æŠ€æœ¯æž¶æž„](#åŽç«¯æŠ€æœ¯æž¶æž„)
- [ä½¿ç”¨æµç¨‹æŒ‡å—](#ä½¿ç”¨æµç¨‹æŒ‡å—)

## ðŸŽ¯ ç³»ç»Ÿæ¦‚è¿°

ç”µå•†æ¬ºè¯ˆé£Žé™©é¢„æµ‹ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºŽæœºå™¨å­¦ä¹ çš„æ™ºèƒ½é£ŽæŽ§å¹³å°ï¼Œé‡‡ç”¨å¤šç­–ç•¥èžåˆçš„æ–¹æ³•è¿›è¡Œæ¬ºè¯ˆæ£€æµ‹ã€‚ç³»ç»Ÿé€šè¿‡æ— ç›‘ç£å­¦ä¹ ã€åŠç›‘ç£å­¦ä¹ å’Œç›‘ç£å­¦ä¹ ç›¸ç»“åˆçš„æ–¹å¼ï¼Œå®žçŽ°å¯¹ç”µå•†äº¤æ˜“çš„å…¨æ–¹ä½é£Žé™©è¯„ä¼°ã€‚

### æ ¸å¿ƒç‰¹æ€§
- **å¤šç»´åº¦é£Žé™©è¯„ä¼°**: ç»“åˆèšç±»åˆ†æžã€ä¸“å®¶è§„åˆ™å’Œæœºå™¨å­¦ä¹ æ¨¡åž‹
- **ä¼ªæ ‡ç­¾ç”Ÿæˆ**: åŸºäºŽå¤šç­–ç•¥çš„é«˜è´¨é‡ä¼ªæ ‡ç­¾ç”Ÿæˆ
- **å®žæ—¶é£Žé™©è¯„åˆ†**: åŠ¨æ€é˜ˆå€¼ç®¡ç†å’Œé£Žé™©ç­‰çº§åˆ’åˆ†
- **æ”»å‡»ç±»åž‹è¯†åˆ«**: æ™ºèƒ½è¯†åˆ«4ç§ä¸»è¦æ”»å‡»æ¨¡å¼
- **å¯è§£é‡Šæ€§åˆ†æž**: SHAP/LIMEæ·±åº¦è§£é‡Šæ¨¡åž‹å†³ç­–

## ðŸ”§ åŠŸèƒ½æ¨¡å—è¯¦è§£

### 1. æ•°æ®ä¸Šä¼ ä¸Žé¢„å¤„ç†æ¨¡å—

#### å‰ç«¯ç•Œé¢åŠŸèƒ½
**æ–‡ä»¶ä½ç½®**: `frontend/pages/upload_page.py`

**ä¸»è¦åŠŸèƒ½**:
- ðŸ“ CSVæ–‡ä»¶ä¸Šä¼ å’ŒéªŒè¯
- ðŸ“Š æ•°æ®è´¨é‡æ£€æµ‹å’ŒæŠ¥å‘Š
- ðŸ”§ è‡ªåŠ¨æ•°æ®æ¸…ç†å’Œé¢„å¤„ç†
- ðŸ“ˆ æ•°æ®åˆ†å¸ƒå¯è§†åŒ–

**ç•Œé¢æ“ä½œæµç¨‹**:
1. é€‰æ‹©æ•°æ®æºï¼ˆä¸Šä¼ æ–‡ä»¶æˆ–ä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼‰
2. ç³»ç»Ÿè‡ªåŠ¨éªŒè¯æ•°æ®æ ¼å¼å’Œå®Œæ•´æ€§
3. æ˜¾ç¤ºæ•°æ®è´¨é‡æŠ¥å‘Šå’Œæ¸…ç†å»ºè®®
4. æ‰§è¡Œæ•°æ®æ¸…ç†å’Œæ ‡å‡†åŒ–å¤„ç†

#### åŽç«¯æŠ€æœ¯å®žçŽ°
**æ–‡ä»¶ä½ç½®**: `backend/data_processor/`

**æ ¸å¿ƒç±»å’Œæ–¹æ³•**:
```python
# backend/data_processor/data_loader.py
class DataLoader:
    def load_csv_file(self, file_path: str) -> pd.DataFrame
        """åŠ è½½CSVæ–‡ä»¶å¹¶è¿›è¡ŒåŸºç¡€éªŒè¯"""
    
    def validate_data_format(self, data: pd.DataFrame) -> bool
        """éªŒè¯æ•°æ®æ ¼å¼æ˜¯å¦ç¬¦åˆè¦æ±‚"""
    
    def get_data_summary(self, data: pd.DataFrame) -> Dict
        """ç”Ÿæˆæ•°æ®æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯"""

# backend/data_processor/data_cleaner.py  
class DataCleaner:
    def clean_data(self, data: pd.DataFrame) -> pd.DataFrame
        """å®Œæ•´çš„æ•°æ®æ¸…ç†æµç¨‹"""
    
    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame
        """å¤„ç†ç¼ºå¤±å€¼ï¼šæ™ºèƒ½å¡«å……ç­–ç•¥"""
    
    def _handle_duplicates(self, df: pd.DataFrame) -> pd.DataFrame
        """å¤„ç†é‡å¤æ•°æ®ï¼šåŸºäºŽå…³é”®å­—æ®µåŽ»é‡"""
    
    def _handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame
        """å¤„ç†å¼‚å¸¸å€¼ï¼šIQRæ–¹æ³•æ£€æµ‹å’Œå¤„ç†"""
```

**æŠ€æœ¯å®žçŽ°ç»†èŠ‚**:
- **æ•°æ®éªŒè¯**: æ£€æŸ¥å¿…éœ€å­—æ®µã€æ•°æ®ç±»åž‹ã€å–å€¼èŒƒå›´
- **ç¼ºå¤±å€¼å¤„ç†**: åŸºäºŽå­—æ®µç±»åž‹çš„æ™ºèƒ½å¡«å……ç­–ç•¥
- **å¼‚å¸¸å€¼æ£€æµ‹**: IQRæ–¹æ³•æ£€æµ‹å’Œå¤„ç†å¼‚å¸¸å€¼
- **é‡å¤æ•°æ®**: åŸºäºŽå…³é”®å­—æ®µçš„åŽ»é‡é€»è¾‘

### 2. ç‰¹å¾å·¥ç¨‹æ¨¡å—

#### å‰ç«¯ç•Œé¢åŠŸèƒ½
**æ–‡ä»¶ä½ç½®**: `frontend/pages/feature_analysis_page.py`

**ä¸»è¦åŠŸèƒ½**:
- ðŸ”§ è‡ªåŠ¨é£Žé™©ç‰¹å¾ç”Ÿæˆ
- ðŸ“Š ç‰¹å¾é‡è¦æ€§åˆ†æž
- ðŸ“ˆ ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–
- ðŸŽ¯ ç‰¹å¾é€‰æ‹©å’Œä¼˜åŒ–

**ç•Œé¢æ“ä½œæµç¨‹**:
1. é€‰æ‹©ç‰¹å¾å·¥ç¨‹ç­–ç•¥ï¼ˆæ ‡å‡†/å¿«é€Ÿæ¨¡å¼ï¼‰
2. ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆå¤šç»´åº¦é£Žé™©ç‰¹å¾
3. æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§æŽ’åºå’Œåˆ†æž
4. å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒå’Œç›¸å…³æ€§

#### åŽç«¯æŠ€æœ¯å®žçŽ°
**æ–‡ä»¶ä½ç½®**: `backend/feature_engineer/risk_features.py`

**æ ¸å¿ƒç±»å’Œæ–¹æ³•**:
```python
class RiskFeatureEngineer:
    def engineer_all_features(self, data: pd.DataFrame) -> pd.DataFrame
        """ç”Ÿæˆæ‰€æœ‰é£Žé™©ç‰¹å¾çš„ä¸»å…¥å£"""
    
    def create_time_features(self, data: pd.DataFrame) -> pd.DataFrame
        """åˆ›å»ºæ—¶é—´ç›¸å…³ç‰¹å¾"""
    
    def create_amount_features(self, data: pd.DataFrame) -> pd.DataFrame
        """åˆ›å»ºé‡‘é¢ç›¸å…³ç‰¹å¾"""
    
    def create_customer_features(self, data: pd.DataFrame) -> pd.DataFrame
        """åˆ›å»ºå®¢æˆ·ç›¸å…³ç‰¹å¾"""
    
    def create_behavioral_features(self, data: pd.DataFrame) -> pd.DataFrame
        """åˆ›å»ºè¡Œä¸ºç›¸å…³ç‰¹å¾"""
```

**ç‰¹å¾ç”Ÿæˆç­–ç•¥**:
1. **æ—¶é—´ç‰¹å¾**: äº¤æ˜“æ—¶é—´ã€å·¥ä½œæ—¥/å‘¨æœ«ã€èŠ‚å‡æ—¥æ ‡è¯†
2. **é‡‘é¢ç‰¹å¾**: é‡‘é¢åˆ†ä½æ•°ã€å¼‚å¸¸é‡‘é¢æ ‡è¯†ã€é‡‘é¢å˜åŒ–çŽ‡
3. **å®¢æˆ·ç‰¹å¾**: è´¦æˆ·å¹´é¾„ã€åŽ†å²äº¤æ˜“é¢‘çŽ‡ã€é£Žé™©è¯„åˆ†
4. **è¡Œä¸ºç‰¹å¾**: è®¾å¤‡æŒ‡çº¹ã€åœ°ç†ä½ç½®ã€äº¤æ˜“æ¨¡å¼

### 3. èšç±»åˆ†æžæ¨¡å— â­

#### å‰ç«¯ç•Œé¢åŠŸèƒ½
**æ–‡ä»¶ä½ç½®**: `frontend/pages/clustering_page.py`

**ä¸»è¦åŠŸèƒ½**:
- ðŸ“Š K-Meansæ— ç›‘ç£èšç±»åˆ†æž
- ðŸŽ¯ èšç±»æ•°é‡è‡ªåŠ¨ä¼˜åŒ–
- ðŸ“ˆ èšç±»ç»“æžœå¯è§†åŒ–
- ðŸ” èšç±»ç‰¹å¾è§£é‡Š

**ç•Œé¢æ“ä½œæµç¨‹**:
1. é€‰æ‹©èšç±»ç‰¹å¾å’Œå‚æ•°
2. æ‰§è¡ŒK-Meansèšç±»ç®—æ³•
3. æ˜¾ç¤ºèšç±»ç»“æžœå’Œç‰¹å¾åˆ†å¸ƒ
4. åˆ†æžæ¯ä¸ªèšç±»çš„é£Žé™©ç‰¹å¾

#### åŽç«¯æŠ€æœ¯å®žçŽ°
**æ–‡ä»¶ä½ç½®**: `backend/clustering/`

**æ ¸å¿ƒç±»å’Œæ–¹æ³•**:
```python
# backend/clustering/cluster_analyzer.py
class ClusterAnalyzer:
    def analyze_clusters(self, data: pd.DataFrame) -> Dict[str, Any]
        """æ‰§è¡Œèšç±»åˆ†æžçš„ä¸»æ–¹æ³•"""
    
    def _prepare_clustering_data(self, data: pd.DataFrame) -> pd.DataFrame
        """å‡†å¤‡èšç±»æ•°æ®ï¼šç‰¹å¾é€‰æ‹©å’Œæ ‡å‡†åŒ–"""
    
    def _perform_clustering(self, cluster_data: pd.DataFrame) -> np.ndarray
        """æ‰§è¡ŒK-Meansèšç±»"""
    
    def _analyze_cluster_characteristics(self, data: pd.DataFrame, labels: np.ndarray) -> Dict
        """åˆ†æžèšç±»ç‰¹å¾å’Œé£Žé™©ç‰¹å¾"""

# backend/clustering/cluster_risk_mapper.py
class ClusterRiskMapper:
    def map_clusters_to_risk_levels(self, cluster_results: Dict) -> Dict[int, str]
        """å°†èšç±»æ˜ å°„åˆ°é£Žé™©ç­‰çº§"""
    
    def _calculate_cluster_risk_score(self, cluster_data: pd.DataFrame) -> float
        """è®¡ç®—èšç±»é£Žé™©åˆ†æ•°"""
    
    def _assign_risk_level(self, risk_score: float) -> str
        """åˆ†é…é£Žé™©ç­‰çº§"""
```

**ç®—æ³•å®žçŽ°æ€è·¯**:
1. **ç‰¹å¾é€‰æ‹©**: é€‰æ‹©å…³é”®é£Žé™©ç‰¹å¾è¿›è¡Œèšç±»
2. **èšç±»ç®—æ³•**: K-Meansèšç±»ï¼Œè‡ªåŠ¨ç¡®å®šæœ€ä¼˜èšç±»æ•°
3. **é£Žé™©æ˜ å°„**: åŸºäºŽèšç±»å†…æ¬ºè¯ˆçŽ‡å’Œé£Žé™©ç‰¹å¾åˆ†å¸ƒç¡®å®šé£Žé™©ç­‰çº§
4. **ç»“æžœè§£é‡Š**: ç”Ÿæˆæ¯ä¸ªèšç±»çš„ç‰¹å¾ç”»åƒå’Œé£Žé™©æè¿°

**æŠ€æœ¯ç»†èŠ‚**:
```python
# èšç±»ç‰¹å¾é€‰æ‹©
clustering_features = [
    'transaction_amount', 'quantity', 'customer_age', 
    'account_age_days', 'transaction_hour'
]

# é£Žé™©ç­‰çº§æ˜ å°„é€»è¾‘
def _assign_risk_level(self, risk_score: float) -> str:
    if risk_score >= 80:
        return 'critical'  # æžé«˜é£Žé™©
    elif risk_score >= 60:
        return 'high'      # é«˜é£Žé™©
    elif risk_score >= 40:
        return 'medium'    # ä¸­é£Žé™©
    else:
        return 'low'       # ä½Žé£Žé™©
```

### 4. é£Žé™©è¯„åˆ†æ¨¡å— â­

#### å‰ç«¯ç•Œé¢åŠŸèƒ½
**æ–‡ä»¶ä½ç½®**: `frontend/pages/risk_scoring_page.py`

**ä¸»è¦åŠŸèƒ½**:
- ðŸŽ¯ å¤šç»´åº¦æ— ç›‘ç£é£Žé™©è¯„åˆ†
- ðŸ“Š åŠ¨æ€é˜ˆå€¼ç®¡ç†
- ðŸ“ˆ é£Žé™©ç­‰çº§åˆ†å¸ƒåˆ†æž
- âš™ï¸ è¯„åˆ†ç­–ç•¥é…ç½®

**ç•Œé¢æ“ä½œæµç¨‹**:
1. é€‰æ‹©é£Žé™©è¯„åˆ†æ¨¡å¼ï¼ˆæ ‡å‡†/å¿«é€Ÿï¼‰
2. é…ç½®è¯„åˆ†æƒé‡å’Œå‚æ•°
3. æ‰§è¡Œå¤šç­–ç•¥é£Žé™©è¯„åˆ†
4. åˆ†æžé£Žé™©åˆ†å¸ƒå’Œé˜ˆå€¼ä¼˜åŒ–

#### åŽç«¯æŠ€æœ¯å®žçŽ°
**æ–‡ä»¶ä½ç½®**: `backend/risk_scoring/`

**æ ¸å¿ƒç±»å’Œæ–¹æ³•**:
```python
# backend/risk_scoring/risk_calculator.py
class RiskCalculator:
    def calculate_unsupervised_risk_score(self, data: pd.DataFrame, cluster_results: Dict) -> Dict
        """è®¡ç®—æ— ç›‘ç£é£Žé™©è¯„åˆ†"""
    
    def _calculate_cluster_risk(self, data: pd.DataFrame, cluster_results: Dict) -> np.ndarray
        """è®¡ç®—èšç±»é£Žé™©åˆ†æ•°"""
    
    def _calculate_rule_risk(self, data: pd.DataFrame) -> np.ndarray
        """è®¡ç®—è§„åˆ™é£Žé™©åˆ†æ•°"""
    
    def _calculate_model_risk(self, data: pd.DataFrame) -> np.ndarray
        """è®¡ç®—æ¨¡åž‹é£Žé™©åˆ†æ•°"""
    
    def _combine_risk_scores(self, cluster_risk: np.ndarray, rule_risk: np.ndarray, model_risk: np.ndarray) -> np.ndarray
        """èžåˆå¤šç§é£Žé™©åˆ†æ•°"""

# backend/risk_scoring/dynamic_threshold_manager.py
class DynamicThresholdManager:
    def calculate_dynamic_thresholds(self, risk_scores: np.ndarray) -> Dict[str, float]
        """è®¡ç®—åŠ¨æ€é£Žé™©é˜ˆå€¼"""
    
    def _calculate_percentile_thresholds(self, risk_scores: np.ndarray) -> Dict[str, float]
        """åŸºäºŽç™¾åˆ†ä½æ•°è®¡ç®—é˜ˆå€¼"""
```

**ç®—æ³•æ€è·¯**:
1. **å¤šç­–ç•¥èžåˆ**: èšç±»é£Žé™© + ä¸“å®¶è§„åˆ™ + æ¨¡åž‹é¢„æµ‹
2. **æƒé‡åˆ†é…**: èšç±»é£Žé™©(40%) + è§„åˆ™é£Žé™©(35%) + æ¨¡åž‹é£Žé™©(25%)
3. **åŠ¨æ€é˜ˆå€¼**: åŸºäºŽæ•°æ®åˆ†å¸ƒè‡ªåŠ¨è°ƒæ•´é£Žé™©ç­‰çº§é˜ˆå€¼
4. **æ ‡å‡†/å¿«é€Ÿæ¨¡å¼**: æä¾›ä¸åŒç²¾åº¦å’Œé€Ÿåº¦çš„è®¡ç®—é€‰é¡¹

**æŠ€æœ¯å®žçŽ°**:
```python
# é£Žé™©è¯„åˆ†èžåˆç®—æ³•
def _combine_risk_scores(self, cluster_risk, rule_risk, model_risk):
    weights = {
        'cluster': 0.40,  # èšç±»é£Žé™©æƒé‡
        'rule': 0.35,     # è§„åˆ™é£Žé™©æƒé‡  
        'model': 0.25     # æ¨¡åž‹é£Žé™©æƒé‡
    }
    
    combined_risk = (
        weights['cluster'] * cluster_risk +
        weights['rule'] * rule_risk +
        weights['model'] * model_risk
    )
    
    return np.clip(combined_risk, 0, 100)

# åŠ¨æ€é˜ˆå€¼è®¡ç®—
def calculate_dynamic_thresholds(self, risk_scores):
    return {
        'low': np.percentile(risk_scores, 25),      # 25åˆ†ä½æ•°
        'medium': np.percentile(risk_scores, 50),   # 50åˆ†ä½æ•°  
        'high': np.percentile(risk_scores, 75),     # 75åˆ†ä½æ•°
        'critical': np.percentile(risk_scores, 90)  # 90åˆ†ä½æ•°
    }
```

### 5. ä¼ªæ ‡ç­¾ç”Ÿæˆæ¨¡å— â­â­

#### å‰ç«¯ç•Œé¢åŠŸèƒ½
**æ–‡ä»¶ä½ç½®**: `frontend/pages/pseudo_labeling_page.py`

**ä¸»è¦åŠŸèƒ½**:
- ðŸ·ï¸ å¤šç­–ç•¥ä¼ªæ ‡ç­¾ç”Ÿæˆ
- ðŸ“Š æ ‡ç­¾è´¨é‡è¯„ä¼°å’Œæ ¡å‡†
- ðŸŽ¯ é«˜è´¨é‡æ ‡ç­¾ç­›é€‰
- ðŸ“ˆ æ ‡ç­¾åˆ†å¸ƒåˆ†æž

**ç•Œé¢æ“ä½œæµç¨‹**:
1. é€‰æ‹©ä¼ªæ ‡ç­¾ç”Ÿæˆç­–ç•¥
2. é…ç½®ç½®ä¿¡åº¦é˜ˆå€¼å’Œå‚æ•°
3. æ‰§è¡Œä¼ªæ ‡ç­¾ç”Ÿæˆç®—æ³•
4. è¯„ä¼°æ ‡ç­¾è´¨é‡å’Œåˆ†å¸ƒ

#### åŽç«¯æŠ€æœ¯å®žçŽ°
**æ–‡ä»¶ä½ç½®**: `backend/pseudo_labeling/`

**æ ¸å¿ƒç±»å’Œæ–¹æ³•**:
```python
# backend/pseudo_labeling/pseudo_label_generator.py
class PseudoLabelGenerator:
    def generate_pseudo_labels(self, data: pd.DataFrame, strategy: str = 'ensemble') -> Dict
        """ç”Ÿæˆä¼ªæ ‡ç­¾çš„ä¸»æ–¹æ³•"""

    def generate_high_quality_pseudo_labels(self, data: pd.DataFrame, min_confidence: float = 0.8) -> Dict
        """ç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾"""

    def _ensemble_strategy(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]
        """é›†æˆç­–ç•¥ï¼šèžåˆå¤šç§æ–¹æ³•"""

    def _risk_based_strategy(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]
        """é£Žé™©ç­–ç•¥ï¼šåŸºäºŽé£Žé™©è¯„åˆ†"""

    def _cluster_based_strategy(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]
        """èšç±»ç­–ç•¥ï¼šåŸºäºŽèšç±»é£Žé™©ç­‰çº§"""

    def _rule_based_strategy(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]
        """è§„åˆ™ç­–ç•¥ï¼šåŸºäºŽä¸“å®¶ä¸šåŠ¡è§„åˆ™"""
```

**ç®—æ³•ç­–ç•¥**:
1. **é›†æˆç­–ç•¥ (Ensemble)**: èžåˆå¤šç§ç­–ç•¥çš„æŠ•ç¥¨ç»“æžœ
2. **é£Žé™©ç­–ç•¥ (Risk-based)**: åŸºäºŽé£Žé™©è¯„åˆ†é˜ˆå€¼ç”Ÿæˆæ ‡ç­¾
3. **èšç±»ç­–ç•¥ (Cluster-based)**: åŸºäºŽèšç±»é£Žé™©ç­‰çº§ç”Ÿæˆæ ‡ç­¾
4. **è§„åˆ™ç­–ç•¥ (Rule-based)**: åŸºäºŽä¸“å®¶ä¸šåŠ¡è§„åˆ™ç”Ÿæˆæ ‡ç­¾

**æŠ€æœ¯å®žçŽ°**:
```python
# é›†æˆç­–ç•¥å®žçŽ°
def _ensemble_strategy(self, data: pd.DataFrame):
    # èŽ·å–å„ç­–ç•¥ç»“æžœ
    risk_labels, risk_conf = self._risk_based_strategy(data)
    cluster_labels, cluster_conf = self._cluster_based_strategy(data)
    rule_labels, rule_conf = self._rule_based_strategy(data)

    # åŠ æƒæŠ•ç¥¨
    weights = {'risk': 0.45, 'cluster': 0.35, 'rule': 0.20}

    ensemble_confidence = (
        weights['risk'] * risk_conf +
        weights['cluster'] * cluster_conf +
        weights['rule'] * rule_conf
    )

    # åŸºäºŽç½®ä¿¡åº¦é˜ˆå€¼ç”Ÿæˆæœ€ç»ˆæ ‡ç­¾
    ensemble_labels = (ensemble_confidence >= self.confidence_threshold).astype(int)

    return ensemble_labels, ensemble_confidence

# é«˜è´¨é‡æ ‡ç­¾ç­›é€‰
def generate_high_quality_pseudo_labels(self, data: pd.DataFrame, min_confidence: float = 0.8):
    labels, confidences = self._ensemble_strategy(data)

    # ç­›é€‰é«˜ç½®ä¿¡åº¦æ ‡ç­¾
    high_quality_mask = confidences >= min_confidence
    high_quality_indices = np.where(high_quality_mask)[0]
    high_quality_labels = labels[high_quality_mask]
    high_quality_confidences = confidences[high_quality_mask]

    return {
        'all_labels': labels,
        'all_confidences': confidences,
        'high_quality_indices': high_quality_indices,
        'high_quality_labels': high_quality_labels,
        'high_quality_confidences': high_quality_confidences,
        'quality_rate': len(high_quality_indices) / len(labels)
    }
```

## ðŸ“Š æ•°æ®æµç¨‹ä¸Žç³»ç»Ÿé›†æˆ

### å®Œæ•´ä¸šåŠ¡æµç¨‹
```
æ•°æ®ä¸Šä¼  â†’ ç‰¹å¾å·¥ç¨‹ â†’ èšç±»åˆ†æž â†’ é£Žé™©è¯„åˆ† â†’ ä¼ªæ ‡ç­¾ç”Ÿæˆ â†’ æ¨¡åž‹é¢„æµ‹ â†’ æ”»å‡»åˆ†ç±» â†’ åˆ†æžæŠ¥å‘Š
```

### æ¨¡å—é—´æ•°æ®ä¼ é€’
1. **æ•°æ®ä¸Šä¼ ** â†’ **ç‰¹å¾å·¥ç¨‹**: æ¸…ç†åŽçš„åŽŸå§‹æ•°æ®
2. **ç‰¹å¾å·¥ç¨‹** â†’ **èšç±»åˆ†æž**: å·¥ç¨‹åŒ–ç‰¹å¾æ•°æ®
3. **èšç±»åˆ†æž** â†’ **é£Žé™©è¯„åˆ†**: èšç±»ç»“æžœå’Œé£Žé™©æ˜ å°„
4. **é£Žé™©è¯„åˆ†** â†’ **ä¼ªæ ‡ç­¾ç”Ÿæˆ**: é£Žé™©åˆ†æ•°å’Œç­‰çº§
5. **ä¼ªæ ‡ç­¾ç”Ÿæˆ** â†’ **æ¨¡åž‹é¢„æµ‹**: é«˜è´¨é‡ä¼ªæ ‡ç­¾
6. **æ¨¡åž‹é¢„æµ‹** â†’ **æ”»å‡»åˆ†ç±»**: é¢„æµ‹ç»“æžœå’Œæ¦‚çŽ‡
7. **æ”»å‡»åˆ†ç±»** â†’ **åˆ†æžæŠ¥å‘Š**: æ”»å‡»ç±»åž‹å’Œé˜²æŠ¤å»ºè®®

### æ ¸å¿ƒç®—æ³•æ€»ç»“

#### 1. èšç±»é£Žé™©æ˜ å°„ç®—æ³•
```python
def _calculate_cluster_risk_score(self, cluster_data: pd.DataFrame) -> float:
    # åŸºäºŽå¤šä¸ªé£Žé™©å› å­è®¡ç®—èšç±»é£Žé™©åˆ†æ•°
    risk_factors = {
        'high_amount_rate': self._calculate_high_amount_rate(cluster_data),
        'new_account_rate': self._calculate_new_account_rate(cluster_data),
        'unusual_time_rate': self._calculate_unusual_time_rate(cluster_data),
        'high_quantity_rate': self._calculate_high_quantity_rate(cluster_data)
    }

    # åŠ æƒè®¡ç®—é£Žé™©åˆ†æ•°
    weights = {'high_amount_rate': 0.3, 'new_account_rate': 0.3,
               'unusual_time_rate': 0.2, 'high_quantity_rate': 0.2}

    risk_score = sum(weights[factor] * score for factor, score in risk_factors.items())
    return min(risk_score * 100, 100)  # å½’ä¸€åŒ–åˆ°0-100
```

#### 2. ä¼ªæ ‡ç­¾è´¨é‡è¯„ä¼°ç®—æ³•
```python
def _evaluate_label_quality(self, labels: np.ndarray, confidences: np.ndarray) -> Dict:
    # è®¡ç®—æ ‡ç­¾è´¨é‡æŒ‡æ ‡
    high_conf_mask = confidences >= 0.8
    medium_conf_mask = (confidences >= 0.6) & (confidences < 0.8)
    low_conf_mask = confidences < 0.6

    quality_metrics = {
        'high_quality_rate': np.sum(high_conf_mask) / len(labels),
        'medium_quality_rate': np.sum(medium_conf_mask) / len(labels),
        'low_quality_rate': np.sum(low_conf_mask) / len(labels),
        'avg_confidence': np.mean(confidences),
        'fraud_rate': np.mean(labels)
    }

    return quality_metrics
```

## ðŸŽ¯ ç³»ç»Ÿç‰¹è‰²ä¸Žåˆ›æ–°

### 1. å¤šç­–ç•¥èžåˆæž¶æž„
- **æ— ç›‘ç£å­¦ä¹ **: å‘çŽ°æœªçŸ¥æ¨¡å¼å’Œå¼‚å¸¸è¡Œä¸º
- **åŠç›‘ç£å­¦ä¹ **: ç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾
- **ç›‘ç£å­¦ä¹ **: æä¾›ç²¾ç¡®é¢„æµ‹
- **ä¸“å®¶è§„åˆ™**: è¡¥å……ä¸šåŠ¡é€»è¾‘

### 2. è‡ªé€‚åº”é£Žé™©è¯„ä¼°
- **åŠ¨æ€é˜ˆå€¼ç®¡ç†**: åŸºäºŽæ•°æ®åˆ†å¸ƒè‡ªåŠ¨è°ƒæ•´
- **å®žæ—¶é£Žé™©ç­‰çº§**: æ ¹æ®æœ€æ–°æ•°æ®æ›´æ–°
- **ä¸ªæ€§åŒ–é£Žé™©ç”»åƒ**: é’ˆå¯¹ä¸åŒç”¨æˆ·ç¾¤ä½“
- **æ™ºèƒ½å¼‚å¸¸æ£€æµ‹**: å‘çŽ°æ–°åž‹æ¬ºè¯ˆæ¨¡å¼

### 3. å¯è§£é‡Šæ€§è®¾è®¡
- **SHAPæ·±åº¦è§£é‡Š**: ç‰¹å¾è´¡çŒ®åº¦åˆ†æž
- **å†³ç­–è·¯å¾„å¯è§†åŒ–**: æ¨¡åž‹å†³ç­–è¿‡ç¨‹é€æ˜ŽåŒ–
- **ä¸šåŠ¡å‹å¥½è§£é‡Š**: éžæŠ€æœ¯äººå‘˜æ˜“ç†è§£
- **é£Žé™©å› å­åˆ†è§£**: è¯¦ç»†é£Žé™©æ¥æºåˆ†æž

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åŽæ›´æ–°**: 2025-01-17
**ç»´æŠ¤å›¢é˜Ÿ**: ç”µå•†é£ŽæŽ§æŠ€æœ¯å›¢é˜Ÿ
