"""
å¢å¼ºæ¨¡å‹è®­ç»ƒå™¨
æ•´åˆæ‰€æœ‰å‰ç½®æ­¥éª¤çš„ç»“æœè¿›è¡Œæ¨¡å‹è®­ç»ƒ
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
import logging
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

logger = logging.getLogger(__name__)

class EnhancedModelTrainer:
    """å¢å¼ºæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        self.models = {}
        self.training_history = []
        self.feature_importance = {}
        
    def train_with_integrated_data(self, 
                                 integrated_data: pd.DataFrame,
                                 feature_mapping: Dict[str, str],
                                 target_column: str = 'pseudo_label',
                                 test_size: float = 0.2) -> Dict[str, Any]:
        """
        ä½¿ç”¨æ•´åˆæ•°æ®è®­ç»ƒæ¨¡å‹
        
        Args:
            integrated_data: æ•´åˆåçš„æ•°æ®
            feature_mapping: ç‰¹å¾æ¥æºæ˜ å°„
            target_column: ç›®æ ‡åˆ—å
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        try:
            logger.info("ğŸš€ å¼€å§‹å¢å¼ºæ¨¡å‹è®­ç»ƒ")
            start_time = datetime.now()
            
            # 1. æ•°æ®é¢„å¤„ç†
            X, y = self._prepare_training_data(integrated_data, target_column)
            if X is None or y is None:
                return self._empty_result("æ•°æ®é¢„å¤„ç†å¤±è´¥")
            
            # 2. ç‰¹å¾é€‰æ‹©
            selected_features = self._select_training_features(X, feature_mapping)
            X_selected = X[selected_features]
            
            # 3. æ•°æ®åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X_selected, y, test_size=test_size, random_state=42, stratify=y
            )
            
            # 4. æ¨¡å‹è®­ç»ƒ
            model_results = self._train_multiple_models(X_train, y_train, X_test, y_test)
            
            # 5. ç‰¹å¾é‡è¦æ€§åˆ†æ
            feature_importance = self._analyze_feature_importance(
                model_results, selected_features, feature_mapping
            )
            
            # 6. æ¨¡å‹è¯„ä¼°
            evaluation_results = self._evaluate_models(model_results, X_test, y_test)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # 7. ä¿å­˜è®­ç»ƒå†å²
            training_record = {
                'timestamp': datetime.now().isoformat(),
                'data_size': len(integrated_data),
                'features_used': len(selected_features),
                'models_trained': list(model_results.keys()),
                'best_model': evaluation_results.get('best_model', 'unknown'),
                'processing_time': processing_time
            }
            self.training_history.append(training_record)
            
            logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            
            return {
                'success': True,
                'models': model_results,
                'feature_importance': feature_importance,
                'evaluation': evaluation_results,
                'training_data_info': {
                    'total_samples': len(integrated_data),
                    'training_samples': len(X_train),
                    'test_samples': len(X_test),
                    'features_used': len(selected_features),
                    'selected_features': selected_features
                },
                'processing_time': processing_time,
                'training_record': training_record
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return self._empty_result(f"è®­ç»ƒå¤±è´¥: {str(e)}")
    
    def _prepare_training_data(self, data: pd.DataFrame, target_column: str) -> Tuple[pd.DataFrame, pd.Series]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        try:
            # æ£€æŸ¥ç›®æ ‡åˆ—
            if target_column not in data.columns:
                logger.error(f"ç›®æ ‡åˆ— {target_column} ä¸å­˜åœ¨")
                return None, None
            
            # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
            y = data[target_column]
            X = data.drop(columns=[target_column])
            
            # ç§»é™¤éæ•°å€¼åˆ—ï¼ˆé™¤äº†å·²ç¼–ç çš„åˆ†ç±»ç‰¹å¾ï¼‰
            numeric_columns = X.select_dtypes(include=[np.number]).columns
            categorical_encoded = [col for col in X.columns if col.endswith('_encoded')]
            
            feature_columns = list(set(numeric_columns) | set(categorical_encoded))
            X = X[feature_columns]
            
            # å¤„ç†ç¼ºå¤±å€¼
            X = X.fillna(X.mean())
            
            logger.info(f"è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ: {len(X)} æ ·æœ¬, {len(X.columns)} ç‰¹å¾")
            return X, y
            
        except Exception as e:
            logger.error(f"è®­ç»ƒæ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return None, None
    
    def _select_training_features(self, X: pd.DataFrame, feature_mapping: Dict[str, str]) -> List[str]:
        """é€‰æ‹©è®­ç»ƒç‰¹å¾"""
        try:
            # æŒ‰é‡è¦æ€§åˆ†ç»„ç‰¹å¾
            high_importance = []
            medium_importance = []
            low_importance = []
            
            # é«˜é‡è¦æ€§ç‰¹å¾ï¼ˆæ¥è‡ªé£é™©è¯„åˆ†å’Œä¼ªæ ‡ç­¾ï¼‰
            high_priority_sources = ['risk_scoring', 'enhanced', 'pseudo_labeling']
            high_priority_features = [
                'risk_score', 'composite_risk_score', 'pseudo_confidence',
                'cluster_fraud_rate', 'risk_confidence_alignment', 'cluster_risk_interaction'
            ]
            
            # ä¸­ç­‰é‡è¦æ€§ç‰¹å¾ï¼ˆæ¥è‡ªèšç±»å’Œç‰¹å¾å·¥ç¨‹ï¼‰
            medium_priority_sources = ['clustering', 'engineered_features']
            medium_priority_features = [
                'is_consistent_prediction', 'label_risk_consistency', 
                'is_amount_outlier', 'cluster_size'
            ]
            
            for feature in X.columns:
                source = feature_mapping.get(feature, 'unknown')
                
                if feature in high_priority_features or source in high_priority_sources:
                    high_importance.append(feature)
                elif feature in medium_priority_features or source in medium_priority_sources:
                    medium_importance.append(feature)
                else:
                    low_importance.append(feature)
            
            # é€‰æ‹©ç‰¹å¾ï¼ˆä¼˜å…ˆé«˜é‡è¦æ€§ï¼‰
            selected_features = high_importance + medium_importance
            
            # å¦‚æœç‰¹å¾å¤ªå°‘ï¼Œæ·»åŠ ä¸€äº›ä½é‡è¦æ€§ç‰¹å¾
            if len(selected_features) < 10:
                selected_features.extend(low_importance[:10-len(selected_features)])
            
            # é™åˆ¶ç‰¹å¾æ•°é‡ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
            if len(selected_features) > 20:
                selected_features = selected_features[:20]
            
            logger.info(f"ç‰¹å¾é€‰æ‹©å®Œæˆ: {len(selected_features)} ä¸ªç‰¹å¾")
            logger.info(f"é«˜é‡è¦æ€§: {len(high_importance)}, ä¸­ç­‰: {len(medium_importance)}, ä½é‡è¦æ€§: {len(low_importance)}")
            
            return selected_features
            
        except Exception as e:
            logger.error(f"ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            return list(X.columns)[:15]  # è¿”å›å‰15ä¸ªç‰¹å¾ä½œä¸ºå¤‡é€‰
    
    def _train_multiple_models(self, X_train, y_train, X_test, y_test) -> Dict[str, Any]:
        """è®­ç»ƒå¤šä¸ªæ¨¡å‹"""
        models_config = {
            'random_forest': {
                'model': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),
                'name': 'Random Forest'
            }
        }
        
        trained_models = {}
        
        for model_key, config in models_config.items():
            try:
                logger.info(f"è®­ç»ƒ {config['name']} æ¨¡å‹...")
                
                model = config['model']
                model.fit(X_train, y_train)
                
                # é¢„æµ‹
                train_pred = model.predict(X_train)
                test_pred = model.predict(X_test)
                
                # é¢„æµ‹æ¦‚ç‡
                if hasattr(model, 'predict_proba'):
                    train_proba = model.predict_proba(X_train)[:, 1]
                    test_proba = model.predict_proba(X_test)[:, 1]
                else:
                    train_proba = train_pred.astype(float)
                    test_proba = test_pred.astype(float)
                
                trained_models[model_key] = {
                    'model': model,
                    'name': config['name'],
                    'train_predictions': train_pred,
                    'test_predictions': test_pred,
                    'train_probabilities': train_proba,
                    'test_probabilities': test_proba
                }
                
                logger.info(f"âœ… {config['name']} è®­ç»ƒå®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ {config['name']} è®­ç»ƒå¤±è´¥: {e}")
        
        return trained_models
    
    def _analyze_feature_importance(self, model_results: Dict, features: List[str], 
                                  feature_mapping: Dict[str, str]) -> Dict[str, Any]:
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        try:
            importance_analysis = {}
            
            for model_key, model_info in model_results.items():
                model = model_info['model']
                
                if hasattr(model, 'feature_importances_'):
                    importances = model.feature_importances_
                    
                    # åˆ›å»ºç‰¹å¾é‡è¦æ€§å­—å…¸
                    feature_importance = {}
                    for i, feature in enumerate(features):
                        feature_importance[feature] = {
                            'importance': float(importances[i]),
                            'source': feature_mapping.get(feature, 'unknown')
                        }
                    
                    # æŒ‰é‡è¦æ€§æ’åº
                    sorted_features = sorted(
                        feature_importance.items(), 
                        key=lambda x: x[1]['importance'], 
                        reverse=True
                    )
                    
                    importance_analysis[model_key] = {
                        'feature_importance': feature_importance,
                        'top_features': sorted_features[:10],
                        'importance_by_source': self._group_importance_by_source(
                            feature_importance, feature_mapping
                        )
                    }
            
            return importance_analysis
            
        except Exception as e:
            logger.error(f"ç‰¹å¾é‡è¦æ€§åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _group_importance_by_source(self, feature_importance: Dict, 
                                  feature_mapping: Dict[str, str]) -> Dict[str, float]:
        """æŒ‰æ¥æºåˆ†ç»„ç‰¹å¾é‡è¦æ€§"""
        source_importance = {}
        
        for feature, info in feature_importance.items():
            source = feature_mapping.get(feature, 'unknown')
            if source not in source_importance:
                source_importance[source] = 0
            source_importance[source] += info['importance']
        
        return source_importance
    
    def _evaluate_models(self, model_results: Dict, X_test, y_test) -> Dict[str, Any]:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        try:
            evaluation = {}
            best_score = 0
            best_model = None
            
            for model_key, model_info in model_results.items():
                test_pred = model_info['test_predictions']
                
                # è®¡ç®—å‡†ç¡®ç‡
                accuracy = np.mean(test_pred == y_test)
                
                # åˆ†ç±»æŠ¥å‘Š
                report = classification_report(y_test, test_pred, output_dict=True)
                
                # æ··æ·†çŸ©é˜µ
                cm = confusion_matrix(y_test, test_pred)
                
                evaluation[model_key] = {
                    'accuracy': float(accuracy),
                    'classification_report': report,
                    'confusion_matrix': cm.tolist(),
                    'model_name': model_info['name']
                }
                
                if accuracy > best_score:
                    best_score = accuracy
                    best_model = model_key
            
            evaluation['best_model'] = best_model
            evaluation['best_accuracy'] = best_score
            
            return evaluation
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return {}
    
    def _empty_result(self, error_message: str) -> Dict[str, Any]:
        """è¿”å›ç©ºç»“æœ"""
        return {
            'success': False,
            'error': error_message,
            'models': {},
            'feature_importance': {},
            'evaluation': {},
            'training_data_info': {},
            'processing_time': 0
        }
