"""
Fast Pseudo Label Generator
Use vectorized operations and caching mechanisms to significantly improve generation speed
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
import logging
from datetime import datetime
import time

logger = logging.getLogger(__name__)


class FastPseudoLabelGenerator:
    """Fast Pseudo Label Generator"""

    def __init__(self):
        """Initialize fast pseudo label generator"""
        self.confidence_threshold = 0.7
        self.cache = {}  # Caching mechanism

        # åŸºäºé£é™©è¯„åˆ†ç”Ÿæˆæ ‡ç­¾çš„é˜ˆå€¼ (é™ä½é˜ˆå€¼ä»¥å¢åŠ æ¬ºè¯ˆæ£€å‡ºç‡)
        self.risk_thresholds = {
            'critical': 70,    # æé«˜é£é™©é˜ˆå€¼ (ä»80é™åˆ°70)
            'high': 45,        # é«˜é£é™©é˜ˆå€¼ (ä»60é™åˆ°45)
            'medium': 30,      # ä¸­é£é™©é˜ˆå€¼ (ä»40é™åˆ°30)
            'low': 15          # ä½é£é™©é˜ˆå€¼ (ä»20é™åˆ°15)
        }
        
    def generate_fast_pseudo_labels(self, data: pd.DataFrame,
                                  risk_results: Optional[Dict] = None,
                                  min_confidence: float = 0.8) -> Dict[str, Any]:
        """
        Fast pseudo label generation

        Args:
            data: Input data
            risk_results: Risk scoring results (if available)
            min_confidence: Minimum confidence threshold

        Returns:
            Pseudo label generation results
        """
        try:
            if data is None or data.empty:
                logger.error("Input data is empty")
                return self._empty_result()

            start_time = time.time()
            logger.info(f"Starting fast pseudo label generation, data size: {len(data)}")

            # 1. Use existing risk scoring results or fast calculation
            if risk_results is None:
                risk_results = self._get_cached_risk_results(data)

            # 2. Vectorized pseudo label generation
            labels, confidences = self._generate_labels_vectorized(data, risk_results)

            # 3. Filter high-quality labels
            high_quality_indices = np.where(np.array(confidences) >= min_confidence)[0].tolist()
            high_quality_labels = [labels[i] for i in high_quality_indices]
            high_quality_confidences = [confidences[i] for i in high_quality_indices]

            # è°ƒè¯•ä¿¡æ¯
            max_conf = max(confidences) if confidences else 0
            avg_conf = np.mean(confidences) if confidences else 0
            logger.info(f"Confidence stats: max={max_conf:.3f}, avg={avg_conf:.3f}, threshold={min_confidence}")
            logger.info(f"High quality filtering: {len(high_quality_labels)}/{len(labels)} samples passed threshold")

            # 4. Generate simplified quality report
            quality_report = self._generate_fast_quality_report(
                labels, confidences, high_quality_indices
            )
            
            result = {
                'success': True,
                'strategy': 'fast_generation',
                'all_labels': labels,
                'all_confidences': confidences,
                'high_quality_indices': high_quality_indices,
                'high_quality_labels': high_quality_labels,
                'high_quality_confidences': high_quality_confidences,
                'quality_report': quality_report,
                'min_confidence_threshold': min_confidence,
                'metadata': {
                    'total_samples': len(labels),
                    'high_quality_count': len(high_quality_labels),
                    'high_quality_rate': len(high_quality_labels) / len(labels) if labels else 0,
                    'avg_confidence_all': float(np.mean(confidences)) if confidences else 0,
                    'avg_confidence_hq': float(np.mean(high_quality_confidences)) if high_quality_confidences else 0,
                    'fraud_rate_all': float(np.mean(labels)) if labels else 0,
                    'fraud_rate_hq': float(np.mean(high_quality_labels)) if high_quality_labels else 0,
                    'generation_time': time.time() - start_time
                }
            }
            
            logger.info(f"Fast pseudo label generation completed, time taken: {time.time() - start_time:.2f} seconds")
            return result

        except Exception as e:
            import traceback
            logger.error(f"Fast pseudo label generation failed: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return self._empty_result()

    def _get_cached_risk_results(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Get cached risk scoring results with proper dependency checking"""
        # æ£€æŸ¥session stateä¸­æ˜¯å¦æœ‰ç°æˆçš„é£é™©è¯„åˆ†ç»“æœ
        try:
            import streamlit as st

            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å››åˆ†ç±»é£é™©è¯„åˆ†ç»“æœï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
            if (hasattr(st, 'session_state') and
                hasattr(st.session_state, 'four_class_risk_results') and
                st.session_state.four_class_risk_results is not None):

                risk_results = st.session_state.four_class_risk_results
                if risk_results.get('success') and 'detailed_results' in risk_results:
                    logger.info("âœ… Using cached four-class risk scoring results")
                    return self._convert_four_class_to_risk_format(risk_results)

            # æ£€æŸ¥æ— ç›‘ç£é£é™©è¯„åˆ†ç»“æœ
            if (hasattr(st.session_state, 'unsupervised_risk_results') and
                st.session_state.unsupervised_risk_results is not None):

                risk_results = st.session_state.unsupervised_risk_results
                if risk_results and 'results' in risk_results:
                    logger.info("âœ… Using cached unsupervised risk scoring results")
                    return risk_results

            # å¦‚æœæ²¡æœ‰ä»»ä½•é£é™©è¯„åˆ†ç»“æœï¼ŒæŠ›å‡ºä¾èµ–é”™è¯¯
            logger.error("âŒ å¿«é€Ÿæ¨¡å¼éœ€è¦å…ˆå®Œæˆé£é™©è¯„åˆ†æ­¥éª¤")
            raise ValueError("å¿«é€Ÿä¼ªæ ‡ç­¾ç”Ÿæˆéœ€è¦å…ˆå®Œæˆé£é™©è¯„åˆ†ã€‚è¯·å…ˆåœ¨'ğŸ¯ Risk Scoring'é¡µé¢å®Œæˆé£é™©è¯„åˆ†ã€‚")

        except ValueError:
            # é‡æ–°æŠ›å‡ºä¾èµ–é”™è¯¯
            raise
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç¼“å­˜é£é™©ç»“æœæ—¶å‡ºé”™: {e}")
            raise ValueError("æ— æ³•è·å–é£é™©è¯„åˆ†ç»“æœï¼Œè¯·å…ˆå®Œæˆé£é™©è¯„åˆ†æ­¥éª¤ã€‚")

    def _convert_four_class_to_risk_format(self, four_class_results: Dict) -> Dict[str, Any]:
        """å°†å››åˆ†ç±»é£é™©ç»“æœè½¬æ¢ä¸ºæ ‡å‡†é£é™©è¯„åˆ†æ ¼å¼"""
        try:
            detailed_results = four_class_results.get('detailed_results', [])
            converted_results = []

            for result in detailed_results:
                # å°†å››åˆ†ç±»é£é™©ç­‰çº§è½¬æ¢ä¸ºé£é™©è¯„åˆ†
                risk_level = result.get('risk_level', 'low')
                risk_score_mapping = {
                    'low': 25,
                    'medium': 50,
                    'high': 75,
                    'critical': 90
                }

                converted_result = {
                    'transaction_id': result.get('transaction_id', ''),
                    'risk_score': result.get('risk_score', risk_score_mapping.get(risk_level, 25)),
                    'risk_level': risk_level,
                    'confidence': result.get('confidence', 0.7)
                }
                converted_results.append(converted_result)

            return {
                'results': converted_results,
                'summary': {
                    'total_samples': len(converted_results),
                    'source': 'four_class_risk_scoring'
                }
            }

        except Exception as e:
            logger.error(f"å››åˆ†ç±»ç»“æœè½¬æ¢å¤±è´¥: {e}")
            raise ValueError("é£é™©è¯„åˆ†ç»“æœæ ¼å¼è½¬æ¢å¤±è´¥")

    # ç§»é™¤äº† _calculate_fast_risk_scores æ–¹æ³•
    # å¿«é€Ÿæ¨¡å¼ç°åœ¨å¼ºåˆ¶ä¾èµ–å‰ç½®çš„é£é™©è¯„åˆ†æ­¥éª¤
    # è¿™ç¡®ä¿äº†æ•°æ®æµç¨‹çš„ä¸€è‡´æ€§å’Œè´¨é‡
    
    def _calculate_amount_scores_vectorized(self, amounts: np.ndarray) -> np.ndarray:
        """Vectorized amount score calculation"""
        scores = np.zeros_like(amounts, dtype=float)
        scores[amounts > 50000] = 90
        scores[(amounts > 20000) & (amounts <= 50000)] = 75
        scores[(amounts > 5000) & (amounts <= 20000)] = 55
        scores[(amounts > 1000) & (amounts <= 5000)] = 35
        scores[(amounts > 100) & (amounts <= 1000)] = 20
        scores[(amounts > 0) & (amounts <= 100)] = 10
        scores[amounts < 10] = 70  # Abnormally small transactions
        return scores
    
    def _calculate_time_scores_vectorized(self, hours: np.ndarray) -> np.ndarray:
        """Vectorized time score calculation"""
        scores = np.full_like(hours, 20.0, dtype=float)  # Default score
        scores[(hours <= 2) | (hours >= 23)] = 80  # Late night
        scores[((hours <= 5) & (hours > 2)) | ((hours >= 22) & (hours < 23))] = 60  # Night
        scores[(hours >= 9) & (hours <= 17)] = 10  # Business hours
        return scores

    def _calculate_account_scores_vectorized(self, account_ages: np.ndarray) -> np.ndarray:
        """Vectorized account score calculation"""
        scores = np.zeros_like(account_ages, dtype=float)
        scores[account_ages < 1] = 80
        scores[(account_ages >= 1) & (account_ages < 7)] = 65
        scores[(account_ages >= 7) & (account_ages < 30)] = 45
        scores[(account_ages >= 30) & (account_ages < 90)] = 25
        scores[account_ages >= 90] = 10
        return scores

    def _calculate_age_scores_vectorized(self, customer_ages: np.ndarray) -> np.ndarray:
        """Vectorized customer age score calculation"""
        scores = np.full_like(customer_ages, 10.0, dtype=float)  # Default score
        scores[(customer_ages < 18) | (customer_ages > 80)] = 40  # Abnormal age
        scores[(customer_ages < 21) | (customer_ages > 70)] = 25  # Boundary age
        return scores
    
    def _generate_labels_vectorized(self, data: pd.DataFrame,
                                  risk_results: Dict[str, Any]) -> Tuple[List[int], List[float]]:
        """Vectorized pseudo label generation"""
        results = risk_results.get('results', [])
        if not results:
            logger.warning("Risk scoring results are empty")
            return [], []
        
        # æå–é£é™©è¯„åˆ†å’Œç­‰çº§
        risk_scores = np.array([r.get('risk_score', 0) for r in results])
        risk_levels = [r.get('risk_level', 'low') for r in results]
        
        # å‘é‡åŒ–ç”Ÿæˆæ ‡ç­¾å’Œç½®ä¿¡åº¦
        labels = np.zeros(len(risk_scores), dtype=int)
        confidences = np.zeros(len(risk_scores), dtype=float)
        
        for i, (score, level) in enumerate(zip(risk_scores, risk_levels)):
            # ç¡®ä¿scoreæ˜¯æ ‡é‡å€¼
            score = float(score) if hasattr(score, '__iter__') and not isinstance(score, str) else score

            if level == 'critical':
                # æé«˜é£é™©ï¼šç¡®å®šæ˜¯æ¬ºè¯ˆ
                labels[i] = 1
                confidences[i] = min(0.95, 0.88 + (score - 75) / 100)
            elif level == 'high':
                # é«˜é£é™©ï¼šå¤§éƒ¨åˆ†æ ‡è®°ä¸ºæ¬ºè¯ˆ
                labels[i] = 1
                confidences[i] = min(0.92, 0.75 + (score - 55) / 80)  # æé«˜ç½®ä¿¡åº¦
            elif level == 'medium':
                # ä¸­é£é™©ï¼šéƒ¨åˆ†æ ‡è®°ä¸ºæ¬ºè¯ˆï¼ŒåŸºäºè¯„åˆ†
                if score >= 45:  # ä¸­é£é™©ä¸­çš„é«˜åˆ†
                    labels[i] = 1
                    confidences[i] = min(0.85, 0.70 + (score - 45) / 60)  # æé«˜ç½®ä¿¡åº¦
                else:
                    labels[i] = 0
                    confidences[i] = min(0.88, 0.75 + (45 - score) / 80)  # æé«˜ç½®ä¿¡åº¦
            else:  # low
                # ä½é£é™©ï¼šå°‘é‡é«˜åˆ†æ ‡è®°ä¸ºæ¬ºè¯ˆ
                if score >= 30:  # ä½é£é™©ä¸­çš„å¼‚å¸¸é«˜åˆ†
                    labels[i] = 1
                    confidences[i] = min(0.82, 0.65 + (score - 30) / 100)  # æé«˜ç½®ä¿¡åº¦
                else:
                    labels[i] = 0
                    confidences[i] = min(0.90, 0.80 + (30 - score) / 150)  # æé«˜ç½®ä¿¡åº¦
        
        # æ·»åŠ ä¸€äº›éšæœºæ€§ä»¥é¿å…è¿‡åº¦ç¡®å®šæ€§
        noise = np.random.normal(0, 0.02, len(confidences))
        confidences = np.clip(confidences + noise, 0.1, 0.95)

        # è°ƒè¯•ä¿¡æ¯
        fraud_count = sum(labels)
        fraud_rate = fraud_count / len(labels) * 100 if len(labels) > 0 else 0
        logger.info(f"Generated {fraud_count} fraud labels out of {len(labels)} total ({fraud_rate:.2f}%)")

        return labels.tolist(), confidences.tolist()
    
    def _generate_fast_quality_report(self, labels: List[int], confidences: List[float],
                                    high_quality_indices: List[int]) -> Dict[str, Any]:
        """Generate fast quality report"""
        if not labels or not confidences:
            return {'quality_score': 0, 'quality_level': 'poor'}
        
        # è®¡ç®—åŸºç¡€è´¨é‡æŒ‡æ ‡
        avg_confidence = np.mean(confidences)
        hq_ratio = len(high_quality_indices) / len(labels)
        fraud_rate = np.mean(labels)
        
        # è®¡ç®—è´¨é‡è¯„åˆ†
        base_score = avg_confidence * 100
        hq_bonus = hq_ratio * 20
        
        # æ ‡ç­¾å¹³è¡¡æ€§è¯„ä¼° (è°ƒæ•´æœŸæœ›çš„æ¬ºè¯ˆç‡èŒƒå›´)
        if 0.03 <= fraud_rate <= 0.20:  # æ‰©å¤§åˆç†èŒƒå›´
            balance_bonus = 15
        elif 0.01 <= fraud_rate <= 0.30:  # æ›´å®½æ¾çš„èŒƒå›´
            balance_bonus = 8
        else:
            balance_bonus = 0
        
        quality_score = min(100, base_score + hq_bonus + balance_bonus)
        
        # ç¡®å®šè´¨é‡ç­‰çº§
        if quality_score >= 85:
            quality_level = 'excellent'
        elif quality_score >= 75:
            quality_level = 'good'
        elif quality_score >= 60:
            quality_level = 'fair'
        else:
            quality_level = 'poor'
        
        return {
            'quality_score': round(quality_score, 1),
            'quality_level': quality_level,
            'avg_confidence': round(avg_confidence, 3),
            'high_quality_ratio': round(hq_ratio, 3),
            'fraud_rate': round(fraud_rate, 3),
            'balance_score': balance_bonus
        }
    
    def _empty_result(self) -> Dict[str, Any]:
        """Return empty result"""
        return {
            'success': False,
            'strategy': 'fast_generation',
            'all_labels': [],
            'all_confidences': [],
            'high_quality_indices': [],
            'high_quality_labels': [],
            'high_quality_confidences': [],
            'quality_report': {'quality_score': 0, 'quality_level': 'poor'},
            'metadata': {
                'total_samples': 0,
                'high_quality_count': 0,
                'high_quality_rate': 0,
                'avg_confidence_all': 0,
                'avg_confidence_hq': 0,
                'fraud_rate_all': 0,
                'fraud_rate_hq': 0
            },
            'error': 'No data to process'
        }
