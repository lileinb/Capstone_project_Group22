"""
Feature Selector
Select the most important 15-20 core features from 52 features to improve computational performance
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Any
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FeatureSelector:
    """Feature Selector"""

    def __init__(self, target_features: int = 18):
        """
        Initialize feature selector

        Args:
            target_features: Target number of features
        """
        self.target_features = target_features
        self.selected_features = []
        self.feature_importance_scores = {}
        self.selection_methods = {}
        
        # Predefined core features (high business importance) - ä¼˜åŒ–ç‰ˆ
        self.core_features = [
            'transaction_amount',
            'customer_age',
            'account_age_days',
            'transaction_hour',
            'device_used',
            'payment_method',
            'shipping_address',
            'billing_address'
        ]

        # èšç±»å‹å¥½ç‰¹å¾ï¼ˆé«˜åŒºåˆ†åº¦ï¼‰
        self.clustering_friendly_features = [
            # é‡‘é¢ç›¸å…³
            'amount_zscore', 'amount_percentile', 'amount_log', 'amount_rank',
            'is_large_amount', 'amount_deviation', 'amount_risk_score',
            # æ—¶é—´ç›¸å…³
            'hour_risk_score', 'is_night_transaction', 'is_weekend',
            'time_risk_score', 'is_off_hours', 'is_deep_night',
            # è´¦æˆ·ç›¸å…³
            'account_age_risk_score', 'is_new_account', 'is_very_new_account',
            'account_maturity', 'customer_risk_score',
            # å¤åˆç‰¹å¾
            'composite_risk_score', 'anomaly_score', 'risk_interaction',
            'behavior_consistency_score', 'pattern_deviation_score'
        ]

        # Feature category grouping
        self.feature_groups = {
            'amount_features': [
                'transaction_amount', 'amount_zscore', 'amount_log', 'amount_percentile',
                'amount_category', 'high_amount_flag'
            ],
            'time_features': [
                'transaction_hour', 'hour_category', 'is_weekend', 'is_holiday',
                'time_since_last_transaction', 'transaction_frequency'
            ],
            'account_features': [
                'customer_age', 'account_age_days', 'age_category', 'account_age_category',
                'customer_risk_score', 'account_activity_score'
            ],
            'device_payment_features': [
                'device_used', 'payment_method', 'device_risk_score', 'payment_risk_score',
                'device_payment_combination'
            ],
            'address_features': [
                'shipping_address', 'billing_address', 'address_match', 'address_risk_score',
                'shipping_country', 'billing_country'
            ],
            'product_features': [
                'product_category', 'quantity', 'category_risk_score', 'quantity_category',
                'electronics_flag', 'high_risk_category_flag'
            ],
            'behavioral_features': [
                'transaction_velocity', 'unusual_pattern_flag', 'behavior_consistency_score',
                'pattern_deviation_score', 'anomaly_score'
            ],
            'statistical_features': [
                'amount_zscore', 'age_zscore', 'account_age_zscore', 'statistical_outlier_score',
                'composite_risk_score', 'risk_interaction_score'
            ]
        }
    
    def select_features(self, data: pd.DataFrame, 
                       pseudo_labels: np.ndarray = None,
                       method: str = 'hybrid') -> List[str]:
        """
        é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾
        
        Args:
            data: ç‰¹å¾æ•°æ®
            pseudo_labels: ä¼ªæ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
            method: é€‰æ‹©æ–¹æ³• ('statistical', 'importance', 'hybrid')
            
        Returns:
            é€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹ç‰¹å¾é€‰æ‹©ï¼ŒåŸå§‹ç‰¹å¾æ•°: {len(data.columns)}")
            
            # 1. ç¡®ä¿æ ¸å¿ƒç‰¹å¾è¢«åŒ…å«
            available_core_features = [f for f in self.core_features if f in data.columns]
            logger.info(f"å¯ç”¨æ ¸å¿ƒç‰¹å¾: {len(available_core_features)}")
            
            # 2. æ ¹æ®æ–¹æ³•é€‰æ‹©ç‰¹å¾
            if method == 'statistical':
                selected_features = self._statistical_selection(data, pseudo_labels)
            elif method == 'importance':
                selected_features = self._importance_based_selection(data, pseudo_labels)
            else:  # hybrid
                selected_features = self._hybrid_selection(data, pseudo_labels)
            
            # 3. ç¡®ä¿æ ¸å¿ƒç‰¹å¾è¢«åŒ…å«
            final_features = list(set(available_core_features + selected_features))
            
            # 4. å¦‚æœç‰¹å¾æ•°é‡è¶…è¿‡ç›®æ ‡ï¼Œè¿›è¡Œè¿›ä¸€æ­¥ç­›é€‰
            if len(final_features) > self.target_features:
                final_features = self._final_selection(data, final_features, pseudo_labels)
            
            # 5. å¦‚æœç‰¹å¾æ•°é‡ä¸è¶³ï¼Œè¡¥å……é‡è¦ç‰¹å¾
            if len(final_features) < self.target_features:
                final_features = self._supplement_features(data, final_features)
            
            self.selected_features = final_features[:self.target_features]
            
            logger.info(f"ç‰¹å¾é€‰æ‹©å®Œæˆï¼Œé€‰æ‹©äº† {len(self.selected_features)} ä¸ªç‰¹å¾")
            logger.info(f"é€‰æ‹©çš„ç‰¹å¾: {self.selected_features}")
            
            return self.selected_features

        except Exception as e:
            logger.error(f"ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            # è¿”å›æ ¸å¿ƒç‰¹å¾ä½œä¸ºå¤‡é€‰
            return [f for f in self.core_features if f in data.columns][:self.target_features]

    def select_clustering_optimized_features(self, data: pd.DataFrame,
                                           max_features: int = 12) -> List[str]:
        """
        ä¸“é—¨ä¸ºèšç±»ä¼˜åŒ–çš„ç‰¹å¾é€‰æ‹©

        Args:
            data: ç‰¹å¾æ•°æ®
            max_features: æœ€å¤§ç‰¹å¾æ•°é‡

        Returns:
            ä¼˜åŒ–çš„ç‰¹å¾åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ¯ å¼€å§‹èšç±»ä¼˜åŒ–ç‰¹å¾é€‰æ‹©ï¼ŒåŸå§‹ç‰¹å¾æ•°: {len(data.columns)}")

            # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºå¢å¼ºç‰¹å¾
            enhanced_data = self._create_clustering_features(data.copy())

            # ç¬¬äºŒæ­¥ï¼šç‰¹å¾è´¨é‡è¯„ä¼°
            feature_scores = self._evaluate_clustering_feature_quality(enhanced_data)

            # ç¬¬ä¸‰æ­¥ï¼šæ™ºèƒ½ç‰¹å¾ç»„åˆé€‰æ‹©
            optimal_features = self._select_optimal_clustering_combination(
                enhanced_data, feature_scores, max_features
            )

            logger.info(f"âœ… èšç±»ä¼˜åŒ–ç‰¹å¾é€‰æ‹©å®Œæˆï¼Œé€‰æ‹©äº† {len(optimal_features)} ä¸ªç‰¹å¾")
            logger.info(f"é€‰æ‹©çš„ç‰¹å¾: {optimal_features}")

            return optimal_features

        except Exception as e:
            logger.error(f"èšç±»ä¼˜åŒ–ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€ç‰¹å¾
            return [f for f in self.core_features if f in data.columns][:max_features]

    def _create_clustering_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºèšç±»å‹å¥½çš„ç‰¹å¾"""
        logger.info("ğŸ”§ åˆ›å»ºèšç±»å‹å¥½ç‰¹å¾")

        # é‡‘é¢ç‰¹å¾å¢å¼º
        if 'transaction_amount' in data.columns:
            # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
            data['amount_log'] = np.log1p(data['transaction_amount'])
            data['amount_sqrt'] = np.sqrt(data['transaction_amount'])
            data['amount_zscore'] = (data['transaction_amount'] - data['transaction_amount'].mean()) / data['transaction_amount'].std()
            data['amount_percentile'] = data['transaction_amount'].rank(pct=True)
            data['amount_rank'] = data['transaction_amount'].rank()

            # å¼‚å¸¸æ£€æµ‹ç‰¹å¾
            Q1 = data['transaction_amount'].quantile(0.25)
            Q3 = data['transaction_amount'].quantile(0.75)
            IQR = Q3 - Q1
            data['is_amount_outlier'] = ((data['transaction_amount'] < (Q1 - 1.5 * IQR)) |
                                       (data['transaction_amount'] > (Q3 + 1.5 * IQR))).astype(int)
            data['amount_deviation'] = abs(data['transaction_amount'] - data['transaction_amount'].median())

            # åˆ†ç±»ç‰¹å¾
            data['is_large_amount'] = (data['transaction_amount'] > data['transaction_amount'].quantile(0.8)).astype(int)
            data['is_small_amount'] = (data['transaction_amount'] < data['transaction_amount'].quantile(0.2)).astype(int)

        # æ—¶é—´ç‰¹å¾å¢å¼º
        if 'transaction_hour' in data.columns:
            # æ—¶é—´åˆ†ç±»
            data['is_night_transaction'] = data['transaction_hour'].isin([22, 23, 0, 1, 2, 3, 4, 5]).astype(int)
            data['is_business_hours'] = data['transaction_hour'].isin([9, 10, 11, 12, 13, 14, 15, 16, 17]).astype(int)
            data['is_evening'] = data['transaction_hour'].isin([18, 19, 20, 21]).astype(int)
            data['is_early_morning'] = data['transaction_hour'].isin([6, 7, 8]).astype(int)

            # é£é™©è¯„åˆ†
            risk_hours = {0: 3, 1: 3, 2: 3, 3: 3, 4: 2, 5: 2, 22: 2, 23: 3}
            data['hour_risk_score'] = data['transaction_hour'].map(risk_hours).fillna(1)

            # å‘¨æœŸæ€§ç‰¹å¾
            data['hour_sin'] = np.sin(2 * np.pi * data['transaction_hour'] / 24)
            data['hour_cos'] = np.cos(2 * np.pi * data['transaction_hour'] / 24)

        # è´¦æˆ·ç‰¹å¾å¢å¼º
        if 'account_age_days' in data.columns:
            data['account_age_log'] = np.log1p(data['account_age_days'])
            data['is_new_account'] = (data['account_age_days'] < 30).astype(int)
            data['is_very_new_account'] = (data['account_age_days'] < 7).astype(int)
            data['is_mature_account'] = (data['account_age_days'] > 365).astype(int)
            data['account_age_percentile'] = data['account_age_days'].rank(pct=True)

        # å®¢æˆ·ç‰¹å¾å¢å¼º
        if 'customer_age' in data.columns:
            data['customer_age_zscore'] = (data['customer_age'] - data['customer_age'].mean()) / data['customer_age'].std()
            data['is_young_customer'] = (data['customer_age'] < 25).astype(int)
            data['is_senior_customer'] = (data['customer_age'] > 65).astype(int)
            data['is_prime_age'] = ((data['customer_age'] >= 25) & (data['customer_age'] <= 45)).astype(int)

        # å¤åˆç‰¹å¾
        risk_components = []
        if 'amount_zscore' in data.columns:
            risk_components.append('amount_zscore')
        if 'hour_risk_score' in data.columns:
            risk_components.append('hour_risk_score')
        if 'is_new_account' in data.columns:
            risk_components.append('is_new_account')

        if len(risk_components) >= 2:
            data['composite_risk'] = data[risk_components].sum(axis=1)
            data['risk_interaction'] = data[risk_components].prod(axis=1)

        logger.info(f"ç‰¹å¾åˆ›å»ºå®Œæˆï¼Œå½“å‰ç‰¹å¾æ•°: {len(data.columns)}")
        return data

    def _evaluate_clustering_feature_quality(self, data: pd.DataFrame) -> Dict[str, float]:
        """è¯„ä¼°ç‰¹å¾å¯¹èšç±»çš„è´¨é‡"""
        feature_scores = {}
        numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()

        # æ’é™¤æ ‡ç­¾å’ŒIDç±»ç‰¹å¾
        exclude_patterns = ['is_fraudulent', '_id', 'customer_id', 'transaction_id']
        candidate_features = [f for f in numeric_features
                            if not any(pattern in f.lower() for pattern in exclude_patterns)]

        for feature in candidate_features:
            if feature not in data.columns:
                continue

            feature_data = data[feature].dropna()
            if len(feature_data) == 0:
                feature_scores[feature] = 0.0
                continue

            score = 0.0

            # 1. æ–¹å·®è¯„åˆ† (25%) - é«˜æ–¹å·®æ›´å¥½
            variance = feature_data.var()
            if variance > 0:
                # æ ‡å‡†åŒ–æ–¹å·®è¯„åˆ†
                variance_score = min(np.log1p(variance) / 10, 1.0)
                score += variance_score * 0.25

            # 2. åˆ†å¸ƒè¯„åˆ† (20%) - åå‘æœ‰åŒºåˆ†åº¦çš„åˆ†å¸ƒ
            try:
                # ä½¿ç”¨å››åˆ†ä½æ•°èŒƒå›´è¯„ä¼°åˆ†å¸ƒ
                q75, q25 = np.percentile(feature_data, [75, 25])
                iqr = q75 - q25
                if iqr > 0:
                    # IQRç›¸å¯¹äºæ ‡å‡†å·®çš„æ¯”ä¾‹
                    iqr_ratio = iqr / (feature_data.std() + 1e-8)
                    distribution_score = min(iqr_ratio / 2, 1.0)
                    score += distribution_score * 0.2
            except:
                pass

            # 3. å”¯ä¸€å€¼è¯„åˆ† (15%) - é€‚ä¸­çš„å”¯ä¸€å€¼æ•°é‡æœ€å¥½
            n_unique = feature_data.nunique()
            total_samples = len(feature_data)
            unique_ratio = n_unique / total_samples

            if 0.01 <= unique_ratio <= 0.8:  # ç†æƒ³èŒƒå›´
                unique_score = 1.0
            elif unique_ratio > 0.8:  # å¤ªå¤šå”¯ä¸€å€¼
                unique_score = max(0.2, 1.0 - (unique_ratio - 0.8) * 2)
            else:  # å¤ªå°‘å”¯ä¸€å€¼
                unique_score = unique_ratio / 0.01

            score += unique_score * 0.15

            # 4. ç‰¹å¾ç±»å‹å¥–åŠ± (25%)
            type_score = 0.0
            if any(suffix in feature for suffix in ['_zscore', '_percentile', '_rank']):
                type_score = 1.0  # æ ‡å‡†åŒ–ç‰¹å¾æœ€å¥½
            elif any(suffix in feature for suffix in ['_log', '_sqrt', '_deviation']):
                type_score = 0.8  # å˜æ¢ç‰¹å¾å¾ˆå¥½
            elif any(prefix in feature for prefix in ['is_', 'has_']):
                type_score = 0.6  # äºŒå…ƒç‰¹å¾ä¸é”™
            elif any(suffix in feature for suffix in ['_score', '_risk']):
                type_score = 0.9  # è¯„åˆ†ç‰¹å¾å¾ˆå¥½
            elif feature in self.core_features:
                type_score = 0.7  # æ ¸å¿ƒç‰¹å¾ä¸é”™
            else:
                type_score = 0.3  # å…¶ä»–ç‰¹å¾ä¸€èˆ¬

            score += type_score * 0.25

            # 5. èšç±»å‹å¥½æ€§å¥–åŠ± (15%)
            clustering_bonus = 0.0
            if feature in self.clustering_friendly_features:
                clustering_bonus = 1.0
            elif any(pattern in feature for pattern in ['composite', 'interaction', 'combined']):
                clustering_bonus = 0.8

            score += clustering_bonus * 0.15

            feature_scores[feature] = min(score, 1.0)  # é™åˆ¶åœ¨[0,1]èŒƒå›´å†…

        return feature_scores

    def _select_optimal_clustering_combination(self, data: pd.DataFrame,
                                             feature_scores: Dict[str, float],
                                             max_features: int) -> List[str]:
        """é€‰æ‹©æœ€ä¼˜çš„ç‰¹å¾ç»„åˆ"""
        # æŒ‰åˆ†æ•°æ’åº
        sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)

        selected_features = []
        correlation_threshold = 0.85  # ç›¸å…³æ€§é˜ˆå€¼

        # ç¡®ä¿åŒ…å«æ ¸å¿ƒç‰¹å¾ä¸­çš„æœ€ä½³ç‰¹å¾
        core_available = [f for f in self.core_features if f in data.columns]
        for feature in core_available[:3]:  # æœ€å¤š3ä¸ªæ ¸å¿ƒç‰¹å¾
            if len(selected_features) < max_features:
                selected_features.append(feature)

        # æ·»åŠ é«˜åˆ†ç‰¹å¾
        for feature, score in sorted_features:
            if len(selected_features) >= max_features:
                break

            if feature in selected_features:
                continue

            if score < 0.4:  # åˆ†æ•°å¤ªä½è·³è¿‡
                continue

            # æ£€æŸ¥ç›¸å…³æ€§
            is_redundant = False
            for selected_feature in selected_features:
                if feature in data.columns and selected_feature in data.columns:
                    try:
                        corr = abs(data[feature].corr(data[selected_feature]))
                        if corr > correlation_threshold:
                            is_redundant = True
                            break
                    except:
                        continue

            if not is_redundant:
                selected_features.append(feature)

        # ç¡®ä¿ç‰¹å¾æ•°é‡åˆç†
        if len(selected_features) < 4:
            # æ·»åŠ å¤‡ç”¨ç‰¹å¾
            backup_features = [f for f in data.select_dtypes(include=[np.number]).columns
                             if f not in selected_features and 'is_fraudulent' not in f]
            selected_features.extend(backup_features[:4-len(selected_features)])

        return selected_features[:max_features]

    def _statistical_selection(self, data: pd.DataFrame,
                             pseudo_labels: np.ndarray = None) -> List[str]:
        """åŸºäºç»Ÿè®¡æ–¹æ³•çš„ç‰¹å¾é€‰æ‹©"""
        selected_features = []
        
        try:
            # è®¡ç®—ç‰¹å¾çš„ç»Ÿè®¡é‡è¦æ€§
            numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()
            
            # æ–¹å·®ç­›é€‰
            feature_variances = data[numeric_features].var()
            high_variance_features = feature_variances[feature_variances > 0.01].index.tolist()
            
            # ç›¸å…³æ€§åˆ†æ
            if len(high_variance_features) > 1:
                correlation_matrix = data[high_variance_features].corr().abs()
                
                # ç§»é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾
                upper_triangle = correlation_matrix.where(
                    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
                )
                
                to_drop = [column for column in upper_triangle.columns 
                          if any(upper_triangle[column] > 0.9)]
                
                selected_features = [f for f in high_variance_features if f not in to_drop]
            else:
                selected_features = high_variance_features
            
            # å¦‚æœæœ‰ä¼ªæ ‡ç­¾ï¼Œä½¿ç”¨ç›‘ç£æ–¹æ³•
            if pseudo_labels is not None and len(np.unique(pseudo_labels)) > 1:
                try:
                    # ä½¿ç”¨äº’ä¿¡æ¯è¿›è¡Œç‰¹å¾é€‰æ‹©
                    selector = SelectKBest(score_func=mutual_info_classif, 
                                         k=min(self.target_features, len(selected_features)))
                    
                    X_selected = data[selected_features].fillna(0)
                    selector.fit(X_selected, pseudo_labels)
                    
                    selected_indices = selector.get_support(indices=True)
                    selected_features = [selected_features[i] for i in selected_indices]
                    
                except Exception as e:
                    logger.warning(f"ç›‘ç£ç‰¹å¾é€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨æ— ç›‘ç£æ–¹æ³•: {e}")
            
            return selected_features
            
        except Exception as e:
            logger.error(f"ç»Ÿè®¡ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            return []
    
    def _importance_based_selection(self, data: pd.DataFrame,
                                  pseudo_labels: np.ndarray = None) -> List[str]:
        """åŸºäºé‡è¦æ€§çš„ç‰¹å¾é€‰æ‹©"""
        selected_features = []
        
        try:
            numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()
            
            if pseudo_labels is not None and len(np.unique(pseudo_labels)) > 1:
                # ä½¿ç”¨éšæœºæ£®æ—è®¡ç®—ç‰¹å¾é‡è¦æ€§
                rf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)
                
                X = data[numeric_features].fillna(0)
                rf.fit(X, pseudo_labels)
                
                # è·å–ç‰¹å¾é‡è¦æ€§
                feature_importance = pd.DataFrame({
                    'feature': numeric_features,
                    'importance': rf.feature_importances_
                }).sort_values('importance', ascending=False)
                
                selected_features = feature_importance.head(self.target_features)['feature'].tolist()
                
                # ä¿å­˜é‡è¦æ€§åˆ†æ•°
                self.feature_importance_scores = dict(zip(
                    feature_importance['feature'], 
                    feature_importance['importance']
                ))
                
            else:
                # æ— ç›‘ç£æƒ…å†µä¸‹ï¼ŒåŸºäºä¸šåŠ¡è§„åˆ™é€‰æ‹©
                selected_features = self._business_rule_selection(data)
            
            return selected_features
            
        except Exception as e:
            logger.error(f"é‡è¦æ€§ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            return []
    
    def _hybrid_selection(self, data: pd.DataFrame,
                         pseudo_labels: np.ndarray = None) -> List[str]:
        """æ··åˆæ–¹æ³•ç‰¹å¾é€‰æ‹©"""
        try:
            # 1. ç»Ÿè®¡æ–¹æ³•é€‰æ‹©
            statistical_features = self._statistical_selection(data, pseudo_labels)
            
            # 2. é‡è¦æ€§æ–¹æ³•é€‰æ‹©
            importance_features = self._importance_based_selection(data, pseudo_labels)
            
            # 3. ä¸šåŠ¡è§„åˆ™é€‰æ‹©
            business_features = self._business_rule_selection(data)
            
            # 4. åˆå¹¶å¹¶å»é‡
            all_features = list(set(statistical_features + importance_features + business_features))
            
            # 5. æŒ‰é‡è¦æ€§æ’åº
            if self.feature_importance_scores:
                all_features.sort(key=lambda x: self.feature_importance_scores.get(x, 0), reverse=True)
            
            return all_features
            
        except Exception as e:
            logger.error(f"æ··åˆç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            return self._business_rule_selection(data)
    
    def _business_rule_selection(self, data: pd.DataFrame) -> List[str]:
        """åŸºäºä¸šåŠ¡è§„åˆ™çš„ç‰¹å¾é€‰æ‹©"""
        selected_features = []
        
        # ä»æ¯ä¸ªç‰¹å¾ç»„ä¸­é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾
        for group_name, features in self.feature_groups.items():
            available_features = [f for f in features if f in data.columns]
            
            if group_name == 'amount_features':
                # é‡‘é¢ç‰¹å¾ï¼šé€‰æ‹©2-3ä¸ªæœ€é‡è¦çš„
                priority = ['transaction_amount', 'amount_zscore', 'amount_percentile']
                selected = [f for f in priority if f in available_features][:3]
                selected_features.extend(selected)
                
            elif group_name == 'time_features':
                # æ—¶é—´ç‰¹å¾ï¼šé€‰æ‹©2ä¸ªæœ€é‡è¦çš„
                priority = ['transaction_hour', 'hour_category']
                selected = [f for f in priority if f in available_features][:2]
                selected_features.extend(selected)
                
            elif group_name == 'account_features':
                # è´¦æˆ·ç‰¹å¾ï¼šé€‰æ‹©3ä¸ªæœ€é‡è¦çš„
                priority = ['customer_age', 'account_age_days', 'customer_risk_score']
                selected = [f for f in priority if f in available_features][:3]
                selected_features.extend(selected)
                
            elif group_name == 'device_payment_features':
                # è®¾å¤‡æ”¯ä»˜ç‰¹å¾ï¼šé€‰æ‹©2-3ä¸ª
                priority = ['device_used', 'payment_method', 'device_payment_combination']
                selected = [f for f in priority if f in available_features][:3]
                selected_features.extend(selected)
                
            elif group_name == 'address_features':
                # åœ°å€ç‰¹å¾ï¼šé€‰æ‹©2ä¸ª
                priority = ['address_match', 'address_risk_score']
                selected = [f for f in priority if f in available_features][:2]
                selected_features.extend(selected)
                
            else:
                # å…¶ä»–ç‰¹å¾ç»„ï¼šé€‰æ‹©1-2ä¸ªæœ€é‡è¦çš„
                selected_features.extend(available_features[:2])
        
        return list(set(selected_features))
    
    def _final_selection(self, data: pd.DataFrame, features: List[str],
                        pseudo_labels: np.ndarray = None) -> List[str]:
        """æœ€ç»ˆç‰¹å¾é€‰æ‹©"""
        if len(features) <= self.target_features:
            return features
        
        # ä¼˜å…ˆä¿ç•™æ ¸å¿ƒç‰¹å¾
        core_in_features = [f for f in features if f in self.core_features]
        other_features = [f for f in features if f not in self.core_features]
        
        # å¦‚æœæœ‰é‡è¦æ€§åˆ†æ•°ï¼ŒæŒ‰åˆ†æ•°æ’åº
        if self.feature_importance_scores:
            other_features.sort(key=lambda x: self.feature_importance_scores.get(x, 0), reverse=True)
        
        # é€‰æ‹©ç‰¹å¾
        remaining_slots = self.target_features - len(core_in_features)
        final_features = core_in_features + other_features[:remaining_slots]
        
        return final_features
    
    def _supplement_features(self, data: pd.DataFrame, current_features: List[str]) -> List[str]:
        """è¡¥å……ç‰¹å¾åˆ°ç›®æ ‡æ•°é‡"""
        all_features = data.columns.tolist()
        available_features = [f for f in all_features if f not in current_features]
        
        # ä¼˜å…ˆæ·»åŠ æ ¸å¿ƒç‰¹å¾
        core_to_add = [f for f in self.core_features if f in available_features]
        
        # ç„¶åæ·»åŠ å…¶ä»–ç‰¹å¾
        other_to_add = [f for f in available_features if f not in core_to_add]
        
        needed = self.target_features - len(current_features)
        to_add = (core_to_add + other_to_add)[:needed]
        
        return current_features + to_add
    
    def get_feature_importance_report(self) -> Dict[str, Any]:
        """è·å–ç‰¹å¾é‡è¦æ€§æŠ¥å‘Š"""
        return {
            'selected_features': self.selected_features,
            'feature_count': len(self.selected_features),
            'target_count': self.target_features,
            'importance_scores': self.feature_importance_scores,
            'core_features_included': [f for f in self.selected_features if f in self.core_features],
            'selection_summary': {
                'total_selected': len(self.selected_features),
                'core_features': len([f for f in self.selected_features if f in self.core_features]),
                'other_features': len([f for f in self.selected_features if f not in self.core_features])
            }
        }
