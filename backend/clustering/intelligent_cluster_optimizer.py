#!/usr/bin/env python3
"""
æ™ºèƒ½èšç±»ä¼˜åŒ–å™¨
è‡ªåŠ¨åŒ–ç‰¹å¾é€‰æ‹©ã€å‚æ•°è°ƒä¼˜ã€é£é™©é˜ˆå€¼ä¼˜åŒ–
"""

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.decomposition import PCA
import logging
from typing import Dict, List, Tuple, Any, Optional
import itertools

logger = logging.getLogger(__name__)

class IntelligentClusterOptimizer:
    """æ™ºèƒ½èšç±»ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        self.scaler = StandardScaler()
        self.best_config = None
        self.optimization_history = []
        
        # ç‰¹å¾é‡è¦æ€§æƒé‡
        self.feature_importance = {
            'transaction_amount': 0.25,
            'customer_age': 0.15,
            'account_age_days': 0.20,
            'transaction_hour': 0.15,
            'quantity': 0.10,
            'is_fraudulent': 0.15  # ç”¨äºç‰¹å¾é€‰æ‹©ï¼Œä¸ç”¨äºèšç±»
        }
    
    def auto_optimize_clustering(self, data: pd.DataFrame, 
                               target_risk_distribution: Dict[str, float] = None) -> Dict[str, Any]:
        """
        è‡ªåŠ¨ä¼˜åŒ–èšç±»
        
        Args:
            data: è¾“å…¥æ•°æ®
            target_risk_distribution: ç›®æ ‡é£é™©åˆ†å¸ƒ {'low': 0.4, 'medium': 0.3, 'high': 0.2, 'critical': 0.1}
        
        Returns:
            æœ€ä¼˜èšç±»é…ç½®å’Œç»“æœ
        """
        logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½èšç±»ä¼˜åŒ–")
        
        if target_risk_distribution is None:
            target_risk_distribution = {'low': 0.5, 'medium': 0.3, 'high': 0.15, 'critical': 0.05}
        
        try:
            # ç¬¬1æ­¥ï¼šæ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
            processed_data = self._advanced_feature_engineering(data)
            
            # ç¬¬2æ­¥ï¼šæ™ºèƒ½ç‰¹å¾é€‰æ‹©
            optimal_features = self._intelligent_feature_selection(processed_data)
            
            # ç¬¬3æ­¥ï¼šèšç±»ç®—æ³•å’Œå‚æ•°ä¼˜åŒ–
            best_clustering = self._optimize_clustering_parameters(processed_data[optimal_features])
            
            # ç¬¬4æ­¥ï¼šé£é™©é˜ˆå€¼è‡ªé€‚åº”è°ƒæ•´
            optimal_thresholds = self._adaptive_risk_threshold_optimization(
                best_clustering, processed_data, target_risk_distribution
            )
            
            # ç¬¬5æ­¥ï¼šç”Ÿæˆæœ€ç»ˆç»“æœ
            final_result = self._generate_optimized_result(
                best_clustering, optimal_thresholds, optimal_features, processed_data
            )
            
            logger.info("âœ… æ™ºèƒ½èšç±»ä¼˜åŒ–å®Œæˆ")
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½èšç±»ä¼˜åŒ–å¤±è´¥: {e}")
            return self._fallback_clustering(data)
    
    def _advanced_feature_engineering(self, data: pd.DataFrame) -> pd.DataFrame:
        """é«˜çº§ç‰¹å¾å·¥ç¨‹"""
        logger.info("ğŸ”§ æ‰§è¡Œé«˜çº§ç‰¹å¾å·¥ç¨‹")
        
        processed_data = data.copy()
        
        # åŸºç¡€ç‰¹å¾æ ‡å‡†åŒ–
        numeric_features = ['transaction_amount', 'quantity', 'customer_age', 'account_age_days']
        for feature in numeric_features:
            if feature in processed_data.columns:
                # Z-scoreæ ‡å‡†åŒ–
                processed_data[f'{feature}_zscore'] = (
                    processed_data[feature] - processed_data[feature].mean()
                ) / processed_data[feature].std()
                
                # åˆ†ä½æ•°ç‰¹å¾
                processed_data[f'{feature}_percentile'] = processed_data[feature].rank(pct=True)
        
        # æ—¶é—´ç‰¹å¾å¢å¼º
        if 'transaction_hour' in processed_data.columns:
            processed_data['is_night_transaction'] = (
                (processed_data['transaction_hour'] >= 22) | 
                (processed_data['transaction_hour'] <= 6)
            ).astype(int)
            
            processed_data['is_business_hour'] = (
                (processed_data['transaction_hour'] >= 9) & 
                (processed_data['transaction_hour'] <= 17)
            ).astype(int)
        
        # è´¦æˆ·é£é™©ç‰¹å¾
        if 'account_age_days' in processed_data.columns:
            processed_data['is_new_account'] = (processed_data['account_age_days'] < 30).astype(int)
            processed_data['is_very_new_account'] = (processed_data['account_age_days'] < 7).astype(int)
            processed_data['account_age_risk_score'] = np.where(
                processed_data['account_age_days'] < 30, 
                100 - processed_data['account_age_days'] * 3, 
                10
            )
        
        # äº¤æ˜“é‡‘é¢é£é™©ç‰¹å¾
        if 'transaction_amount' in processed_data.columns:
            amount_q95 = processed_data['transaction_amount'].quantile(0.95)
            amount_q75 = processed_data['transaction_amount'].quantile(0.75)
            
            processed_data['is_high_amount'] = (
                processed_data['transaction_amount'] > amount_q95
            ).astype(int)
            
            processed_data['is_medium_amount'] = (
                (processed_data['transaction_amount'] > amount_q75) & 
                (processed_data['transaction_amount'] <= amount_q95)
            ).astype(int)
            
            processed_data['amount_risk_score'] = np.where(
                processed_data['transaction_amount'] > amount_q95, 80,
                np.where(processed_data['transaction_amount'] > amount_q75, 40, 10)
            )
        
        # ç»„åˆé£é™©ç‰¹å¾
        if all(col in processed_data.columns for col in ['is_new_account', 'is_high_amount']):
            processed_data['high_risk_combination'] = (
                processed_data['is_new_account'] & processed_data['is_high_amount']
            ).astype(int)
        
        if all(col in processed_data.columns for col in ['is_night_transaction', 'is_high_amount']):
            processed_data['suspicious_pattern'] = (
                processed_data['is_night_transaction'] & processed_data['is_high_amount']
            ).astype(int)
        
        logger.info(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç”Ÿæˆ {len(processed_data.columns)} ä¸ªç‰¹å¾")
        return processed_data
    
    def _intelligent_feature_selection(self, data: pd.DataFrame) -> List[str]:
        """æ™ºèƒ½ç‰¹å¾é€‰æ‹©"""
        logger.info("ğŸ¯ æ‰§è¡Œæ™ºèƒ½ç‰¹å¾é€‰æ‹©")
        
        # æ’é™¤éæ•°å€¼ç‰¹å¾å’Œæ ‡ç­¾
        exclude_features = ['is_fraudulent', 'payment_method', 'device']
        numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()
        candidate_features = [f for f in numeric_features if f not in exclude_features]
        
        if len(candidate_features) < 3:
            logger.warning("å¯ç”¨ç‰¹å¾å¤ªå°‘ï¼Œä½¿ç”¨æ‰€æœ‰æ•°å€¼ç‰¹å¾")
            return candidate_features
        
        # æ–¹æ³•1ï¼šåŸºäºæ–¹å·®çš„ç‰¹å¾é€‰æ‹©
        feature_variance = data[candidate_features].var()
        high_variance_features = feature_variance[feature_variance > 0.01].index.tolist()
        
        # æ–¹æ³•2ï¼šåŸºäºç›¸å…³æ€§çš„ç‰¹å¾é€‰æ‹©
        if 'is_fraudulent' in data.columns:
            correlations = data[candidate_features].corrwith(data['is_fraudulent']).abs()
            high_corr_features = correlations.nlargest(min(8, len(candidate_features))).index.tolist()
        else:
            high_corr_features = candidate_features[:8]
        
        # æ–¹æ³•3ï¼šåŸºäºèšç±»å‹å¥½æ€§çš„ç‰¹å¾é€‰æ‹©
        clustering_friendly_features = []
        for feature in candidate_features:
            if feature.endswith('_zscore') or feature.endswith('_percentile') or feature.endswith('_score'):
                clustering_friendly_features.append(feature)
        
        # ç»¼åˆé€‰æ‹©
        selected_features = list(set(high_variance_features + high_corr_features + clustering_friendly_features))
        
        # ç¡®ä¿è‡³å°‘æœ‰5ä¸ªç‰¹å¾
        if len(selected_features) < 5:
            selected_features = candidate_features[:min(8, len(candidate_features))]
        
        # é™åˆ¶æœ€å¤š12ä¸ªç‰¹å¾é¿å…ç»´åº¦è¯…å’’
        selected_features = selected_features[:12]
        
        logger.info(f"âœ… é€‰æ‹©äº† {len(selected_features)} ä¸ªæœ€ä¼˜ç‰¹å¾: {selected_features}")
        return selected_features
    
    def _optimize_clustering_parameters(self, feature_data: pd.DataFrame) -> Dict[str, Any]:
        """ä¼˜åŒ–èšç±»ç®—æ³•å’Œå‚æ•°"""
        logger.info("âš™ï¸ ä¼˜åŒ–èšç±»ç®—æ³•å’Œå‚æ•°")
        
        # æ ‡å‡†åŒ–æ•°æ®
        scaled_data = self.scaler.fit_transform(feature_data)
        
        best_score = -1
        best_config = None
        
        # æµ‹è¯•ä¸åŒçš„èšç±»é…ç½®
        configurations = [
            # KMeansé…ç½®
            {'algorithm': 'kmeans', 'n_clusters': 3, 'init': 'k-means++', 'n_init': 20},
            {'algorithm': 'kmeans', 'n_clusters': 4, 'init': 'k-means++', 'n_init': 20},
            {'algorithm': 'kmeans', 'n_clusters': 5, 'init': 'k-means++', 'n_init': 20},
            {'algorithm': 'kmeans', 'n_clusters': 6, 'init': 'k-means++', 'n_init': 20},
            
            # DBSCANé…ç½®
            {'algorithm': 'dbscan', 'eps': 0.5, 'min_samples': 10},
            {'algorithm': 'dbscan', 'eps': 0.8, 'min_samples': 15},
            {'algorithm': 'dbscan', 'eps': 1.0, 'min_samples': 20},
        ]
        
        for config in configurations:
            try:
                if config['algorithm'] == 'kmeans':
                    model = KMeans(
                        n_clusters=config['n_clusters'],
                        init=config['init'],
                        n_init=config['n_init'],
                        random_state=42
                    )
                    labels = model.fit_predict(scaled_data)
                    
                elif config['algorithm'] == 'dbscan':
                    model = DBSCAN(
                        eps=config['eps'],
                        min_samples=config['min_samples']
                    )
                    labels = model.fit_predict(scaled_data)
                
                # è¯„ä¼°èšç±»è´¨é‡
                if len(set(labels)) > 1 and -1 not in labels:  # ç¡®ä¿æœ‰å¤šä¸ªæœ‰æ•ˆèšç±»
                    silhouette = silhouette_score(scaled_data, labels)
                    calinski = calinski_harabasz_score(scaled_data, labels)
                    
                    # ç»¼åˆè¯„åˆ† (è½®å»“ç³»æ•°æƒé‡æ›´é«˜)
                    combined_score = silhouette * 0.7 + (calinski / 1000) * 0.3
                    
                    if combined_score > best_score:
                        best_score = combined_score
                        best_config = {
                            'config': config,
                            'model': model,
                            'labels': labels,
                            'silhouette_score': silhouette,
                            'calinski_score': calinski,
                            'combined_score': combined_score,
                            'n_clusters': len(set(labels))
                        }
                        
                        logger.info(f"æ–°çš„æœ€ä½³é…ç½®: {config}, è½®å»“ç³»æ•°: {silhouette:.3f}")
                
            except Exception as e:
                logger.warning(f"é…ç½® {config} å¤±è´¥: {e}")
                continue
        
        if best_config is None:
            logger.warning("æ‰€æœ‰é…ç½®éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤KMeans")
            model = KMeans(n_clusters=4, random_state=42)
            labels = model.fit_predict(scaled_data)
            best_config = {
                'config': {'algorithm': 'kmeans', 'n_clusters': 4},
                'model': model,
                'labels': labels,
                'silhouette_score': silhouette_score(scaled_data, labels),
                'calinski_score': calinski_harabasz_score(scaled_data, labels),
                'n_clusters': 4
            }
        
        logger.info(f"âœ… æœ€ä¼˜èšç±»é…ç½®: è½®å»“ç³»æ•° {best_config['silhouette_score']:.3f}")
        return best_config

    def _adaptive_risk_threshold_optimization(self, clustering_result: Dict[str, Any],
                                            data: pd.DataFrame,
                                            target_distribution: Dict[str, float]) -> Dict[str, float]:
        """è‡ªé€‚åº”é£é™©é˜ˆå€¼ä¼˜åŒ–"""
        logger.info("ğŸ¯ æ‰§è¡Œè‡ªé€‚åº”é£é™©é˜ˆå€¼ä¼˜åŒ–")

        # è®¡ç®—æ¯ä¸ªèšç±»çš„é£é™©ç‰¹å¾
        labels = clustering_result['labels']
        cluster_risk_scores = []

        for cluster_id in range(clustering_result['n_clusters']):
            cluster_mask = labels == cluster_id
            cluster_data = data[cluster_mask]

            if len(cluster_data) == 0:
                continue

            # è®¡ç®—èšç±»é£é™©è¯„åˆ†
            risk_score = self._calculate_cluster_risk_score(cluster_data)
            cluster_risk_scores.append(risk_score)

        if not cluster_risk_scores:
            logger.warning("æ— æ³•è®¡ç®—èšç±»é£é™©è¯„åˆ†ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼")
            return {'low': 15, 'medium': 30, 'high': 50, 'critical': 100}

        # åŸºäºå®é™…é£é™©è¯„åˆ†åˆ†å¸ƒè®¡ç®—æœ€ä¼˜é˜ˆå€¼
        cluster_risk_scores = sorted(cluster_risk_scores)
        n_clusters = len(cluster_risk_scores)

        # æ ¹æ®ç›®æ ‡åˆ†å¸ƒè®¡ç®—é˜ˆå€¼
        low_threshold = np.percentile(cluster_risk_scores, target_distribution['low'] * 100)
        medium_threshold = np.percentile(cluster_risk_scores,
                                       (target_distribution['low'] + target_distribution['medium']) * 100)
        high_threshold = np.percentile(cluster_risk_scores,
                                     (1 - target_distribution['critical']) * 100)

        # ç¡®ä¿é˜ˆå€¼åˆç†æ€§
        optimal_thresholds = {
            'low': max(10, min(low_threshold, 25)),
            'medium': max(20, min(medium_threshold, 40)),
            'high': max(35, min(high_threshold, 60)),
            'critical': 100
        }

        logger.info(f"âœ… ä¼˜åŒ–åé˜ˆå€¼: {optimal_thresholds}")
        return optimal_thresholds

    def _calculate_cluster_risk_score(self, cluster_data: pd.DataFrame) -> float:
        """è®¡ç®—èšç±»é£é™©è¯„åˆ†"""
        risk_score = 0.0

        # æ¬ºè¯ˆç‡é£é™©
        if 'is_fraudulent' in cluster_data.columns:
            fraud_rate = cluster_data['is_fraudulent'].mean()
            if fraud_rate > 0.15:
                risk_score += 50
            elif fraud_rate > 0.08:
                risk_score += 35
            elif fraud_rate > 0.04:
                risk_score += 25
            elif fraud_rate > 0.02:
                risk_score += 15
            else:
                risk_score += 10

        # äº¤æ˜“é‡‘é¢é£é™©
        if 'transaction_amount' in cluster_data.columns:
            avg_amount = cluster_data['transaction_amount'].mean()
            if avg_amount > 2000:
                risk_score += 30
            elif avg_amount > 1000:
                risk_score += 20
            elif avg_amount > 500:
                risk_score += 10

        # è´¦æˆ·å¹´é¾„é£é™©
        if 'account_age_days' in cluster_data.columns:
            avg_age = cluster_data['account_age_days'].mean()
            if avg_age < 30:
                risk_score += 25
            elif avg_age < 90:
                risk_score += 15
            elif avg_age < 180:
                risk_score += 5

        # æ—¶é—´æ¨¡å¼é£é™©
        if 'is_night_transaction' in cluster_data.columns:
            night_rate = cluster_data['is_night_transaction'].mean()
            if night_rate > 0.3:
                risk_score += 20
            elif night_rate > 0.15:
                risk_score += 10

        # ç»„åˆé£é™©
        if 'high_risk_combination' in cluster_data.columns:
            combo_rate = cluster_data['high_risk_combination'].mean()
            if combo_rate > 0.1:
                risk_score += 15

        return min(100, risk_score)

    def _generate_optimized_result(self, clustering_result: Dict[str, Any],
                                 optimal_thresholds: Dict[str, float],
                                 selected_features: List[str],
                                 data: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–åçš„æœ€ç»ˆç»“æœ"""
        logger.info("ğŸ“Š ç”Ÿæˆä¼˜åŒ–åçš„æœ€ç»ˆç»“æœ")

        return {
            'clustering_result': clustering_result,
            'optimal_thresholds': optimal_thresholds,
            'selected_features': selected_features,
            'optimization_summary': {
                'silhouette_score': clustering_result['silhouette_score'],
                'calinski_score': clustering_result['calinski_score'],
                'n_clusters': clustering_result['n_clusters'],
                'algorithm': clustering_result['config']['algorithm'],
                'feature_count': len(selected_features),
                'thresholds': optimal_thresholds
            },
            'recommendations': self._generate_recommendations(clustering_result, optimal_thresholds)
        }

    def _generate_recommendations(self, clustering_result: Dict[str, Any],
                                optimal_thresholds: Dict[str, float]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        silhouette = clustering_result['silhouette_score']

        if silhouette > 0.5:
            recommendations.append("âœ… èšç±»è´¨é‡ä¼˜ç§€ï¼Œè½®å»“ç³»æ•° > 0.5")
        elif silhouette > 0.3:
            recommendations.append("âš ï¸ èšç±»è´¨é‡è‰¯å¥½ï¼Œè½®å»“ç³»æ•° > 0.3")
        else:
            recommendations.append("âŒ èšç±»è´¨é‡è¾ƒå·®ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è°ƒæ•´ç‰¹å¾")

        if clustering_result['n_clusters'] >= 4:
            recommendations.append("âœ… èšç±»æ•°é‡é€‚ä¸­ï¼Œæ”¯æŒå››å±‚é£é™©åˆ†å¸ƒ")
        else:
            recommendations.append("âš ï¸ èšç±»æ•°é‡è¾ƒå°‘ï¼Œå¯èƒ½å½±å“é£é™©åˆ†å±‚æ•ˆæœ")

        if optimal_thresholds['low'] < 20:
            recommendations.append("âœ… é˜ˆå€¼è®¾ç½®åˆç†ï¼Œæœ‰åˆ©äºé£é™©åˆ†å±‚")
        else:
            recommendations.append("âš ï¸ é˜ˆå€¼å¯èƒ½åé«˜ï¼Œå»ºè®®è¿›ä¸€æ­¥é™ä½")

        return recommendations

    def _fallback_clustering(self, data: pd.DataFrame) -> Dict[str, Any]:
        """å¤‡ç”¨èšç±»æ–¹æ¡ˆ"""
        logger.warning("ä½¿ç”¨å¤‡ç”¨èšç±»æ–¹æ¡ˆ")

        # ç®€å•çš„ç‰¹å¾é€‰æ‹©
        numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()
        if 'is_fraudulent' in numeric_features:
            numeric_features.remove('is_fraudulent')

        selected_features = numeric_features[:6]  # é€‰æ‹©å‰6ä¸ªæ•°å€¼ç‰¹å¾

        # ç®€å•çš„KMeansèšç±»
        scaled_data = self.scaler.fit_transform(data[selected_features])
        model = KMeans(n_clusters=4, random_state=42)
        labels = model.fit_predict(scaled_data)

        clustering_result = {
            'config': {'algorithm': 'kmeans', 'n_clusters': 4},
            'model': model,
            'labels': labels,
            'silhouette_score': silhouette_score(scaled_data, labels),
            'calinski_score': calinski_harabasz_score(scaled_data, labels),
            'n_clusters': 4
        }

        return {
            'clustering_result': clustering_result,
            'optimal_thresholds': {'low': 15, 'medium': 30, 'high': 50, 'critical': 100},
            'selected_features': selected_features,
            'optimization_summary': {
                'silhouette_score': clustering_result['silhouette_score'],
                'n_clusters': 4,
                'algorithm': 'kmeans',
                'feature_count': len(selected_features),
                'note': 'å¤‡ç”¨æ–¹æ¡ˆ'
            },
            'recommendations': ['âš ï¸ ä½¿ç”¨å¤‡ç”¨èšç±»æ–¹æ¡ˆï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡']
        }
