"""
èšç±»åˆ†æå™¨
åŸºäºçœŸå®æ•°æ®é›†ç‰¹å¾çš„èšç±»åˆ†æ
æ”¯æŒK-meansã€DBSCANã€é«˜æ–¯æ··åˆèšç±»
ç”¨äºè¯†åˆ«å¼‚å¸¸äº¤æ˜“æ¨¡å¼å’Œç”¨æˆ·è¡Œä¸ºç¾¤ä½“
"""
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from typing import Dict, Any, List, Tuple
import logging
from .cluster_risk_mapper import ClusterRiskMapper
from .intelligent_cluster_optimizer import IntelligentClusterOptimizer

# å¯¼å…¥é…ç½®ç®¡ç†
try:
    from config.optimization_config import optimization_config
    CONFIG_AVAILABLE = True
except ImportError:
    CONFIG_AVAILABLE = False
    optimization_config = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ClusterAnalyzer:
    """åŸºäºçœŸå®æ•°æ®é›†çš„èšç±»åˆ†æå™¨"""

    def __init__(self, n_clusters: int = 5, random_state: int = 42):
        self.n_clusters = n_clusters
        self.random_state = random_state
        self.scaler = StandardScaler()
        self.pca = None
        self.risk_mapper = ClusterRiskMapper()  # é£é™©æ˜ å°„å™¨
        self.intelligent_optimizer = IntelligentClusterOptimizer()  # æ™ºèƒ½ä¼˜åŒ–å™¨

        # åŠ è½½é…ç½®
        self.config = self._load_clustering_config()

        # ç”¨äºèšç±»çš„å…³é”®ç‰¹å¾
        self.clustering_features = [
            'transaction_amount', 'quantity', 'customer_age', 'account_age_days',
            'transaction_hour'
        ]

        # åˆ†ç±»ç‰¹å¾ç¼–ç æ˜ å°„
        self.categorical_mappings = {
            'payment_method': {'credit card': 1, 'debit card': 2, 'bank transfer': 3, 'PayPal': 4},
            'product_category': {'clothing': 1, 'electronics': 2, 'home & garden': 3, 'health & beauty': 4, 'toys & games': 5},
            'device_used': {'desktop': 1, 'mobile': 2, 'tablet': 3}
        }

    def analyze_clusters(self, data: pd.DataFrame, algorithm: str = 'kmeans') -> Dict[str, Any]:
        """
        åŸºäºçœŸå®æ•°æ®é›†è¿›è¡Œèšç±»åˆ†æ

        Args:
            data: æ¸…ç†åçš„DataFrame
            algorithm: èšç±»ç®—æ³• ('kmeans', 'dbscan', 'gaussian_mixture')

        Returns:
            èšç±»åˆ†æç»“æœå­—å…¸
        """
        if data is None or data.empty:
            logger.error("è¾“å…¥æ•°æ®ä¸ºç©º")
            return self._empty_result()

        try:
            # å‡†å¤‡èšç±»ç‰¹å¾
            cluster_data = self._prepare_clustering_data(data)
            if cluster_data is None or cluster_data.empty:
                logger.error("æ— æ³•å‡†å¤‡èšç±»æ•°æ®")
                return self._empty_result()

            # æ‰§è¡Œèšç±»
            if algorithm == 'kmeans':
                results = self._kmeans_clustering(cluster_data, data)
            elif algorithm == 'dbscan':
                results = self._dbscan_clustering(cluster_data, data)
            elif algorithm == 'gaussian_mixture':
                results = self._gaussian_mixture_clustering(cluster_data, data)
            else:
                logger.warning(f"ä¸æ”¯æŒçš„èšç±»ç®—æ³•: {algorithm}ï¼Œä½¿ç”¨é»˜è®¤kmeans")
                results = self._kmeans_clustering(cluster_data, data)

            # æ·»åŠ èšç±»è´¨é‡è¯„ä¼°
            if len(set(results['cluster_labels'])) > 1:
                results['quality_metrics'] = self._evaluate_clustering_quality(
                    cluster_data, results['cluster_labels']
                )

            # åˆ†æå¼‚å¸¸ç¾¤ä½“
            results['anomaly_analysis'] = self._analyze_anomalies(data, results['cluster_labels'])

            # æ·»åŠ é£é™©æ˜ å°„
            risk_mapping_results = self.risk_mapper.map_clusters_to_risk_levels(results, data)
            results.update(risk_mapping_results)

            # å°†é£é™©æ˜ å°„ç»“æœåˆå¹¶åˆ°cluster_detailsä¸­
            cluster_risk_mapping = risk_mapping_results.get('cluster_risk_mapping', {})
            if cluster_risk_mapping and 'cluster_details' in results:
                for detail in results['cluster_details']:
                    cluster_id = detail.get('cluster_id', -1)
                    if cluster_id in cluster_risk_mapping:
                        # æ›´æ–°é£é™©ç­‰çº§å’Œè¯„åˆ†
                        risk_info = cluster_risk_mapping[cluster_id]
                        detail['risk_level'] = risk_info.get('risk_level', detail.get('risk_level', 'low'))
                        detail['risk_score'] = risk_info.get('risk_score', 0)
                        detail['risk_indicators'] = risk_info.get('risk_indicators', {})
                        detail['risk_explanation'] = risk_info.get('risk_explanation', [])

            logger.info(f"èšç±»åˆ†æå®Œæˆ: {algorithm}, å‘ç°{results['cluster_count']}ä¸ªç¾¤ä½“")
            return results

        except Exception as e:
            logger.error(f"èšç±»åˆ†æå¤±è´¥: {e}")
            return self._empty_result()

    def _prepare_clustering_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """å‡†å¤‡èšç±»æ•°æ®"""
        cluster_df = pd.DataFrame()

        # æ·»åŠ æ•°å€¼ç‰¹å¾
        for feature in self.clustering_features:
            if feature in data.columns:
                cluster_df[feature] = data[feature]

        # ç¼–ç åˆ†ç±»ç‰¹å¾
        for cat_feature, mapping in self.categorical_mappings.items():
            if cat_feature in data.columns:
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥é¿å…åˆ†ç±»ç±»å‹é—®é¢˜
                feature_values = data[cat_feature].astype(str)
                cluster_df[f'{cat_feature}_encoded'] = feature_values.map(mapping).fillna(0)

        # æ·»åŠ å·¥ç¨‹ç‰¹å¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        engineering_features = [
            'time_risk_score', 'amount_risk_score', 'device_risk_score',
            'account_age_risk_score', 'is_night_transaction', 'is_large_amount'
        ]

        for feature in engineering_features:
            if feature in data.columns:
                cluster_df[feature] = data[feature]

        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
        if cluster_df.empty or cluster_df.shape[1] == 0:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„èšç±»ç‰¹å¾")
            return None

        # ä¼˜åŒ–çš„æ•°æ®é¢„å¤„ç†
        cluster_df = self._enhanced_data_preprocessing(cluster_df)

        if cluster_df is None or cluster_df.empty:
            logger.warning("æ•°æ®é¢„å¤„ç†åä¸ºç©º")
            return None

        # æ ‡å‡†åŒ–ç‰¹å¾
        cluster_df_scaled = pd.DataFrame(
            self.scaler.fit_transform(cluster_df),
            columns=cluster_df.columns,
            index=cluster_df.index
        )

        return cluster_df_scaled

    def _load_clustering_config(self) -> Dict[str, Any]:
        """åŠ è½½èšç±»é…ç½®"""
        if CONFIG_AVAILABLE and optimization_config:
            return optimization_config.get_clustering_config()
        else:
            # é»˜è®¤é…ç½®
            return {
                "auto_k_optimization": True,
                "max_k": 10,
                "min_k": 2,
                "dbscan_auto_params": True,
                "feature_selection": {
                    "enabled": True,
                    "variance_threshold": 0.01,
                    "correlation_threshold": 0.95,
                    "min_features": 3
                },
                "data_quality": {
                    "outlier_method": "iqr",
                    "outlier_factor": 1.5,
                    "missing_value_threshold": 0.5,
                    "quality_threshold": 70.0
                }
            }

    def _enhanced_data_preprocessing(self, data: pd.DataFrame) -> pd.DataFrame:
        """å¢å¼ºçš„æ•°æ®é¢„å¤„ç†"""
        try:
            processed_data = data.copy()

            # 1. å¤„ç†ç¼ºå¤±å€¼
            processed_data = self._handle_missing_values_smart(processed_data)

            # 2. å¤„ç†å¼‚å¸¸å€¼
            processed_data = self._handle_outliers_iqr(processed_data)

            # 3. å¤„ç†æ— ç©·å€¼
            processed_data = processed_data.replace([np.inf, -np.inf], np.nan)
            processed_data = processed_data.fillna(processed_data.median())

            # 4. æ•°æ®è´¨é‡æ£€æŸ¥
            quality_score = self._calculate_data_quality_score(processed_data)
            logger.info(f"æ•°æ®è´¨é‡è¯„åˆ†: {quality_score:.2f}/100")

            return processed_data

        except Exception as e:
            logger.error(f"å¢å¼ºæ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return data.fillna(0)  # å›é€€åˆ°ç®€å•å¤„ç†

    def _handle_missing_values_smart(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†"""
        try:
            processed_data = data.copy()

            for column in processed_data.columns:
                missing_ratio = processed_data[column].isnull().sum() / len(processed_data)

                if missing_ratio > 0:
                    if missing_ratio > 0.5:
                        # ç¼ºå¤±å€¼è¿‡å¤šï¼Œåˆ é™¤è¯¥åˆ—
                        logger.warning(f"åˆ— {column} ç¼ºå¤±å€¼æ¯”ä¾‹ {missing_ratio:.2f}ï¼Œå°†è¢«åˆ é™¤")
                        processed_data = processed_data.drop(columns=[column])
                    else:
                        # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©å¡«å……ç­–ç•¥
                        if processed_data[column].dtype in ['int64', 'float64']:
                            # æ•°å€¼å‹ï¼šä½¿ç”¨ä¸­ä½æ•°
                            fill_value = processed_data[column].median()
                        else:
                            # å…¶ä»–ç±»å‹ï¼šä½¿ç”¨ä¼—æ•°
                            mode_values = processed_data[column].mode()
                            fill_value = mode_values.iloc[0] if not mode_values.empty else 0

                        processed_data[column] = processed_data[column].fillna(fill_value)

            return processed_data

        except Exception as e:
            logger.warning(f"æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†å¤±è´¥: {e}")
            return data.fillna(0)

    def _handle_outliers_iqr(self, data: pd.DataFrame) -> pd.DataFrame:
        """ä½¿ç”¨IQRæ–¹æ³•å¤„ç†å¼‚å¸¸å€¼"""
        try:
            processed_data = data.copy()

            for column in processed_data.columns:
                if processed_data[column].dtype in ['int64', 'float64']:
                    Q1 = processed_data[column].quantile(0.25)
                    Q3 = processed_data[column].quantile(0.75)
                    IQR = Q3 - Q1

                    if IQR > 0:  # é¿å…é™¤é›¶é”™è¯¯
                        # å®šä¹‰å¼‚å¸¸å€¼è¾¹ç•Œ
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR

                        # ç»Ÿè®¡å¼‚å¸¸å€¼æ•°é‡
                        outliers_count = ((processed_data[column] < lower_bound) |
                                        (processed_data[column] > upper_bound)).sum()

                        if outliers_count > 0:
                            outlier_ratio = outliers_count / len(processed_data)
                            logger.info(f"åˆ— {column} å‘ç° {outliers_count} ä¸ªå¼‚å¸¸å€¼ ({outlier_ratio:.2%})")

                            # ä½¿ç”¨Winsorizationæ–¹æ³•å¤„ç†å¼‚å¸¸å€¼
                            processed_data[column] = processed_data[column].clip(lower_bound, upper_bound)

            return processed_data

        except Exception as e:
            logger.warning(f"å¼‚å¸¸å€¼å¤„ç†å¤±è´¥: {e}")
            return data

    def _calculate_data_quality_score(self, data: pd.DataFrame) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†"""
        try:
            if data.empty:
                return 0.0

            # 1. å®Œæ•´æ€§è¯„åˆ† (40%)
            completeness = (1 - data.isnull().sum().sum() / (len(data) * len(data.columns))) * 40

            # 2. ä¸€è‡´æ€§è¯„åˆ† (30%) - åŸºäºæ•°æ®ç±»å‹ä¸€è‡´æ€§
            consistency = 30  # å‡è®¾æ•°æ®ç±»å‹ä¸€è‡´

            # 3. æœ‰æ•ˆæ€§è¯„åˆ† (20%) - åŸºäºæ•°å€¼èŒƒå›´åˆç†æ€§
            validity = 0
            for column in data.columns:
                if data[column].dtype in ['int64', 'float64']:
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ— ç©·å€¼æˆ–æç«¯å€¼
                    if not np.isinf(data[column]).any() and data[column].std() > 0:
                        validity += 20 / len(data.columns)

            # 4. å”¯ä¸€æ€§è¯„åˆ† (10%) - åŸºäºé‡å¤è¡Œæ¯”ä¾‹
            uniqueness = (1 - data.duplicated().sum() / len(data)) * 10

            total_score = completeness + consistency + validity + uniqueness
            return min(100.0, max(0.0, total_score))

        except Exception as e:
            logger.warning(f"æ•°æ®è´¨é‡è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 50.0  # é»˜è®¤ä¸­ç­‰è´¨é‡

    def _find_optimal_k(self, data: pd.DataFrame, max_k: int = None) -> int:
        """è‡ªåŠ¨ç¡®å®šæœ€ä¼˜Kå€¼"""
        try:
            # ä»é…ç½®è·å–å‚æ•°
            if max_k is None:
                max_k = self.config.get('max_k', 10)
            min_k = self.config.get('min_k', 2)

            if len(data) < 10:
                return min(min_k, len(data))

            # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨Kä¼˜åŒ–
            if not self.config.get('auto_k_optimization', True):
                return self.n_clusters

            # è®¡ç®—ä¸åŒKå€¼çš„è¯„ä¼°æŒ‡æ ‡
            k_range = range(min_k, min(max_k + 1, len(data) // 2))
            inertias = []
            silhouette_scores = []

            for k in k_range:
                try:
                    kmeans = KMeans(n_clusters=k, random_state=self.random_state, n_init=10)
                    labels = kmeans.fit_predict(data)

                    # è®¡ç®—æƒ¯æ€§ï¼ˆè‚˜éƒ¨æ³•åˆ™ï¼‰
                    inertias.append(kmeans.inertia_)

                    # è®¡ç®—è½®å»“ç³»æ•°
                    if len(set(labels)) > 1:
                        sil_score = silhouette_score(data, labels)
                        silhouette_scores.append(sil_score)
                    else:
                        silhouette_scores.append(0)

                except Exception as e:
                    logger.warning(f"K={k}æ—¶èšç±»å¤±è´¥: {e}")
                    inertias.append(float('inf'))
                    silhouette_scores.append(0)

            # ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™æ‰¾åˆ°æœ€ä¼˜K
            optimal_k_elbow = self._find_elbow_point(list(k_range), inertias)

            # ä½¿ç”¨è½®å»“ç³»æ•°æ‰¾åˆ°æœ€ä¼˜K
            if silhouette_scores:
                optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]
            else:
                optimal_k_silhouette = optimal_k_elbow

            # ç»¼åˆè€ƒè™‘ä¸¤ç§æ–¹æ³•ï¼Œä¼˜å…ˆé€‰æ‹©è½®å»“ç³»æ•°è¾ƒé«˜çš„Kå€¼
            if abs(optimal_k_elbow - optimal_k_silhouette) <= 1:
                optimal_k = optimal_k_silhouette
            else:
                # å¦‚æœå·®å¼‚è¾ƒå¤§ï¼Œé€‰æ‹©è½®å»“ç³»æ•°æœ€é«˜çš„
                optimal_k = optimal_k_silhouette

            # ç¡®ä¿Kå€¼åœ¨åˆç†èŒƒå›´å†…
            optimal_k = max(2, min(optimal_k, self.n_clusters))

            logger.info(f"Kå€¼ä¼˜åŒ–: è‚˜éƒ¨æ³•åˆ™={optimal_k_elbow}, è½®å»“ç³»æ•°={optimal_k_silhouette}, æœ€ç»ˆé€‰æ‹©={optimal_k}")
            return optimal_k

        except Exception as e:
            logger.warning(f"Kå€¼ä¼˜åŒ–å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤å€¼")
            return self.n_clusters

    def _find_elbow_point(self, k_values: List[int], inertias: List[float]) -> int:
        """ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™æ‰¾åˆ°æœ€ä¼˜Kå€¼"""
        try:
            if len(k_values) < 3:
                return k_values[0] if k_values else 2

            # è®¡ç®—äºŒé˜¶å¯¼æ•°æ¥æ‰¾åˆ°è‚˜éƒ¨ç‚¹
            diffs = np.diff(inertias)
            diffs2 = np.diff(diffs)

            # æ‰¾åˆ°äºŒé˜¶å¯¼æ•°æœ€å¤§çš„ç‚¹ï¼ˆè‚˜éƒ¨ï¼‰
            if len(diffs2) > 0:
                elbow_idx = np.argmax(diffs2) + 2  # +2å› ä¸ºäºŒé˜¶å¯¼æ•°çš„ç´¢å¼•åç§»
                if elbow_idx < len(k_values):
                    return k_values[elbow_idx]

            # å¦‚æœæ‰¾ä¸åˆ°æ˜æ˜¾çš„è‚˜éƒ¨ï¼Œé€‰æ‹©ä¸­é—´å€¼
            return k_values[len(k_values) // 2]

        except Exception as e:
            logger.warning(f"è‚˜éƒ¨ç‚¹è®¡ç®—å¤±è´¥: {e}")
            return k_values[0] if k_values else 2

    def _find_optimal_dbscan_params(self, data: pd.DataFrame) -> Tuple[float, int]:
        """è‡ªåŠ¨ç¡®å®šDBSCANæœ€ä¼˜å‚æ•°"""
        try:
            from sklearn.neighbors import NearestNeighbors

            # 1. ç¡®å®šmin_samples (é€šå¸¸è®¾ä¸ºç‰¹å¾æ•°é‡çš„2å€)
            min_samples = max(4, min(2 * data.shape[1], len(data) // 10))

            # 2. ä½¿ç”¨k-distanceå›¾ç¡®å®šeps
            # è®¡ç®—æ¯ä¸ªç‚¹åˆ°ç¬¬kä¸ªæœ€è¿‘é‚»çš„è·ç¦»
            neighbors = NearestNeighbors(n_neighbors=min_samples)
            neighbors_fit = neighbors.fit(data)
            distances, indices = neighbors_fit.kneighbors(data)

            # å–ç¬¬kä¸ªæœ€è¿‘é‚»çš„è·ç¦»å¹¶æ’åº
            k_distances = distances[:, min_samples-1]
            k_distances = np.sort(k_distances)

            # å¯»æ‰¾è‚˜éƒ¨ç‚¹ä½œä¸ºeps
            eps = self._find_eps_elbow_point(k_distances)

            # éªŒè¯å‚æ•°çš„æœ‰æ•ˆæ€§
            eps, min_samples = self._validate_dbscan_params(data, eps, min_samples)

            return eps, min_samples

        except Exception as e:
            logger.warning(f"DBSCANå‚æ•°ä¼˜åŒ–å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤å‚æ•°")
            return 0.5, 5

    def _find_eps_elbow_point(self, k_distances: np.ndarray) -> float:
        """åœ¨k-distanceå›¾ä¸­æ‰¾åˆ°è‚˜éƒ¨ç‚¹ç¡®å®šeps"""
        try:
            if len(k_distances) < 10:
                return np.median(k_distances)

            # è®¡ç®—ä¸€é˜¶å’ŒäºŒé˜¶å¯¼æ•°
            x = np.arange(len(k_distances))
            diffs = np.diff(k_distances)
            diffs2 = np.diff(diffs)

            # æ‰¾åˆ°äºŒé˜¶å¯¼æ•°æœ€å¤§çš„ç‚¹
            if len(diffs2) > 0:
                elbow_idx = np.argmax(diffs2)
                # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                elbow_idx = min(elbow_idx, len(k_distances) - 1)
                return k_distances[elbow_idx]
            else:
                # å¦‚æœæ‰¾ä¸åˆ°è‚˜éƒ¨ï¼Œä½¿ç”¨75åˆ†ä½æ•°
                return np.percentile(k_distances, 75)

        except Exception as e:
            logger.warning(f"epsè‚˜éƒ¨ç‚¹è®¡ç®—å¤±è´¥: {e}")
            return np.median(k_distances) if len(k_distances) > 0 else 0.5

    def _validate_dbscan_params(self, data: pd.DataFrame, eps: float, min_samples: int) -> Tuple[float, int]:
        """éªŒè¯å’Œè°ƒæ•´DBSCANå‚æ•°"""
        try:
            # æµ‹è¯•å‚æ•°æ˜¯å¦ä¼šäº§ç”Ÿåˆç†çš„èšç±»ç»“æœ
            test_dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            test_labels = test_dbscan.fit_predict(data)

            n_clusters = len(set(test_labels)) - (1 if -1 in test_labels else 0)
            noise_ratio = (test_labels == -1).sum() / len(test_labels)

            # è°ƒæ•´å‚æ•°ä»¥è·å¾—æ›´å¥½çš„ç»“æœ
            if n_clusters == 0 or noise_ratio > 0.5:
                # èšç±»æ•°å¤ªå°‘æˆ–å™ªå£°å¤ªå¤šï¼Œå‡å°eps
                eps = eps * 0.7
                logger.info(f"è°ƒæ•´eps: {eps:.3f} (å‡å°)")
            elif n_clusters > len(data) // 10:
                # èšç±»æ•°å¤ªå¤šï¼Œå¢å¤§eps
                eps = eps * 1.3
                logger.info(f"è°ƒæ•´eps: {eps:.3f} (å¢å¤§)")

            # ç¡®ä¿min_samplesåœ¨åˆç†èŒƒå›´å†…
            min_samples = max(3, min(min_samples, len(data) // 5))

            return eps, min_samples

        except Exception as e:
            logger.warning(f"DBSCANå‚æ•°éªŒè¯å¤±è´¥: {e}")
            return eps, min_samples

    def _kmeans_clustering(self, cluster_data: pd.DataFrame, original_data: pd.DataFrame) -> Dict[str, Any]:
        """K-meansèšç±» - ä¼˜åŒ–ç‰ˆæœ¬"""
        # è‡ªåŠ¨ç¡®å®šæœ€ä¼˜Kå€¼
        optimal_k = self._find_optimal_k(cluster_data)
        logger.info(f"è‡ªåŠ¨ç¡®å®šæœ€ä¼˜Kå€¼: {optimal_k}")

        kmeans = KMeans(n_clusters=optimal_k, random_state=self.random_state, n_init=10)
        labels = kmeans.fit_predict(cluster_data)

        # åˆ†ææ¯ä¸ªèšç±»
        cluster_details = []
        for i in range(self.n_clusters):
            cluster_mask = labels == i
            cluster_size = np.sum(cluster_mask)

            if cluster_size > 0:
                # è·å–è¯¥èšç±»çš„åŸå§‹æ•°æ®
                cluster_original = original_data[cluster_mask]

                # è®¡ç®—èšç±»ç‰¹å¾
                cluster_info = self._analyze_cluster_characteristics(cluster_original, i)
                cluster_info['size'] = cluster_size
                cluster_info['percentage'] = round(cluster_size / len(labels) * 100, 2)

                cluster_details.append(cluster_info)

        return {
            'algorithm': 'kmeans',
            'cluster_count': self.n_clusters,
            'cluster_labels': labels.tolist(),
            'cluster_details': cluster_details,
            'centers': kmeans.cluster_centers_.tolist()
        }

    def _dbscan_clustering(self, cluster_data: pd.DataFrame, original_data: pd.DataFrame) -> Dict[str, Any]:
        """DBSCANèšç±» - ä¼˜åŒ–ç‰ˆæœ¬"""
        # è‡ªåŠ¨ç¡®å®šæœ€ä¼˜å‚æ•°
        optimal_eps, optimal_min_samples = self._find_optimal_dbscan_params(cluster_data)
        logger.info(f"è‡ªåŠ¨ç¡®å®šDBSCANå‚æ•°: eps={optimal_eps:.3f}, min_samples={optimal_min_samples}")

        dbscan = DBSCAN(eps=optimal_eps, min_samples=optimal_min_samples)
        labels = dbscan.fit_predict(cluster_data)

        unique_labels = set(labels)
        n_clusters = len(unique_labels) - (1 if -1 in labels else 0)

        cluster_details = []
        for label in unique_labels:
            if label == -1:
                continue  # è·³è¿‡å™ªå£°ç‚¹

            cluster_mask = labels == label
            cluster_size = np.sum(cluster_mask)

            if cluster_size > 0:
                cluster_original = original_data[cluster_mask]
                cluster_info = self._analyze_cluster_characteristics(cluster_original, label)
                cluster_info['size'] = cluster_size
                cluster_info['percentage'] = round(cluster_size / len(labels) * 100, 2)
                cluster_details.append(cluster_info)

        # åˆ†æå™ªå£°ç‚¹
        noise_count = np.sum(labels == -1)

        return {
            'algorithm': 'dbscan',
            'cluster_count': n_clusters,
            'cluster_labels': labels.tolist(),
            'cluster_details': cluster_details,
            'noise_points': noise_count,
            'noise_percentage': round(noise_count / len(labels) * 100, 2)
        }

    def _gaussian_mixture_clustering(self, cluster_data: pd.DataFrame, original_data: pd.DataFrame) -> Dict[str, Any]:
        """é«˜æ–¯æ··åˆæ¨¡å‹èšç±»"""
        gmm = GaussianMixture(n_components=self.n_clusters, random_state=self.random_state)
        labels = gmm.fit_predict(cluster_data)

        cluster_details = []
        for i in range(self.n_clusters):
            cluster_mask = labels == i
            cluster_size = np.sum(cluster_mask)

            if cluster_size > 0:
                cluster_original = original_data[cluster_mask]
                cluster_info = self._analyze_cluster_characteristics(cluster_original, i)
                cluster_info['size'] = cluster_size
                cluster_info['percentage'] = round(cluster_size / len(labels) * 100, 2)
                cluster_details.append(cluster_info)

        return {
            'algorithm': 'gaussian_mixture',
            'cluster_count': self.n_clusters,
            'cluster_labels': labels.tolist(),
            'cluster_details': cluster_details,
            'bic_score': gmm.bic(cluster_data),
            'aic_score': gmm.aic(cluster_data)
        }

    def fit_kmeans(self, data: pd.DataFrame, n_clusters: int = 4, random_state: int = 42) -> np.ndarray:
        self.method = 'kmeans'
        self.model = KMeans(n_clusters=n_clusters, random_state=random_state)
        self.labels_ = self.model.fit_predict(data)
        self.centers_ = self.model.cluster_centers_
        logger.info(f"K-meansèšç±»å®Œæˆï¼Œèšç±»æ•°: {n_clusters}")
        return self.labels_

    def _analyze_cluster_characteristics(self, cluster_data: pd.DataFrame, cluster_id: int) -> Dict[str, Any]:
        """åˆ†æèšç±»ç‰¹å¾"""
        characteristics = {
            'cluster_id': cluster_id,
            'avg_transaction_amount': 0,
            'avg_customer_age': 0,
            'common_payment_method': 'unknown',
            'common_device': 'unknown',
            'common_category': 'unknown',
            'fraud_rate': 0,
            'risk_level': 'low'
        }

        if cluster_data.empty:
            return characteristics

        # äº¤æ˜“é‡‘é¢ç‰¹å¾
        if 'transaction_amount' in cluster_data.columns:
            amounts = cluster_data['transaction_amount']
            characteristics['avg_transaction_amount'] = round(amounts.mean(), 2)
            characteristics['median_transaction_amount'] = round(amounts.median(), 2)
            characteristics['transaction_amount_std'] = round(amounts.std(), 2)

            # è®¡ç®—é«˜é‡‘é¢äº¤æ˜“æ¯”ä¾‹ï¼ˆå¤§äºå¹³å‡å€¼+2å€æ ‡å‡†å·®ï¼‰
            mean_amount = amounts.mean()
            std_amount = amounts.std()
            high_threshold = mean_amount + 2 * std_amount if std_amount > 0 else mean_amount * 2
            characteristics['high_amount_rate'] = round((amounts > high_threshold).mean(), 3)

        # å®¢æˆ·å¹´é¾„ç‰¹å¾
        if 'customer_age' in cluster_data.columns:
            characteristics['avg_customer_age'] = round(cluster_data['customer_age'].mean(), 1)

        # æœ€å¸¸è§çš„åˆ†ç±»ç‰¹å¾
        if 'payment_method' in cluster_data.columns:
            characteristics['common_payment_method'] = cluster_data['payment_method'].mode().iloc[0] if not cluster_data['payment_method'].mode().empty else 'unknown'

        if 'device_used' in cluster_data.columns:
            characteristics['common_device'] = cluster_data['device_used'].mode().iloc[0] if not cluster_data['device_used'].mode().empty else 'unknown'

        if 'product_category' in cluster_data.columns:
            characteristics['common_category'] = cluster_data['product_category'].mode().iloc[0] if not cluster_data['product_category'].mode().empty else 'unknown'

        # æ¬ºè¯ˆç‡
        if 'is_fraudulent' in cluster_data.columns:
            characteristics['fraud_rate'] = round(cluster_data['is_fraudulent'].mean(), 3)

            # åŸºäºæ¬ºè¯ˆç‡ç¡®å®šé£é™©ç­‰çº§
            fraud_rate = characteristics['fraud_rate']
            if fraud_rate > 0.1:
                characteristics['risk_level'] = 'high'
            elif fraud_rate > 0.05:
                characteristics['risk_level'] = 'medium'
            else:
                characteristics['risk_level'] = 'low'

        # æ—¶é—´æ¨¡å¼
        if 'transaction_hour' in cluster_data.columns:
            characteristics['common_hour'] = int(cluster_data['transaction_hour'].mode().iloc[0]) if not cluster_data['transaction_hour'].mode().empty else 12

            # è®¡ç®—å¤œé—´äº¤æ˜“æ¯”ä¾‹ (22ç‚¹-5ç‚¹)
            night_transactions = ((cluster_data['transaction_hour'] >= 22) | (cluster_data['transaction_hour'] <= 5))
            characteristics['night_transaction_rate'] = round(night_transactions.mean(), 3)

        # è´¦æˆ·å¹´é¾„ç‰¹å¾
        if 'account_age_days' in cluster_data.columns:
            account_ages = cluster_data['account_age_days']
            characteristics['avg_account_age_days'] = round(account_ages.mean(), 1)
            characteristics['median_account_age_days'] = round(account_ages.median(), 1)

            # è®¡ç®—æ–°è´¦æˆ·æ¯”ä¾‹ (å°äº90å¤©)
            new_accounts = (account_ages < 90)
            characteristics['new_account_rate'] = round(new_accounts.mean(), 3)

        # è®¾å¤‡ä½¿ç”¨æ¨¡å¼
        if 'device' in cluster_data.columns:
            # ç§»åŠ¨è®¾å¤‡ä½¿ç”¨æ¯”ä¾‹
            mobile_usage = (cluster_data['device'] == 'mobile')
            characteristics['mobile_device_rate'] = round(mobile_usage.mean(), 3)
            characteristics['common_device'] = cluster_data['device'].mode().iloc[0] if not cluster_data['device'].mode().empty else 'unknown'

        # æ”¯ä»˜æ–¹å¼æ¨¡å¼
        if 'payment_method' in cluster_data.columns:
            # é“¶è¡Œè½¬è´¦æ¯”ä¾‹
            bank_transfer_usage = (cluster_data['payment_method'] == 'bank_transfer')
            characteristics['bank_transfer_rate'] = round(bank_transfer_usage.mean(), 3)

        # å•†å“ç±»åˆ«é£é™©
        if 'product_category' in cluster_data.columns:
            # é«˜é£é™©ç±»åˆ«æ¯”ä¾‹ï¼ˆelectronicsé€šå¸¸é£é™©è¾ƒé«˜ï¼‰
            electronics_rate = (cluster_data['product_category'] == 'electronics').mean()
            characteristics['electronics_rate'] = round(electronics_rate, 3)
        else:
            characteristics['electronics_rate'] = 0.3  # é»˜è®¤å€¼

        # åœ°å€ä¸€è‡´æ€§ï¼ˆå¦‚æœæœ‰ç›¸å…³å­—æ®µï¼‰
        if 'shipping_address' in cluster_data.columns and 'billing_address' in cluster_data.columns:
            address_mismatch_rate = (cluster_data['shipping_address'] != cluster_data['billing_address']).mean()
            characteristics['address_mismatch_rate'] = round(address_mismatch_rate, 3)
        else:
            # å¦‚æœæ²¡æœ‰åœ°å€å­—æ®µï¼ŒåŸºäºå…¶ä»–å› ç´ ä¼°ç®—åœ°å€é£é™©
            characteristics['address_mismatch_rate'] = 0.1  # é»˜è®¤å€¼

        # å•†å“ç±»åˆ«é£é™©
        if 'product_category' in cluster_data.columns:
            characteristics['electronics_rate'] = round(
                len(cluster_data[cluster_data['product_category'] == 'electronics']) / len(cluster_data), 3
            )

        # åœ°å€ä¸€è‡´æ€§
        if 'shipping_address' in cluster_data.columns and 'billing_address' in cluster_data.columns:
            address_mismatch = cluster_data['shipping_address'] != cluster_data['billing_address']
            characteristics['address_mismatch_rate'] = round(address_mismatch.mean(), 3)

        # äº¤æ˜“é‡‘é¢ç»Ÿè®¡
        if 'transaction_amount' in cluster_data.columns:
            characteristics['transaction_amount_std'] = round(cluster_data['transaction_amount'].std(), 2)
            characteristics['high_amount_rate'] = round(
                len(cluster_data[cluster_data['transaction_amount'] > 1000]) / len(cluster_data), 3
            )

        return characteristics

    def _evaluate_clustering_quality(self, cluster_data: pd.DataFrame, labels: List[int]) -> Dict[str, float]:
        """è¯„ä¼°èšç±»è´¨é‡"""
        try:
            silhouette = silhouette_score(cluster_data, labels)
            calinski_harabasz = calinski_harabasz_score(cluster_data, labels)

            return {
                'silhouette_score': round(silhouette, 3),
                'calinski_harabasz_score': round(calinski_harabasz, 3)
            }
        except Exception as e:
            logger.warning(f"èšç±»è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return {'silhouette_score': 0, 'calinski_harabasz_score': 0}

    def _analyze_anomalies(self, data: pd.DataFrame, labels: List[int]) -> Dict[str, Any]:
        """åˆ†æå¼‚å¸¸ç¾¤ä½“"""
        anomaly_info = {
            'high_risk_clusters': [],
            'small_clusters': [],
            'unusual_patterns': []
        }

        # ç»Ÿè®¡æ¯ä¸ªèšç±»çš„å¤§å°å’Œæ¬ºè¯ˆç‡
        unique_labels = set(labels)
        total_size = len(labels)

        for label in unique_labels:
            if label == -1:  # DBSCANçš„å™ªå£°ç‚¹
                continue

            cluster_mask = [l == label for l in labels]
            cluster_data = data[cluster_mask]
            cluster_size = len(cluster_data)

            # å°èšç±»ï¼ˆå æ¯”å°äº5%ï¼‰
            if cluster_size / total_size < 0.05:
                anomaly_info['small_clusters'].append({
                    'cluster_id': label,
                    'size': cluster_size,
                    'percentage': round(cluster_size / total_size * 100, 2)
                })

            # é«˜é£é™©èšç±»ï¼ˆæ¬ºè¯ˆç‡>10%ï¼‰
            if 'is_fraudulent' in cluster_data.columns:
                fraud_rate = cluster_data['is_fraudulent'].mean()
                if fraud_rate > 0.1:
                    anomaly_info['high_risk_clusters'].append({
                        'cluster_id': label,
                        'fraud_rate': round(fraud_rate, 3),
                        'size': cluster_size
                    })

        return anomaly_info

    def _empty_result(self) -> Dict[str, Any]:
        """è¿”å›ç©ºç»“æœ"""
        return {
            'algorithm': 'none',
            'cluster_count': 0,
            'cluster_labels': [],
            'cluster_details': [],
            'quality_metrics': {},
            'anomaly_analysis': {}
        }

    def fit_dbscan(self, data: pd.DataFrame, eps: float = 0.5, min_samples: int = 5) -> np.ndarray:
        self.method = 'dbscan'
        self.model = DBSCAN(eps=eps, min_samples=min_samples)
        self.labels_ = self.model.fit_predict(data)
        self.centers_ = None
        logger.info(f"DBSCANèšç±»å®Œæˆï¼Œç°‡æ•°: {len(set(self.labels_)) - (1 if -1 in self.labels_ else 0)}")
        return self.labels_

    def fit_gmm(self, data: pd.DataFrame, n_components: int = 4, random_state: int = 42) -> np.ndarray:
        self.method = 'gmm'
        self.model = GaussianMixture(n_components=n_components, random_state=random_state)
        self.labels_ = self.model.fit_predict(data)
        self.centers_ = self.model.means_
        logger.info(f"é«˜æ–¯æ··åˆèšç±»å®Œæˆï¼Œç»„ä»¶æ•°: {n_components}")
        return self.labels_

    def get_cluster_centers(self):
        return self.centers_

    def get_labels(self):
        return self.labels_

    def get_method(self):
        return self.method

    def intelligent_auto_clustering(self, data: pd.DataFrame,
                                   target_risk_distribution: Dict[str, float] = None) -> Dict[str, Any]:
        """
        æ™ºèƒ½è‡ªåŠ¨èšç±» - ä¸€é”®æœ€ä¼˜èšç±»

        Args:
            data: è¾“å…¥æ•°æ®
            target_risk_distribution: ç›®æ ‡é£é™©åˆ†å¸ƒ

        Returns:
            å®Œæ•´çš„èšç±»åˆ†æç»“æœ
        """
        logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½è‡ªåŠ¨èšç±»")

        try:
            # ä½¿ç”¨æ™ºèƒ½ä¼˜åŒ–å™¨è¿›è¡Œè‡ªåŠ¨ä¼˜åŒ–
            optimization_result = self.intelligent_optimizer.auto_optimize_clustering(
                data, target_risk_distribution
            )

            # æå–ä¼˜åŒ–ç»“æœ
            clustering_result = optimization_result['clustering_result']
            optimal_thresholds = optimization_result['optimal_thresholds']
            selected_features = optimization_result['selected_features']

            # æ›´æ–°é£é™©æ˜ å°„å™¨çš„é˜ˆå€¼
            self.risk_mapper.cluster_risk_thresholds = optimal_thresholds

            # ç”Ÿæˆèšç±»è¯¦æƒ…
            cluster_details = self._generate_cluster_details_from_labels(
                data, clustering_result['labels'], selected_features
            )

            # æ‰§è¡Œé£é™©æ˜ å°„
            risk_mapping_result = self.risk_mapper.map_clusters_to_risk_levels(
                {'cluster_details': cluster_details}, data
            )

            # åˆå¹¶ç»“æœ
            final_result = {
                'algorithm': clustering_result['config']['algorithm'],
                'n_clusters': clustering_result['n_clusters'],
                'cluster_details': cluster_details,
                'cluster_labels': clustering_result['labels'].tolist(),
                'silhouette_score': clustering_result['silhouette_score'],
                'calinski_harabasz_score': clustering_result['calinski_score'],
                'selected_features': selected_features,
                'optimal_thresholds': optimal_thresholds,
                'optimization_summary': optimization_result['optimization_summary'],
                'recommendations': optimization_result['recommendations'],
                'risk_mapping': risk_mapping_result
            }

            logger.info("âœ… æ™ºèƒ½è‡ªåŠ¨èšç±»å®Œæˆ")
            return final_result

        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½è‡ªåŠ¨èšç±»å¤±è´¥: {e}")
            # å›é€€åˆ°ä¼ ç»Ÿèšç±»
            return self.analyze_clusters(data, algorithm='kmeans', n_clusters=4)

    def _generate_cluster_details_from_labels(self, data: pd.DataFrame,
                                            labels: np.ndarray,
                                            features: List[str]) -> List[Dict[str, Any]]:
        """ä»èšç±»æ ‡ç­¾ç”Ÿæˆèšç±»è¯¦æƒ…"""
        cluster_details = []

        for cluster_id in range(len(set(labels))):
            cluster_mask = labels == cluster_id
            cluster_data = data[cluster_mask]

            if len(cluster_data) == 0:
                continue

            # åŸºç¡€ç»Ÿè®¡
            detail = {
                'cluster_id': cluster_id,
                'size': len(cluster_data),
                'percentage': len(cluster_data) / len(data) * 100
            }

            # æ•°å€¼ç‰¹å¾ç»Ÿè®¡
            for feature in features:
                if feature in cluster_data.columns and pd.api.types.is_numeric_dtype(cluster_data[feature]):
                    detail[f'avg_{feature}'] = cluster_data[feature].mean()
                    detail[f'{feature}_std'] = cluster_data[feature].std()

            # ç‰¹æ®Šç‰¹å¾è®¡ç®—
            if 'is_fraudulent' in cluster_data.columns:
                detail['fraud_rate'] = cluster_data['is_fraudulent'].mean()

            if 'transaction_amount' in cluster_data.columns:
                detail['avg_transaction_amount'] = cluster_data['transaction_amount'].mean()
                detail['transaction_amount_std'] = cluster_data['transaction_amount'].std()
                # é«˜é¢äº¤æ˜“æ¯”ä¾‹
                high_amount_threshold = data['transaction_amount'].quantile(0.75)
                detail['high_amount_rate'] = (cluster_data['transaction_amount'] > high_amount_threshold).mean()

            if 'account_age_days' in cluster_data.columns:
                detail['avg_account_age_days'] = cluster_data['account_age_days'].mean()
                detail['new_account_rate'] = (cluster_data['account_age_days'] < 30).mean()

            if 'transaction_hour' in cluster_data.columns:
                detail['night_transaction_rate'] = (
                    (cluster_data['transaction_hour'] >= 22) |
                    (cluster_data['transaction_hour'] <= 6)
                ).mean()
                detail['common_hour'] = cluster_data['transaction_hour'].mode().iloc[0] if len(cluster_data['transaction_hour'].mode()) > 0 else 12

            # åˆ†ç±»ç‰¹å¾ç»Ÿè®¡
            if 'device' in cluster_data.columns:
                device_counts = cluster_data['device'].value_counts()
                detail['common_device'] = device_counts.index[0] if len(device_counts) > 0 else 'unknown'
                detail['mobile_device_rate'] = (cluster_data['device'] == 'mobile').mean()

            if 'payment_method' in cluster_data.columns:
                payment_counts = cluster_data['payment_method'].value_counts()
                detail['common_payment_method'] = payment_counts.index[0] if len(payment_counts) > 0 else 'unknown'
                detail['bank_transfer_rate'] = (cluster_data['payment_method'] == 'bank_transfer').mean()

            cluster_details.append(detail)

        return cluster_details