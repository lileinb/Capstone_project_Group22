"""
Clustering Analyzer
Clustering analysis based on real dataset features
Supports K-means, DBSCAN, Gaussian Mixture clustering
Used to identify abnormal transaction patterns and user behavior groups
"""
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from typing import Dict, Any, List, Tuple
import logging
from .cluster_risk_mapper import ClusterRiskMapper
from .intelligent_cluster_optimizer import IntelligentClusterOptimizer

# Import configuration management
try:
    from config.optimization_config import optimization_config
    CONFIG_AVAILABLE = True
except ImportError:
    CONFIG_AVAILABLE = False
    optimization_config = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ClusterAnalyzer:
    """Clustering analyzer based on real datasets"""

    def __init__(self, n_clusters: int = 5, random_state: int = 42):
        self.n_clusters = n_clusters
        self.random_state = random_state
        self.scaler = StandardScaler()
        self.pca = None
        self.risk_mapper = ClusterRiskMapper()  # Risk mapper
        self.intelligent_optimizer = IntelligentClusterOptimizer()  # Intelligent optimizer

        # Load configuration
        self.config = self._load_clustering_config()

        # Key features for clustering - optimized version, using more discriminative features
        self.clustering_features = [
            # Original basic features
            'transaction_amount', 'quantity', 'customer_age', 'account_age_days', 'transaction_hour',
            # Engineering features - amount related
            'amount_zscore', 'amount_percentile', 'is_large_amount', 'amount_risk_score',
            # Engineering features - time related
            'time_risk_score', 'is_night_transaction', 'is_early_morning',
            # Engineering features - account related
            'is_new_account', 'account_age_risk_score', 'is_frequent_user',
            # Engineering features - composite risk
            'composite_risk_score', 'anomaly_score'
        ]

        # Fallback basic features (if engineering features are not available)
        self.fallback_features = [
            'transaction_amount', 'quantity', 'customer_age', 'account_age_days', 'transaction_hour'
        ]

        # åˆ†ç±»ç‰¹å¾ç¼–ç æ˜ å°„
        self.categorical_mappings = {
            'payment_method': {'credit card': 1, 'debit card': 2, 'bank transfer': 3, 'PayPal': 4},
            'product_category': {'clothing': 1, 'electronics': 2, 'home & garden': 3, 'health & beauty': 4, 'toys & games': 5},
            'device_used': {'desktop': 1, 'mobile': 2, 'tablet': 3}
        }

    def analyze_clusters(self, data: pd.DataFrame, algorithm: str = 'auto') -> Dict[str, Any]:
        """
        Perform clustering analysis based on real dataset

        Args:
            data: Cleaned DataFrame
            algorithm: Clustering algorithm ('kmeans', 'dbscan', 'gaussian_mixture')

        Returns:
            Clustering analysis result dictionary
        """
        if data is None or data.empty:
            logger.error("Input data is empty")
            return self._empty_result()

        try:
            # Prepare clustering features
            cluster_data = self._prepare_clustering_data(data)
            if cluster_data is None or cluster_data.empty:
                logger.error("Unable to prepare clustering data")
                return self._empty_result()

            # Execute clustering - optimized version
            if algorithm == 'auto':
                # Intelligently select best algorithm
                results = self._auto_select_best_algorithm(cluster_data, data)
            elif algorithm == 'kmeans':
                results = self._kmeans_clustering(cluster_data, data)
            elif algorithm == 'dbscan':
                results = self._dbscan_clustering(cluster_data, data)
            elif algorithm == 'gaussian_mixture':
                results = self._gaussian_mixture_clustering(cluster_data, data)
            else:
                logger.warning(f"Unsupported clustering algorithm: {algorithm}, using intelligent selection")
                results = self._auto_select_best_algorithm(cluster_data, data)

            # Add clustering quality evaluation
            if len(set(results['cluster_labels'])) > 1:
                results['quality_metrics'] = self._evaluate_clustering_quality(
                    cluster_data, results['cluster_labels']
                )

            # Analyze anomalous groups
            results['anomaly_analysis'] = self._analyze_anomalies(data, results['cluster_labels'])

            # Add risk mapping
            risk_mapping_results = self.risk_mapper.map_clusters_to_risk_levels(results, data)
            results.update(risk_mapping_results)

            # å°†é£é™©æ˜ å°„ç»“æœåˆå¹¶åˆ°cluster_detailsä¸­
            cluster_risk_mapping = risk_mapping_results.get('cluster_risk_mapping', {})
            if cluster_risk_mapping and 'cluster_details' in results:
                for detail in results['cluster_details']:
                    cluster_id = detail.get('cluster_id', -1)
                    if cluster_id in cluster_risk_mapping:
                        # æ›´æ–°é£é™©ç­‰çº§å’Œè¯„åˆ†
                        risk_info = cluster_risk_mapping[cluster_id]
                        detail['risk_level'] = risk_info.get('risk_level', detail.get('risk_level', 'low'))
                        detail['risk_score'] = risk_info.get('risk_score', 0)
                        detail['risk_indicators'] = risk_info.get('risk_indicators', {})
                        detail['risk_explanation'] = risk_info.get('risk_explanation', [])

            logger.info(f"èšç±»åˆ†æå®Œæˆ: {algorithm}, å‘ç°{results['cluster_count']}ä¸ªç¾¤ä½“")
            return results

        except Exception as e:
            logger.error(f"èšç±»åˆ†æå¤±è´¥: {e}")
            return self._empty_result()

    def _prepare_clustering_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """å‡†å¤‡èšç±»æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬"""
        # ä½¿ç”¨å¢å¼ºç‰ˆç‰¹å¾é€‰æ‹©å™¨
        from backend.feature_engineer.feature_selector import FeatureSelector
        feature_selector = FeatureSelector(target_features=12)

        # åˆ›å»ºå¢å¼ºç‰¹å¾å¹¶é€‰æ‹©æœ€ä¼˜ç‰¹å¾
        enhanced_data = feature_selector._create_clustering_features(data.copy())
        selected_features = feature_selector.select_clustering_optimized_features(enhanced_data, max_features=10)

        # ä½¿ç”¨å¢å¼ºåçš„æ•°æ®
        data = enhanced_data

        if not selected_features:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„èšç±»ç‰¹å¾")
            return None

        cluster_df = pd.DataFrame()

        # æ·»åŠ é€‰ä¸­çš„ç‰¹å¾ï¼ˆç¡®ä¿ä½¿ç”¨å¢å¼ºç‰ˆç‰¹å¾é€‰æ‹©çš„ç»“æœï¼‰
        for feature in selected_features:
            if feature in data.columns:
                cluster_df[feature] = data[feature]
            else:
                logger.warning(f"é€‰ä¸­çš„ç‰¹å¾ {feature} ä¸åœ¨æ•°æ®ä¸­")

        # åªæœ‰åœ¨é€‰ä¸­ç‰¹å¾ä¸è¶³æ—¶æ‰æ·»åŠ ç¼–ç åˆ†ç±»ç‰¹å¾
        if len(cluster_df.columns) < 5:
            logger.info("é€‰ä¸­ç‰¹å¾ä¸è¶³ï¼Œæ·»åŠ ç¼–ç åˆ†ç±»ç‰¹å¾")
            for cat_feature, mapping in self.categorical_mappings.items():
                if cat_feature in data.columns and len(cluster_df.columns) < 8:
                    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥é¿å…åˆ†ç±»ç±»å‹é—®é¢˜
                    feature_values = data[cat_feature].astype(str)
                    cluster_df[f'{cat_feature}_encoded'] = feature_values.map(mapping).fillna(0)

        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
        if cluster_df.empty or cluster_df.shape[1] == 0:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„èšç±»ç‰¹å¾")
            return None

        # ä¼˜åŒ–çš„æ•°æ®é¢„å¤„ç†
        cluster_df = self._enhanced_data_preprocessing(cluster_df)

        if cluster_df is None or cluster_df.empty:
            logger.warning("æ•°æ®é¢„å¤„ç†åä¸ºç©º")
            return None

        # ä½¿ç”¨æ›´é²æ£’çš„æ ‡å‡†åŒ–æ–¹æ³•
        cluster_df_scaled = self._robust_feature_scaling(cluster_df)

        logger.info(f"èšç±»æ•°æ®å‡†å¤‡å®Œæˆï¼Œç‰¹å¾æ•°: {cluster_df_scaled.shape[1]}, æ ·æœ¬æ•°: {cluster_df_scaled.shape[0]}")
        logger.info(f"ä½¿ç”¨çš„ç‰¹å¾: {list(cluster_df_scaled.columns)}")

        return cluster_df_scaled

    def _select_optimal_clustering_features(self, data: pd.DataFrame) -> List[str]:
        """Intelligently select optimal clustering features - enhanced version"""
        logger.info("ğŸ¯ Starting intelligent feature selection")

        # Step 1: Create more discriminative features
        enhanced_data = self._create_discriminative_features(data.copy())

        # Step 2: Multi-level feature selection
        candidate_features = self._get_candidate_features(enhanced_data)

        # Step 3: Score features based on clustering friendliness
        scored_features = self._score_features_for_clustering(enhanced_data, candidate_features)

        # Step 4: Select optimal feature combination
        final_features = self._select_optimal_feature_combination(enhanced_data, scored_features)

        logger.info(f"âœ… Selected {len(final_features)} optimized features: {final_features}")
        return final_features

    def _create_discriminative_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºæ›´æœ‰åŒºåˆ†åº¦çš„ç‰¹å¾"""
        logger.info("ğŸ”§ åˆ›å»ºåŒºåˆ†æ€§ç‰¹å¾")

        # é‡‘é¢å¼‚å¸¸ç‰¹å¾
        if 'transaction_amount' in data.columns:
            amount_mean = data['transaction_amount'].mean()
            amount_std = data['transaction_amount'].std()

            data['amount_deviation'] = abs(data['transaction_amount'] - amount_mean) / amount_std
            data['is_extreme_amount'] = (data['amount_deviation'] > 2).astype(int)
            data['amount_log'] = np.log1p(data['transaction_amount'])
            data['amount_rank'] = data['transaction_amount'].rank(pct=True)

        # æ—¶é—´é£é™©ç‰¹å¾
        if 'transaction_hour' in data.columns:
            # æ·±å¤œäº¤æ˜“é£é™©
            data['is_deep_night'] = data['transaction_hour'].isin([0, 1, 2, 3, 4, 5]).astype(int)
            # å·¥ä½œæ—¶é—´å¤–äº¤æ˜“
            data['is_off_hours'] = (~data['transaction_hour'].isin([9, 10, 11, 12, 13, 14, 15, 16, 17])).astype(int)
            # æ—¶é—´é£é™©è¯„åˆ†
            risk_hours = {0: 3, 1: 3, 2: 3, 3: 3, 4: 2, 5: 2, 22: 2, 23: 3}
            data['hour_risk_score'] = data['transaction_hour'].map(risk_hours).fillna(1)

        # è´¦æˆ·é£é™©ç‰¹å¾
        if 'account_age_days' in data.columns:
            data['is_very_new_account'] = (data['account_age_days'] < 30).astype(int)
            data['is_new_account'] = (data['account_age_days'] < 90).astype(int)
            data['account_maturity'] = np.log1p(data['account_age_days'])

        # ç”¨æˆ·è¡Œä¸ºç‰¹å¾
        if 'customer_age' in data.columns:
            data['is_young_customer'] = (data['customer_age'] < 25).astype(int)
            data['is_senior_customer'] = (data['customer_age'] > 65).astype(int)

        # å¤åˆé£é™©ç‰¹å¾
        risk_features = []
        if 'amount_deviation' in data.columns:
            risk_features.append('amount_deviation')
        if 'hour_risk_score' in data.columns:
            risk_features.append('hour_risk_score')
        if 'is_very_new_account' in data.columns:
            risk_features.append('is_very_new_account')

        if len(risk_features) >= 2:
            data['composite_risk'] = data[risk_features].sum(axis=1)
            data['risk_interaction'] = data[risk_features].prod(axis=1)

        logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œå½“å‰ç‰¹å¾æ•°: {len(data.columns)}")
        return data

    def _get_candidate_features(self, data: pd.DataFrame) -> List[str]:
        """è·å–å€™é€‰ç‰¹å¾"""
        # æ’é™¤éæ•°å€¼ç‰¹å¾å’Œæ ‡ç­¾
        exclude_patterns = ['is_fraudulent', '_encoded', 'payment_method', 'device_used', 'product_category']
        numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()

        candidate_features = []
        for feature in numeric_features:
            if not any(pattern in feature for pattern in exclude_patterns):
                candidate_features.append(feature)

        return candidate_features

    def _score_features_for_clustering(self, data: pd.DataFrame, features: List[str]) -> Dict[str, float]:
        """ä¸ºèšç±»è¯„åˆ†ç‰¹å¾"""
        feature_scores = {}

        for feature in features:
            if feature not in data.columns:
                continue

            feature_data = data[feature].dropna()
            if len(feature_data) == 0:
                feature_scores[feature] = 0.0
                continue

            score = 0.0

            # 1. æ–¹å·®è¯„åˆ† (30%)
            variance = feature_data.var()
            if variance > 0:
                score += min(variance / 10, 1.0) * 0.3

            # 2. åˆ†å¸ƒè¯„åˆ† (25%) - åå‘æ­£æ€åˆ†å¸ƒçš„ç‰¹å¾
            try:
                from scipy import stats
                _, p_value = stats.normaltest(feature_data)
                if p_value > 0.05:  # æ¥è¿‘æ­£æ€åˆ†å¸ƒ
                    score += 0.25
                else:
                    score += 0.1
            except:
                score += 0.1

            # 3. åŒºåˆ†åº¦è¯„åˆ† (25%) - åŸºäºåˆ†ä½æ•°èŒƒå›´
            q75, q25 = np.percentile(feature_data, [75, 25])
            iqr = q75 - q25
            if iqr > 0:
                score += min(iqr / feature_data.std(), 1.0) * 0.25

            # 4. ç‰¹å¾ç±»å‹å¥–åŠ± (20%)
            if any(suffix in feature for suffix in ['_zscore', '_percentile', '_score', '_deviation', '_rank']):
                score += 0.2
            elif any(suffix in feature for suffix in ['_log', '_sqrt', 'composite_', 'risk_']):
                score += 0.15
            elif feature in ['transaction_amount', 'customer_age', 'account_age_days']:
                score += 0.1

            feature_scores[feature] = score

        return feature_scores

    def _select_optimal_feature_combination(self, data: pd.DataFrame, scored_features: Dict[str, float]) -> List[str]:
        """é€‰æ‹©æœ€ä¼˜ç‰¹å¾ç»„åˆ"""
        # æŒ‰åˆ†æ•°æ’åº
        sorted_features = sorted(scored_features.items(), key=lambda x: x[1], reverse=True)

        selected_features = []
        correlation_threshold = 0.8

        for feature, score in sorted_features:
            if len(selected_features) >= 10:  # é™åˆ¶ç‰¹å¾æ•°é‡
                break

            if score < 0.3:  # åˆ†æ•°å¤ªä½çš„ç‰¹å¾è·³è¿‡
                continue

            # æ£€æŸ¥ä¸å·²é€‰ç‰¹å¾çš„ç›¸å…³æ€§
            is_redundant = False
            for selected_feature in selected_features:
                if feature in data.columns and selected_feature in data.columns:
                    try:
                        corr = data[feature].corr(data[selected_feature])
                        if abs(corr) > correlation_threshold:
                            is_redundant = True
                            break
                    except:
                        continue

            if not is_redundant:
                selected_features.append(feature)

        # ç¡®ä¿è‡³å°‘æœ‰åŸºç¡€ç‰¹å¾
        essential_features = ['transaction_amount', 'customer_age', 'account_age_days', 'transaction_hour']
        for feature in essential_features:
            if feature in data.columns and feature not in selected_features and len(selected_features) < 10:
                selected_features.append(feature)

        return selected_features[:8]  # æœ€å¤š8ä¸ªç‰¹å¾

    def _is_feature_valid_for_clustering(self, feature_series: pd.Series) -> bool:
        """æ£€æŸ¥ç‰¹å¾æ˜¯å¦é€‚åˆèšç±»"""
        # æ£€æŸ¥ç¼ºå¤±å€¼æ¯”ä¾‹
        if feature_series.isna().sum() / len(feature_series) > 0.5:
            return False

        # æ£€æŸ¥æ–¹å·®
        if feature_series.var() < 1e-6:
            return False

        # æ£€æŸ¥æ˜¯å¦å…¨ä¸ºåŒä¸€å€¼
        if feature_series.nunique() <= 1:
            return False

        return True

    def _robust_feature_scaling(self, data: pd.DataFrame) -> pd.DataFrame:
        """ä½¿ç”¨æ›´é²æ£’çš„ç‰¹å¾ç¼©æ”¾æ–¹æ³•"""
        from sklearn.preprocessing import RobustScaler, QuantileTransformer

        # æ£€æŸ¥æ•°æ®åˆ†å¸ƒï¼Œé€‰æ‹©åˆé€‚çš„ç¼©æ”¾æ–¹æ³•
        skewness_threshold = 2.0
        highly_skewed_features = []

        for col in data.columns:
            if data[col].dtype in ['int64', 'float64']:
                skewness = abs(data[col].skew())
                if skewness > skewness_threshold:
                    highly_skewed_features.append(col)

        scaled_data = data.copy()

        if len(highly_skewed_features) > len(data.columns) * 0.5:
            # å¦‚æœå¤§éƒ¨åˆ†ç‰¹å¾éƒ½é«˜åº¦åæ–œï¼Œä½¿ç”¨åˆ†ä½æ•°å˜æ¢
            logger.info("ä½¿ç”¨åˆ†ä½æ•°å˜æ¢å¤„ç†é«˜åº¦åæ–œçš„æ•°æ®")
            transformer = QuantileTransformer(output_distribution='normal', random_state=42)
            scaled_values = transformer.fit_transform(data)
            scaled_data = pd.DataFrame(scaled_values, columns=data.columns, index=data.index)
        else:
            # å¦åˆ™ä½¿ç”¨é²æ£’ç¼©æ”¾å™¨
            logger.info("ä½¿ç”¨é²æ£’ç¼©æ”¾å™¨å¤„ç†æ•°æ®")
            scaler = RobustScaler()
            scaled_values = scaler.fit_transform(data)
            scaled_data = pd.DataFrame(scaled_values, columns=data.columns, index=data.index)

        return scaled_data

    def _auto_select_best_algorithm(self, cluster_data: pd.DataFrame, original_data: pd.DataFrame) -> Dict[str, Any]:
        """Intelligently select best clustering algorithm"""
        logger.info("ğŸ¤– Starting intelligent algorithm selection")

        algorithms_to_try = ['dbscan', 'kmeans', 'gaussian_mixture']
        best_result = None
        best_score = -1

        for algorithm in algorithms_to_try:
            try:
                logger.info(f"Trying algorithm: {algorithm}")

                if algorithm == 'dbscan':
                    result = self._dbscan_clustering(cluster_data, original_data)
                elif algorithm == 'kmeans':
                    result = self._kmeans_clustering(cluster_data, original_data)
                elif algorithm == 'gaussian_mixture':
                    result = self._gaussian_mixture_clustering(cluster_data, original_data)

                # è¯„ä¼°èšç±»è´¨é‡
                labels = result['cluster_labels']
                if len(set(labels)) > 1:
                    # è®¡ç®—ç»¼åˆè¯„åˆ†
                    silhouette = silhouette_score(cluster_data, labels)
                    calinski = calinski_harabasz_score(cluster_data, labels)

                    # å¯¹äºæ¬ºè¯ˆæ£€æµ‹ï¼Œæˆ‘ä»¬æ›´å…³å¿ƒèƒ½å¦å‘ç°å¼‚å¸¸ç¾¤ä½“
                    cluster_count = result['cluster_count']
                    size_balance = self._evaluate_cluster_size_balance(labels)

                    # æ”¹è¿›çš„ç»¼åˆè¯„åˆ†æœºåˆ¶ - åå‘4ä¸ªé£é™©ç­‰çº§
                    # å¯¹äºæ¬ºè¯ˆæ£€æµ‹ï¼Œæˆ‘ä»¬æ›´é‡è§†èƒ½å‘ç°å¼‚å¸¸æ¨¡å¼çš„èƒ½åŠ›
                    silhouette_weight = 0.35 if silhouette > 0.2 else 0.25
                    cluster_quality = self._evaluate_cluster_quality_for_fraud_detection(labels, original_data)

                    # èšç±»æ•°é‡åå¥½ï¼š4-6ä¸ªèšç±»æœ€ä½³ï¼Œå¯¹åº”é£é™©ç­‰çº§
                    cluster_preference = 0.0
                    if cluster_count == 4:
                        cluster_preference = 0.3  # æœ€ä½³ï¼š4ä¸ªèšç±»å¯¹åº”4ä¸ªé£é™©ç­‰çº§
                    elif cluster_count == 5:
                        cluster_preference = 0.25  # æ¬¡ä½³ï¼š5ä¸ªèšç±»
                    elif cluster_count == 3:
                        cluster_preference = 0.2   # å¯æ¥å—ï¼š3ä¸ªèšç±»
                    elif cluster_count == 6:
                        cluster_preference = 0.15  # å¯æ¥å—ï¼š6ä¸ªèšç±»
                    elif cluster_count == 2:
                        cluster_preference = 0.1   # è¾ƒå·®ï¼šåªæœ‰2ä¸ªèšç±»
                    elif cluster_count == 1:
                        cluster_preference = -0.3  # å¾ˆå·®ï¼šåªæœ‰1ä¸ªèšç±»
                    else:
                        cluster_preference = max(0, 0.1 - abs(cluster_count - 4) * 0.05)

                    combined_score = (
                        silhouette * silhouette_weight +  # è½®å»“ç³»æ•°ï¼ˆé™ä½æƒé‡ï¼‰
                        (calinski / 2000) * 0.1 +  # Calinski-HarabaszæŒ‡æ•°
                        size_balance * 0.1 +  # èšç±»å¤§å°å¹³è¡¡æ€§
                        cluster_preference +  # èšç±»æ•°é‡åå¥½ï¼ˆæ–°å¢ï¼‰
                        cluster_quality * 0.15  # æ¬ºè¯ˆæ£€æµ‹è´¨é‡
                    )

                    logger.info(f"{algorithm} è¯„åˆ†: {combined_score:.3f} (è½®å»“ç³»æ•°: {silhouette:.3f})")

                    if combined_score > best_score:
                        best_score = combined_score
                        best_result = result
                        logger.info(f"ğŸ† æ–°çš„æœ€ä½³ç®—æ³•: {algorithm}")

            except Exception as e:
                logger.warning(f"ç®—æ³• {algorithm} å¤±è´¥: {e}")
                continue

        if best_result is None:
            logger.warning("æ‰€æœ‰ç®—æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤KMeans")
            best_result = self._kmeans_clustering(cluster_data, original_data)

        logger.info(f"âœ… æœ€ç»ˆé€‰æ‹©ç®—æ³•: {best_result['algorithm']}, è¯„åˆ†: {best_score:.3f}")
        return best_result

    def _evaluate_cluster_size_balance(self, labels: List[int]) -> float:
        """è¯„ä¼°èšç±»å¤§å°çš„å¹³è¡¡æ€§"""
        from collections import Counter

        label_counts = Counter(labels)
        if -1 in label_counts:  # ç§»é™¤DBSCANçš„å™ªå£°ç‚¹
            del label_counts[-1]

        if len(label_counts) <= 1:
            return 0.0

        sizes = list(label_counts.values())
        total = sum(sizes)
        proportions = [s / total for s in sizes]

        # è®¡ç®—å¹³è¡¡æ€§ï¼ˆè¶Šæ¥è¿‘å‡åŒ€åˆ†å¸ƒè¶Šå¥½ï¼‰
        ideal_proportion = 1.0 / len(proportions)
        balance_score = 1.0 - sum(abs(p - ideal_proportion) for p in proportions) / 2

        return balance_score

    def _evaluate_cluster_quality_for_fraud_detection(self, labels: List[int], data: pd.DataFrame) -> float:
        """è¯„ä¼°èšç±»å¯¹æ¬ºè¯ˆæ£€æµ‹çš„è´¨é‡"""
        try:
            from collections import Counter

            # è®¡ç®—èšç±»çš„å¼‚å¸¸ç‰¹å¾åˆ†å¸ƒ
            quality_score = 0.0
            label_counts = Counter(labels)

            # ç§»é™¤å™ªå£°ç‚¹
            if -1 in label_counts:
                del label_counts[-1]

            if len(label_counts) <= 1:
                return 0.0

            # æ£€æŸ¥æ˜¯å¦æœ‰å°èšç±»ï¼ˆå¯èƒ½æ˜¯å¼‚å¸¸ç¾¤ä½“ï¼‰
            total_samples = sum(label_counts.values())
            small_clusters = sum(1 for count in label_counts.values() if count / total_samples < 0.1)
            if small_clusters > 0:
                quality_score += 0.3  # æœ‰å°èšç±»æ˜¯å¥½äº‹

            # æ£€æŸ¥èšç±»é—´çš„ç‰¹å¾å·®å¼‚
            if 'transaction_amount' in data.columns:
                cluster_amounts = []
                for label in label_counts.keys():
                    cluster_mask = [l == label for l in labels]
                    cluster_data = data[cluster_mask]
                    if len(cluster_data) > 0:
                        cluster_amounts.append(cluster_data['transaction_amount'].mean())

                if len(cluster_amounts) > 1:
                    amount_std = np.std(cluster_amounts)
                    amount_mean = np.mean(cluster_amounts)
                    if amount_mean > 0:
                        cv = amount_std / amount_mean  # å˜å¼‚ç³»æ•°
                        quality_score += min(cv, 1.0) * 0.4

            # æ£€æŸ¥æ—¶é—´åˆ†å¸ƒå·®å¼‚
            if 'transaction_hour' in data.columns:
                hour_distributions = []
                for label in label_counts.keys():
                    cluster_mask = [l == label for l in labels]
                    cluster_data = data[cluster_mask]
                    if len(cluster_data) > 0:
                        night_ratio = sum(cluster_data['transaction_hour'].isin([0, 1, 2, 3, 4, 5])) / len(cluster_data)
                        hour_distributions.append(night_ratio)

                if len(hour_distributions) > 1:
                    hour_std = np.std(hour_distributions)
                    quality_score += hour_std * 0.3

            return min(quality_score, 1.0)

        except Exception as e:
            logger.warning(f"æ¬ºè¯ˆæ£€æµ‹è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return 0.0

    def _load_clustering_config(self) -> Dict[str, Any]:
        """Load clustering configuration"""
        if CONFIG_AVAILABLE and optimization_config:
            return optimization_config.get_clustering_config()
        else:
            # Default configuration
            return {
                "auto_k_optimization": True,
                "max_k": 10,
                "min_k": 2,
                "dbscan_auto_params": True,
                "feature_selection": {
                    "enabled": True,
                    "variance_threshold": 0.01,
                    "correlation_threshold": 0.95,
                    "min_features": 3
                },
                "data_quality": {
                    "outlier_method": "iqr",
                    "outlier_factor": 1.5,
                    "missing_value_threshold": 0.5,
                    "quality_threshold": 70.0
                }
            }

    def _enhanced_data_preprocessing(self, data: pd.DataFrame) -> pd.DataFrame:
        """å¢å¼ºçš„æ•°æ®é¢„å¤„ç†"""
        try:
            processed_data = data.copy()

            # 1. å¤„ç†ç¼ºå¤±å€¼
            processed_data = self._handle_missing_values_smart(processed_data)

            # 2. å¤„ç†å¼‚å¸¸å€¼
            processed_data = self._handle_outliers_iqr(processed_data)

            # 3. å¤„ç†æ— ç©·å€¼
            processed_data = processed_data.replace([np.inf, -np.inf], np.nan)
            processed_data = processed_data.fillna(processed_data.median())

            # 4. æ•°æ®è´¨é‡æ£€æŸ¥
            quality_score = self._calculate_data_quality_score(processed_data)
            logger.info(f"æ•°æ®è´¨é‡è¯„åˆ†: {quality_score:.2f}/100")

            return processed_data

        except Exception as e:
            logger.error(f"å¢å¼ºæ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return data.fillna(0)  # å›é€€åˆ°ç®€å•å¤„ç†

    def _handle_missing_values_smart(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†"""
        try:
            processed_data = data.copy()

            for column in processed_data.columns:
                missing_ratio = processed_data[column].isnull().sum() / len(processed_data)

                if missing_ratio > 0:
                    if missing_ratio > 0.5:
                        # ç¼ºå¤±å€¼è¿‡å¤šï¼Œåˆ é™¤è¯¥åˆ—
                        logger.warning(f"åˆ— {column} ç¼ºå¤±å€¼æ¯”ä¾‹ {missing_ratio:.2f}ï¼Œå°†è¢«åˆ é™¤")
                        processed_data = processed_data.drop(columns=[column])
                    else:
                        # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©å¡«å……ç­–ç•¥
                        if processed_data[column].dtype in ['int64', 'float64']:
                            # æ•°å€¼å‹ï¼šä½¿ç”¨ä¸­ä½æ•°
                            fill_value = processed_data[column].median()
                        else:
                            # å…¶ä»–ç±»å‹ï¼šä½¿ç”¨ä¼—æ•°
                            mode_values = processed_data[column].mode()
                            fill_value = mode_values.iloc[0] if not mode_values.empty else 0

                        processed_data[column] = processed_data[column].fillna(fill_value)

            return processed_data

        except Exception as e:
            logger.warning(f"æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†å¤±è´¥: {e}")
            return data.fillna(0)

    def _handle_outliers_iqr(self, data: pd.DataFrame) -> pd.DataFrame:
        """ä½¿ç”¨IQRæ–¹æ³•å¤„ç†å¼‚å¸¸å€¼"""
        try:
            processed_data = data.copy()

            for column in processed_data.columns:
                if processed_data[column].dtype in ['int64', 'float64']:
                    Q1 = processed_data[column].quantile(0.25)
                    Q3 = processed_data[column].quantile(0.75)
                    IQR = Q3 - Q1

                    if IQR > 0:  # é¿å…é™¤é›¶é”™è¯¯
                        # å®šä¹‰å¼‚å¸¸å€¼è¾¹ç•Œ
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR

                        # ç»Ÿè®¡å¼‚å¸¸å€¼æ•°é‡
                        outliers_count = ((processed_data[column] < lower_bound) |
                                        (processed_data[column] > upper_bound)).sum()

                        if outliers_count > 0:
                            outlier_ratio = outliers_count / len(processed_data)
                            logger.info(f"åˆ— {column} å‘ç° {outliers_count} ä¸ªå¼‚å¸¸å€¼ ({outlier_ratio:.2%})")

                            # ä½¿ç”¨Winsorizationæ–¹æ³•å¤„ç†å¼‚å¸¸å€¼
                            processed_data[column] = processed_data[column].clip(lower_bound, upper_bound)

            return processed_data

        except Exception as e:
            logger.warning(f"å¼‚å¸¸å€¼å¤„ç†å¤±è´¥: {e}")
            return data

    def _calculate_data_quality_score(self, data: pd.DataFrame) -> float:
        """Calculate data quality score"""
        try:
            if data.empty:
                return 0.0

            # 1. Completeness score (40%)
            completeness = (1 - data.isnull().sum().sum() / (len(data) * len(data.columns))) * 40

            # 2. Consistency score (30%) - based on data type consistency
            consistency = 30  # Assume data types are consistent

            # 3. Validity score (20%) - based on numerical range reasonableness
            validity = 0
            for column in data.columns:
                if data[column].dtype in ['int64', 'float64']:
                    # Check for infinite values or extreme values
                    if not np.isinf(data[column]).any() and data[column].std() > 0:
                        validity += 20 / len(data.columns)

            # 4. Uniqueness score (10%) - based on duplicate row proportion
            uniqueness = (1 - data.duplicated().sum() / len(data)) * 10

            total_score = completeness + consistency + validity + uniqueness
            return min(100.0, max(0.0, total_score))

        except Exception as e:
            logger.warning(f"æ•°æ®è´¨é‡è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 50.0  # é»˜è®¤ä¸­ç­‰è´¨é‡

    def _find_optimal_k(self, data: pd.DataFrame, max_k: int = None) -> int:
        """è‡ªåŠ¨ç¡®å®šæœ€ä¼˜Kå€¼"""
        try:
            # ä»é…ç½®è·å–å‚æ•° - æ‰©å¤§æœç´¢èŒƒå›´
            if max_k is None:
                max_k = self.config.get('max_k', 12)  # å¢åŠ åˆ°12
            min_k = self.config.get('min_k', 2)

            if len(data) < 10:
                return min(min_k, len(data))

            # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨Kä¼˜åŒ–
            if not self.config.get('auto_k_optimization', True):
                return self.n_clusters

            # è®¡ç®—ä¸åŒKå€¼çš„è¯„ä¼°æŒ‡æ ‡
            k_range = range(min_k, min(max_k + 1, len(data) // 2))
            inertias = []
            silhouette_scores = []

            for k in k_range:
                try:
                    kmeans = KMeans(n_clusters=k, random_state=self.random_state, n_init=10)
                    labels = kmeans.fit_predict(data)

                    # è®¡ç®—æƒ¯æ€§ï¼ˆè‚˜éƒ¨æ³•åˆ™ï¼‰
                    inertias.append(kmeans.inertia_)

                    # è®¡ç®—è½®å»“ç³»æ•°
                    if len(set(labels)) > 1:
                        sil_score = silhouette_score(data, labels)
                        silhouette_scores.append(sil_score)
                    else:
                        silhouette_scores.append(0)

                except Exception as e:
                    logger.warning(f"K={k}æ—¶èšç±»å¤±è´¥: {e}")
                    inertias.append(float('inf'))
                    silhouette_scores.append(0)

            # ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™æ‰¾åˆ°æœ€ä¼˜K
            optimal_k_elbow = self._find_elbow_point(list(k_range), inertias)

            # ä½¿ç”¨è½®å»“ç³»æ•°æ‰¾åˆ°æœ€ä¼˜K
            if silhouette_scores:
                optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]
            else:
                optimal_k_silhouette = optimal_k_elbow

            # ç»¼åˆè€ƒè™‘ä¸¤ç§æ–¹æ³•ï¼Œä½†åå‘4ä¸ªèšç±»ï¼ˆå¯¹åº”4ä¸ªé£é™©ç­‰çº§ï¼‰
            candidate_k_values = [optimal_k_elbow, optimal_k_silhouette]

            # å¦‚æœ4åœ¨å€™é€‰èŒƒå›´å†…ä¸”è½®å»“ç³»æ•°åˆç†ï¼Œä¼˜å…ˆé€‰æ‹©4
            if 4 in k_range and len(silhouette_scores) >= (4 - min_k):
                k4_silhouette = silhouette_scores[4 - min_k]
                max_silhouette = max(silhouette_scores) if silhouette_scores else 0

                # æ”¾å®½K=4çš„é€‰æ‹©æ¡ä»¶ï¼Œæ›´å¼ºè°ƒä¸šåŠ¡éœ€æ±‚
                if k4_silhouette >= max_silhouette - 0.4 and k4_silhouette > 0.3:
                    optimal_k = 4
                    logger.info(f"ä¼˜å…ˆé€‰æ‹©K=4ï¼ˆé£é™©ç­‰çº§å¯¹åº”ï¼‰ï¼Œè½®å»“ç³»æ•°: {k4_silhouette:.3f}")
                else:
                    optimal_k = optimal_k_silhouette
            else:
                # å¦‚æœ4ä¸åœ¨èŒƒå›´å†…ï¼ŒæŒ‰åŸé€»è¾‘é€‰æ‹©
                if abs(optimal_k_elbow - optimal_k_silhouette) <= 1:
                    optimal_k = optimal_k_silhouette
                else:
                    optimal_k = optimal_k_silhouette

            # ç¡®ä¿Kå€¼åœ¨åˆç†èŒƒå›´å†…
            optimal_k = max(min_k, min(optimal_k, max_k))

            logger.info(f"Kå€¼ä¼˜åŒ–: è‚˜éƒ¨æ³•åˆ™={optimal_k_elbow}, è½®å»“ç³»æ•°={optimal_k_silhouette}, æœ€ç»ˆé€‰æ‹©={optimal_k}")
            logger.info(f"è½®å»“ç³»æ•°åˆ†å¸ƒ: {dict(zip(k_range, silhouette_scores))}")
            return optimal_k

        except Exception as e:
            logger.warning(f"Kå€¼ä¼˜åŒ–å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤å€¼")
            return 4  # ä½¿ç”¨å›ºå®šé»˜è®¤å€¼è€Œä¸æ˜¯self.n_clusters

    def _find_elbow_point(self, k_values: List[int], inertias: List[float]) -> int:
        """ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™æ‰¾åˆ°æœ€ä¼˜Kå€¼"""
        try:
            if len(k_values) < 3:
                return k_values[0] if k_values else 2

            # è®¡ç®—äºŒé˜¶å¯¼æ•°æ¥æ‰¾åˆ°è‚˜éƒ¨ç‚¹
            diffs = np.diff(inertias)
            diffs2 = np.diff(diffs)

            # æ‰¾åˆ°äºŒé˜¶å¯¼æ•°æœ€å¤§çš„ç‚¹ï¼ˆè‚˜éƒ¨ï¼‰
            if len(diffs2) > 0:
                elbow_idx = np.argmax(diffs2) + 2  # +2å› ä¸ºäºŒé˜¶å¯¼æ•°çš„ç´¢å¼•åç§»
                if elbow_idx < len(k_values):
                    return k_values[elbow_idx]

            # å¦‚æœæ‰¾ä¸åˆ°æ˜æ˜¾çš„è‚˜éƒ¨ï¼Œé€‰æ‹©ä¸­é—´å€¼
            return k_values[len(k_values) // 2]

        except Exception as e:
            logger.warning(f"è‚˜éƒ¨ç‚¹è®¡ç®—å¤±è´¥: {e}")
            return k_values[0] if k_values else 2

    def _find_optimal_dbscan_params(self, data: pd.DataFrame) -> Tuple[float, int]:
        """è‡ªåŠ¨ç¡®å®šDBSCANæœ€ä¼˜å‚æ•°"""
        try:
            from sklearn.neighbors import NearestNeighbors

            # 1. ç¡®å®šmin_samples (é€šå¸¸è®¾ä¸ºç‰¹å¾æ•°é‡çš„2å€)
            min_samples = max(4, min(2 * data.shape[1], len(data) // 10))

            # 2. ä½¿ç”¨k-distanceå›¾ç¡®å®šeps
            # è®¡ç®—æ¯ä¸ªç‚¹åˆ°ç¬¬kä¸ªæœ€è¿‘é‚»çš„è·ç¦»
            neighbors = NearestNeighbors(n_neighbors=min_samples)
            neighbors_fit = neighbors.fit(data)
            distances, indices = neighbors_fit.kneighbors(data)

            # å–ç¬¬kä¸ªæœ€è¿‘é‚»çš„è·ç¦»å¹¶æ’åº
            k_distances = distances[:, min_samples-1]
            k_distances = np.sort(k_distances)

            # å¯»æ‰¾è‚˜éƒ¨ç‚¹ä½œä¸ºeps
            eps = self._find_eps_elbow_point(k_distances)

            # éªŒè¯å‚æ•°çš„æœ‰æ•ˆæ€§
            eps, min_samples = self._validate_dbscan_params(data, eps, min_samples)

            return eps, min_samples

        except Exception as e:
            logger.warning(f"DBSCANå‚æ•°ä¼˜åŒ–å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤å‚æ•°")
            return 0.5, 5

    def _find_eps_elbow_point(self, k_distances: np.ndarray) -> float:
        """åœ¨k-distanceå›¾ä¸­æ‰¾åˆ°è‚˜éƒ¨ç‚¹ç¡®å®šeps"""
        try:
            if len(k_distances) < 10:
                return np.median(k_distances)

            # è®¡ç®—ä¸€é˜¶å’ŒäºŒé˜¶å¯¼æ•°
            x = np.arange(len(k_distances))
            diffs = np.diff(k_distances)
            diffs2 = np.diff(diffs)

            # æ‰¾åˆ°äºŒé˜¶å¯¼æ•°æœ€å¤§çš„ç‚¹
            if len(diffs2) > 0:
                elbow_idx = np.argmax(diffs2)
                # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                elbow_idx = min(elbow_idx, len(k_distances) - 1)
                return k_distances[elbow_idx]
            else:
                # å¦‚æœæ‰¾ä¸åˆ°è‚˜éƒ¨ï¼Œä½¿ç”¨75åˆ†ä½æ•°
                return np.percentile(k_distances, 75)

        except Exception as e:
            logger.warning(f"epsè‚˜éƒ¨ç‚¹è®¡ç®—å¤±è´¥: {e}")
            return np.median(k_distances) if len(k_distances) > 0 else 0.5

    def _validate_dbscan_params(self, data: pd.DataFrame, eps: float, min_samples: int) -> Tuple[float, int]:
        """éªŒè¯å’Œè°ƒæ•´DBSCANå‚æ•°"""
        try:
            # æµ‹è¯•å‚æ•°æ˜¯å¦ä¼šäº§ç”Ÿåˆç†çš„èšç±»ç»“æœ
            test_dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            test_labels = test_dbscan.fit_predict(data)

            n_clusters = len(set(test_labels)) - (1 if -1 in test_labels else 0)
            noise_ratio = (test_labels == -1).sum() / len(test_labels)

            # è°ƒæ•´å‚æ•°ä»¥è·å¾—æ›´å¥½çš„ç»“æœ
            if n_clusters == 0 or noise_ratio > 0.5:
                # èšç±»æ•°å¤ªå°‘æˆ–å™ªå£°å¤ªå¤šï¼Œå‡å°eps
                eps = eps * 0.7
                logger.info(f"è°ƒæ•´eps: {eps:.3f} (å‡å°)")
            elif n_clusters > len(data) // 10:
                # èšç±»æ•°å¤ªå¤šï¼Œå¢å¤§eps
                eps = eps * 1.3
                logger.info(f"è°ƒæ•´eps: {eps:.3f} (å¢å¤§)")

            # ç¡®ä¿min_samplesåœ¨åˆç†èŒƒå›´å†…
            min_samples = max(3, min(min_samples, len(data) // 5))

            return eps, min_samples

        except Exception as e:
            logger.warning(f"DBSCANå‚æ•°éªŒè¯å¤±è´¥: {e}")
            return eps, min_samples

    def _kmeans_clustering(self, cluster_data: pd.DataFrame, original_data: pd.DataFrame) -> Dict[str, Any]:
        """K-meansèšç±» - ä¼˜åŒ–ç‰ˆæœ¬"""
        # è‡ªåŠ¨ç¡®å®šæœ€ä¼˜Kå€¼
        optimal_k = self._find_optimal_k(cluster_data)
        logger.info(f"è‡ªåŠ¨ç¡®å®šæœ€ä¼˜Kå€¼: {optimal_k}")

        # ä½¿ç”¨æ›´å¥½çš„KMeanså‚æ•°
        kmeans = KMeans(
            n_clusters=optimal_k,
            random_state=self.random_state,
            n_init=20,  # å¢åŠ åˆå§‹åŒ–æ¬¡æ•°
            max_iter=500,  # å¢åŠ æœ€å¤§è¿­ä»£æ¬¡æ•°
            algorithm='lloyd'  # ä½¿ç”¨ç»å…¸ç®—æ³•ç¡®ä¿ç¨³å®šæ€§
        )
        labels = kmeans.fit_predict(cluster_data)

        # åˆ†ææ¯ä¸ªèšç±»
        cluster_details = []
        for i in range(optimal_k):  # ä½¿ç”¨optimal_kè€Œä¸æ˜¯self.n_clusters
            cluster_mask = labels == i
            cluster_size = np.sum(cluster_mask)

            if cluster_size > 0:
                # è·å–è¯¥èšç±»çš„åŸå§‹æ•°æ®
                cluster_original = original_data[cluster_mask]

                # è®¡ç®—èšç±»ç‰¹å¾
                cluster_info = self._analyze_cluster_characteristics(cluster_original, i)
                cluster_info['size'] = cluster_size
                cluster_info['percentage'] = round(cluster_size / len(labels) * 100, 2)

                cluster_details.append(cluster_info)

        return {
            'algorithm': 'kmeans',
            'cluster_count': optimal_k,  # ä½¿ç”¨optimal_k
            'cluster_labels': labels.tolist(),
            'cluster_details': cluster_details,
            'centers': kmeans.cluster_centers_.tolist()
        }

    def _dbscan_clustering(self, cluster_data: pd.DataFrame, original_data: pd.DataFrame) -> Dict[str, Any]:
        """DBSCANèšç±» - ä¼˜åŒ–ç‰ˆæœ¬"""
        # è‡ªåŠ¨ç¡®å®šæœ€ä¼˜å‚æ•°
        optimal_eps, optimal_min_samples = self._find_optimal_dbscan_params(cluster_data)
        logger.info(f"è‡ªåŠ¨ç¡®å®šDBSCANå‚æ•°: eps={optimal_eps:.3f}, min_samples={optimal_min_samples}")

        dbscan = DBSCAN(eps=optimal_eps, min_samples=optimal_min_samples)
        labels = dbscan.fit_predict(cluster_data)

        unique_labels = set(labels)
        n_clusters = len(unique_labels) - (1 if -1 in labels else 0)

        cluster_details = []
        for label in unique_labels:
            if label == -1:
                continue  # Skip noise points

            cluster_mask = labels == label
            cluster_size = np.sum(cluster_mask)

            if cluster_size > 0:
                cluster_original = original_data[cluster_mask]
                cluster_info = self._analyze_cluster_characteristics(cluster_original, label)
                cluster_info['size'] = cluster_size
                cluster_info['percentage'] = round(cluster_size / len(labels) * 100, 2)
                cluster_details.append(cluster_info)

        # Analyze noise points
        noise_count = np.sum(labels == -1)

        return {
            'algorithm': 'dbscan',
            'cluster_count': n_clusters,
            'cluster_labels': labels.tolist(),
            'cluster_details': cluster_details,
            'noise_points': noise_count,
            'noise_percentage': round(noise_count / len(labels) * 100, 2)
        }

    def _gaussian_mixture_clustering(self, cluster_data: pd.DataFrame, original_data: pd.DataFrame) -> Dict[str, Any]:
        """é«˜æ–¯æ··åˆæ¨¡å‹èšç±»"""
        gmm = GaussianMixture(n_components=self.n_clusters, random_state=self.random_state)
        labels = gmm.fit_predict(cluster_data)

        cluster_details = []
        for i in range(self.n_clusters):
            cluster_mask = labels == i
            cluster_size = np.sum(cluster_mask)

            if cluster_size > 0:
                cluster_original = original_data[cluster_mask]
                cluster_info = self._analyze_cluster_characteristics(cluster_original, i)
                cluster_info['size'] = cluster_size
                cluster_info['percentage'] = round(cluster_size / len(labels) * 100, 2)
                cluster_details.append(cluster_info)

        return {
            'algorithm': 'gaussian_mixture',
            'cluster_count': self.n_clusters,
            'cluster_labels': labels.tolist(),
            'cluster_details': cluster_details,
            'bic_score': gmm.bic(cluster_data),
            'aic_score': gmm.aic(cluster_data)
        }

    def fit_kmeans(self, data: pd.DataFrame, n_clusters: int = 4, random_state: int = 42) -> np.ndarray:
        self.method = 'kmeans'
        self.model = KMeans(n_clusters=n_clusters, random_state=random_state)
        self.labels_ = self.model.fit_predict(data)
        self.centers_ = self.model.cluster_centers_
        logger.info(f"K-meansèšç±»å®Œæˆï¼Œèšç±»æ•°: {n_clusters}")
        return self.labels_

    def _analyze_cluster_characteristics(self, cluster_data: pd.DataFrame, cluster_id: int) -> Dict[str, Any]:
        """Analyze cluster characteristics"""
        characteristics = {
            'cluster_id': cluster_id,
            'avg_transaction_amount': 0,
            'avg_customer_age': 0,
            'common_payment_method': 'unknown',
            'common_device': 'unknown',
            'common_category': 'unknown',
            'fraud_rate': 0,
            'risk_level': 'low'
        }

        if cluster_data.empty:
            return characteristics

        # äº¤æ˜“é‡‘é¢ç‰¹å¾
        if 'transaction_amount' in cluster_data.columns:
            amounts = cluster_data['transaction_amount']
            characteristics['avg_transaction_amount'] = round(amounts.mean(), 2)
            characteristics['median_transaction_amount'] = round(amounts.median(), 2)
            characteristics['transaction_amount_std'] = round(amounts.std(), 2)

            # è®¡ç®—é«˜é‡‘é¢äº¤æ˜“æ¯”ä¾‹ï¼ˆå¤§äºå¹³å‡å€¼+2å€æ ‡å‡†å·®ï¼‰
            mean_amount = amounts.mean()
            std_amount = amounts.std()
            high_threshold = mean_amount + 2 * std_amount if std_amount > 0 else mean_amount * 2
            characteristics['high_amount_rate'] = round((amounts > high_threshold).mean(), 3)

        # å®¢æˆ·å¹´é¾„ç‰¹å¾
        if 'customer_age' in cluster_data.columns:
            characteristics['avg_customer_age'] = round(cluster_data['customer_age'].mean(), 1)

        # æœ€å¸¸è§çš„åˆ†ç±»ç‰¹å¾
        if 'payment_method' in cluster_data.columns:
            characteristics['common_payment_method'] = cluster_data['payment_method'].mode().iloc[0] if not cluster_data['payment_method'].mode().empty else 'unknown'

        if 'device_used' in cluster_data.columns:
            characteristics['common_device'] = cluster_data['device_used'].mode().iloc[0] if not cluster_data['device_used'].mode().empty else 'unknown'

        if 'product_category' in cluster_data.columns:
            characteristics['common_category'] = cluster_data['product_category'].mode().iloc[0] if not cluster_data['product_category'].mode().empty else 'unknown'

        # Fraud rate
        if 'is_fraudulent' in cluster_data.columns:
            characteristics['fraud_rate'] = round(cluster_data['is_fraudulent'].mean(), 3)

            # Determine risk level based on fraud rate
            fraud_rate = characteristics['fraud_rate']
            if fraud_rate > 0.1:
                characteristics['risk_level'] = 'high'
            elif fraud_rate > 0.05:
                characteristics['risk_level'] = 'medium'
            else:
                characteristics['risk_level'] = 'low'

        # Time patterns
        if 'transaction_hour' in cluster_data.columns:
            characteristics['common_hour'] = int(cluster_data['transaction_hour'].mode().iloc[0]) if not cluster_data['transaction_hour'].mode().empty else 12

            # Calculate night transaction ratio (22:00-5:00)
            night_transactions = ((cluster_data['transaction_hour'] >= 22) | (cluster_data['transaction_hour'] <= 5))
            characteristics['night_transaction_rate'] = round(night_transactions.mean(), 3)

        # Account age features
        if 'account_age_days' in cluster_data.columns:
            account_ages = cluster_data['account_age_days']
            characteristics['avg_account_age_days'] = round(account_ages.mean(), 1)
            characteristics['median_account_age_days'] = round(account_ages.median(), 1)

            # Calculate new account ratio (less than 90 days)
            new_accounts = (account_ages < 90)
            characteristics['new_account_rate'] = round(new_accounts.mean(), 3)

        # Device usage patterns
        if 'device' in cluster_data.columns:
            # Mobile device usage ratio
            mobile_usage = (cluster_data['device'] == 'mobile')
            characteristics['mobile_device_rate'] = round(mobile_usage.mean(), 3)
            characteristics['common_device'] = cluster_data['device'].mode().iloc[0] if not cluster_data['device'].mode().empty else 'unknown'

        # æ”¯ä»˜æ–¹å¼æ¨¡å¼
        if 'payment_method' in cluster_data.columns:
            # é“¶è¡Œè½¬è´¦æ¯”ä¾‹
            bank_transfer_usage = (cluster_data['payment_method'] == 'bank_transfer')
            characteristics['bank_transfer_rate'] = round(bank_transfer_usage.mean(), 3)

        # å•†å“ç±»åˆ«é£é™©
        if 'product_category' in cluster_data.columns:
            # é«˜é£é™©ç±»åˆ«æ¯”ä¾‹ï¼ˆelectronicsé€šå¸¸é£é™©è¾ƒé«˜ï¼‰
            electronics_rate = (cluster_data['product_category'] == 'electronics').mean()
            characteristics['electronics_rate'] = round(electronics_rate, 3)
        else:
            characteristics['electronics_rate'] = 0.3  # é»˜è®¤å€¼

        # åœ°å€ä¸€è‡´æ€§ï¼ˆå¦‚æœæœ‰ç›¸å…³å­—æ®µï¼‰
        if 'shipping_address' in cluster_data.columns and 'billing_address' in cluster_data.columns:
            address_mismatch_rate = (cluster_data['shipping_address'] != cluster_data['billing_address']).mean()
            characteristics['address_mismatch_rate'] = round(address_mismatch_rate, 3)
        else:
            # å¦‚æœæ²¡æœ‰åœ°å€å­—æ®µï¼ŒåŸºäºå…¶ä»–å› ç´ ä¼°ç®—åœ°å€é£é™©
            characteristics['address_mismatch_rate'] = 0.1  # é»˜è®¤å€¼

        # å•†å“ç±»åˆ«é£é™©
        if 'product_category' in cluster_data.columns:
            characteristics['electronics_rate'] = round(
                len(cluster_data[cluster_data['product_category'] == 'electronics']) / len(cluster_data), 3
            )

        # Address consistency
        if 'shipping_address' in cluster_data.columns and 'billing_address' in cluster_data.columns:
            address_mismatch = cluster_data['shipping_address'] != cluster_data['billing_address']
            characteristics['address_mismatch_rate'] = round(address_mismatch.mean(), 3)

        # Transaction amount statistics
        if 'transaction_amount' in cluster_data.columns:
            characteristics['transaction_amount_std'] = round(cluster_data['transaction_amount'].std(), 2)
            characteristics['high_amount_rate'] = round(
                len(cluster_data[cluster_data['transaction_amount'] > 1000]) / len(cluster_data), 3
            )

        return characteristics

    def _evaluate_clustering_quality(self, cluster_data: pd.DataFrame, labels: List[int]) -> Dict[str, float]:
        """è¯„ä¼°èšç±»è´¨é‡"""
        try:
            silhouette = silhouette_score(cluster_data, labels)
            calinski_harabasz = calinski_harabasz_score(cluster_data, labels)

            return {
                'silhouette_score': round(silhouette, 3),
                'calinski_harabasz_score': round(calinski_harabasz, 3)
            }
        except Exception as e:
            logger.warning(f"èšç±»è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return {'silhouette_score': 0, 'calinski_harabasz_score': 0}

    def _analyze_anomalies(self, data: pd.DataFrame, labels: List[int]) -> Dict[str, Any]:
        """åˆ†æå¼‚å¸¸ç¾¤ä½“"""
        anomaly_info = {
            'high_risk_clusters': [],
            'small_clusters': [],
            'unusual_patterns': []
        }

        # Count size and fraud rate for each cluster
        unique_labels = set(labels)
        total_size = len(labels)

        for label in unique_labels:
            if label == -1:  # DBSCAN noise points
                continue

            cluster_mask = [l == label for l in labels]
            cluster_data = data[cluster_mask]
            cluster_size = len(cluster_data)

            # Small clusters (proportion less than 5%)
            if cluster_size / total_size < 0.05:
                anomaly_info['small_clusters'].append({
                    'cluster_id': label,
                    'size': cluster_size,
                    'percentage': round(cluster_size / total_size * 100, 2)
                })

            # High-risk clusters (fraud rate > 10%)
            if 'is_fraudulent' in cluster_data.columns:
                fraud_rate = cluster_data['is_fraudulent'].mean()
                if fraud_rate > 0.1:
                    anomaly_info['high_risk_clusters'].append({
                        'cluster_id': label,
                        'fraud_rate': round(fraud_rate, 3),
                        'size': cluster_size
                    })

        return anomaly_info

    def _empty_result(self) -> Dict[str, Any]:
        """Return empty result"""
        return {
            'algorithm': 'none',
            'cluster_count': 0,
            'cluster_labels': [],
            'cluster_details': [],
            'quality_metrics': {},
            'anomaly_analysis': {}
        }

    def fit_dbscan(self, data: pd.DataFrame, eps: float = 0.5, min_samples: int = 5) -> np.ndarray:
        self.method = 'dbscan'
        self.model = DBSCAN(eps=eps, min_samples=min_samples)
        self.labels_ = self.model.fit_predict(data)
        self.centers_ = None
        logger.info(f"DBSCAN clustering completed, number of clusters: {len(set(self.labels_)) - (1 if -1 in self.labels_ else 0)}")
        return self.labels_

    def fit_gmm(self, data: pd.DataFrame, n_components: int = 4, random_state: int = 42) -> np.ndarray:
        self.method = 'gmm'
        self.model = GaussianMixture(n_components=n_components, random_state=random_state)
        self.labels_ = self.model.fit_predict(data)
        self.centers_ = self.model.means_
        logger.info(f"é«˜æ–¯æ··åˆèšç±»å®Œæˆï¼Œç»„ä»¶æ•°: {n_components}")
        return self.labels_

    def get_cluster_centers(self):
        return self.centers_

    def get_labels(self):
        return self.labels_

    def get_method(self):
        return self.method

    def intelligent_auto_clustering(self, data: pd.DataFrame,
                                   target_risk_distribution: Dict[str, float] = None) -> Dict[str, Any]:
        """
        æ™ºèƒ½è‡ªåŠ¨èšç±» - ä¸€é”®æœ€ä¼˜èšç±»

        Args:
            data: è¾“å…¥æ•°æ®
            target_risk_distribution: ç›®æ ‡é£é™©åˆ†å¸ƒ

        Returns:
            å®Œæ•´çš„èšç±»åˆ†æç»“æœ
        """
        logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½è‡ªåŠ¨èšç±»")

        try:
            # ä½¿ç”¨æ™ºèƒ½ä¼˜åŒ–å™¨è¿›è¡Œè‡ªåŠ¨ä¼˜åŒ–
            optimization_result = self.intelligent_optimizer.auto_optimize_clustering(
                data, target_risk_distribution
            )

            # æ™ºèƒ½ä¼˜åŒ–å™¨ç°åœ¨è¿”å›æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
            # æ›´æ–°é£é™©æ˜ å°„å™¨çš„é˜ˆå€¼
            optimal_thresholds = optimization_result.get('optimal_thresholds', {})
            if optimal_thresholds:
                self.risk_mapper.cluster_risk_thresholds = optimal_thresholds

            # æ‰§è¡Œé£é™©æ˜ å°„ï¼ˆå¦‚æœè¿˜æ²¡æœ‰é£é™©ä¿¡æ¯ï¼‰
            if 'cluster_details' in optimization_result and optimization_result['cluster_details']:
                cluster_details = optimization_result['cluster_details']

                # æ£€æŸ¥æ˜¯å¦éœ€è¦é£é™©æ˜ å°„
                needs_risk_mapping = True
                if cluster_details and len(cluster_details) > 0:
                    first_cluster = cluster_details[0]
                    if 'risk_level' in first_cluster and first_cluster['risk_level'] != 'unknown':
                        needs_risk_mapping = False

                if needs_risk_mapping:
                    risk_mapping_result = self.risk_mapper.map_clusters_to_risk_levels(
                        {'cluster_details': cluster_details}, data
                    )

                    # æ›´æ–°èšç±»è¯¦æƒ…ä¸­çš„é£é™©ä¿¡æ¯
                    cluster_risk_mapping = risk_mapping_result.get('cluster_risk_mapping', {})
                    for detail in cluster_details:
                        cluster_id = detail.get('cluster_id', -1)
                        if cluster_id in cluster_risk_mapping:
                            risk_info = cluster_risk_mapping[cluster_id]
                            detail['risk_level'] = risk_info.get('risk_level', 'low')
                            detail['risk_score'] = risk_info.get('risk_score', 0)

            # è¿”å›ä¼˜åŒ–ç»“æœï¼ˆå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼‰
            final_result = optimization_result

            logger.info("âœ… æ™ºèƒ½è‡ªåŠ¨èšç±»å®Œæˆ")
            return final_result

        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½è‡ªåŠ¨èšç±»å¤±è´¥: {e}")
            # å›é€€åˆ°ä¼ ç»Ÿèšç±»
            return self.analyze_clusters(data, algorithm='kmeans')

    def _generate_cluster_details_from_labels(self, data: pd.DataFrame,
                                            labels: np.ndarray,
                                            features: List[str]) -> List[Dict[str, Any]]:
        """ä»èšç±»æ ‡ç­¾ç”Ÿæˆèšç±»è¯¦æƒ…"""
        cluster_details = []

        for cluster_id in range(len(set(labels))):
            cluster_mask = labels == cluster_id
            cluster_data = data[cluster_mask]

            if len(cluster_data) == 0:
                continue

            # Basic statistics
            detail = {
                'cluster_id': cluster_id,
                'size': len(cluster_data),
                'percentage': len(cluster_data) / len(data) * 100
            }

            # Numerical feature statistics
            for feature in features:
                if feature in cluster_data.columns and pd.api.types.is_numeric_dtype(cluster_data[feature]):
                    detail[f'avg_{feature}'] = cluster_data[feature].mean()
                    detail[f'{feature}_std'] = cluster_data[feature].std()

            # Special feature calculations
            if 'is_fraudulent' in cluster_data.columns:
                detail['fraud_rate'] = cluster_data['is_fraudulent'].mean()

            if 'transaction_amount' in cluster_data.columns:
                detail['avg_transaction_amount'] = cluster_data['transaction_amount'].mean()
                detail['transaction_amount_std'] = cluster_data['transaction_amount'].std()
                # High amount transaction ratio
                high_amount_threshold = data['transaction_amount'].quantile(0.75)
                detail['high_amount_rate'] = (cluster_data['transaction_amount'] > high_amount_threshold).mean()

            if 'account_age_days' in cluster_data.columns:
                detail['avg_account_age_days'] = cluster_data['account_age_days'].mean()
                detail['new_account_rate'] = (cluster_data['account_age_days'] < 30).mean()

            if 'transaction_hour' in cluster_data.columns:
                detail['night_transaction_rate'] = (
                    (cluster_data['transaction_hour'] >= 22) |
                    (cluster_data['transaction_hour'] <= 6)
                ).mean()
                detail['common_hour'] = cluster_data['transaction_hour'].mode().iloc[0] if len(cluster_data['transaction_hour'].mode()) > 0 else 12

            # Categorical feature statistics
            if 'device' in cluster_data.columns:
                device_counts = cluster_data['device'].value_counts()
                detail['common_device'] = device_counts.index[0] if len(device_counts) > 0 else 'unknown'
                detail['mobile_device_rate'] = (cluster_data['device'] == 'mobile').mean()

            if 'payment_method' in cluster_data.columns:
                payment_counts = cluster_data['payment_method'].value_counts()
                detail['common_payment_method'] = payment_counts.index[0] if len(payment_counts) > 0 else 'unknown'
                detail['bank_transfer_rate'] = (cluster_data['payment_method'] == 'bank_transfer').mean()

            cluster_details.append(detail)

        return cluster_details